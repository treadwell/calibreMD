---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

## Options

```{r}
# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

rm(list = ls()) # remove everything in the global environment
gc() # reclaim memory

```

## Load libraries

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()
```

# Load data from SQLite to EAV table

```{r}
dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)

eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features() %>% 
  explode_tags()

# Debug: Print book count
cat("Total unique books in EAV:", length(unique(eav$id)), "\n")
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Build the graph

```{r}
# unique tags
# unique books
# number tags + book from 1 to n where n = #tags + #books
# provide node type and tag name lookup
# if book has tag, positive edge

# book-tag mapping
book_tags <- eav %>%
  filter(feature == "tag") %>%
  mutate(tag_name = value,
         book_id = id) %>%
  select(book_id, tag_name) %>%
  distinct() %>%
  arrange(book_id)

# distinct book_ids and add a node_id (simply 0-n)
book_ids <- book_tags %>% 
  select(book_id) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number())

# distinct tags and add a node_id (n+1 to...)
tag_names <- book_tags %>% 
  select(tag_name) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number() + length(book_ids$book_id))

# unified node_id to type lookup (answers is a node a book or a tag?)
node_types <- data_frame(node_id = seq(nrow(book_ids) + nrow(tag_names)),
                         type = dplyr::if_else(node_id <= length(book_ids$book_id), "book", "tag"))

positive_edges <- book_tags %>%               # book_id, tag name
  inner_join(book_ids,  by = "book_id")  %>%   # adds column `node_id`
  rename(node_a = node_id) %>%                # book → node_a
  inner_join(tag_names, by = "tag_name") %>%   # adds 2nd `node_id`
  rename(node_b = node_id) %>%                # tag  → node_b
  select(node_a, node_b)                      # keep only the edge list

g <- igraph::graph_from_data_frame(
        d         = positive_edges,
        vertices  = node_types$node_id,     # all nodes, even isolates
        directed  = FALSE
      )

cmp <- igraph::components(g)

reps <- tibble(
          vid  = seq_along(cmp$membership),   # vertex index (1…N)
          comp = cmp$membership               # component ID
        ) %>%
        group_by(comp) %>%
        summarise(rep = first(vid), .groups = "drop") %>%  # << first member >>
        pull(rep)                           # plain integer vector

new_edges <- as.vector(rbind(reps[-length(reps)], reps[-1]))

positive_edges <- bind_rows(positive_edges, tibble(
  node_a = new_edges[c(TRUE,  FALSE)],   # odd positions
  node_b = new_edges[c(FALSE, TRUE)]     # even positions
))

# Create graph with weights (all positive edges have weight 1)
g <- igraph::graph_from_data_frame(
  positive_edges %>% mutate(weight = 1), 
  vertices  = node_types$node_id,
  directed = FALSE
)
igraph::components(g)$no

```

## Adjacency Matrix
```{r}
# A <- Matrix::sparseMatrix(
#   i = symmetric_edges$node_a,
#   j = symmetric_edges$node_b,
#   x = 1.0,
#   dims = c(nrow(node_types), nrow(node_types)),
#   dimnames = list(node_types$node_id, node_types$node_id)
# )

A <- igraph::as_adjacency_matrix(
        g,
        type = "both",
        sparse = TRUE        # <- returns Matrix::dgCMatrix
      )

head(igraph::V(g)$name)
```

## Laplacian
```{r}
# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_norm <- Matrix::Diagonal(n = nrow(node_types)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```

## Get book embeddings

* think of these as ways to separate the space into increasingly higher resolution clusters
* they are binary cuts in the graph
* you need to collectively progress through them to get the finer level clustering

```{r}
# Set number of nontrivial eigenpairs desired
k <- 256 

eig <- RSpectra::eigs_sym(
  L_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

# Remove the trivial eigenpair (usually eigenvalue ~0)
# Optionally filter based on a tolerance if needed
eigvals <- eig$values[-1]
eigvecs <- eig$vectors[, -1]

eigvals

```
## Visualize graph

```{r}
library(igraph)

# Simplify graph for visualization (remove loops and multiple edges)
g_viz <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = "sum")

plot(g_viz, layout = eigvecs[, 1:2], vertex.size = 4, vertex.label = NA,
     edge.color = ifelse(E(g_viz)$weight > 0, "steelblue", "firebrick"),
     edge.width = abs(E(g_viz)$weight) + .2)
```

## Iterative quantization

* Rotate the embeddings to minimize quantization error (like PCA?)
* In PCA rotate feature space to explain the most variance
* rotate eigenvectors as if they were raw feature vectors, binarizing at each rotation, and
  trying to match the binary hamming distance to the dot product of the vectors
* you're mapping dot product to hamming distance here
* this allows you to obtain embeddings, continuous from eigenvectors, and binary, from the
  iterative quantization, where dot product (cosine similarity) and hamming distance approximate
  proximity in the graph
* This can be used to find similar books per the underlying graph

```{r}
iterative_quantization <- function(X, n_iter = 25) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: PCA via SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code in {-1, +1}
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes in {0,1} as integer matrix (uint8-like)
  B_final <- sign(X_pca %*% R)
  storage.mode(B_final) <- "integer"
  B01 <- ifelse(B_final > 0L, 1L, 0L)
  mode(B01) <- "integer"
  B01
}

binary_embedding <- iterative_quantization(eigvecs)  # uint8-like 0/1 integer matrix
```

## Check binary embeddings
```{r}
cat("binary_embedding dimensions:", dim(binary_embedding), "\n")
cat("NA count:", sum(is.na(binary_embedding)), "\n")
cat("NaN count:", sum(is.nan(binary_embedding)), "\n")
cat("Inf count:", sum(is.infinite(binary_embedding)), "\n")
cat("Value range:", range(binary_embedding, na.rm = TRUE), "\n")
cat("First 10 rownames:", head(rownames(binary_embedding), 10), "\n")
cat("Total rownames:", length(rownames(binary_embedding)), "\n")
print(binary_embedding[1:5, 1:5])

# Check embedding diversity
cat("\n=== EMBEDDING DIVERSITY ANALYSIS ===\n")

# Count unique binary codes
unique_codes <- unique(as.data.frame(binary_embedding))
cat("Unique binary codes:", nrow(unique_codes), "out of", nrow(binary_embedding), "books\n")
cat("Diversity ratio:", round(nrow(unique_codes) / nrow(binary_embedding), 3), "\n")

# Check eigvecs diversity
unique_eigvecs <- unique(as.data.frame(round(eigvecs, 6)))
cat("Unique eigenvector codes (rounded to 6 digits):", nrow(unique_eigvecs), "out of", nrow(eigvecs), "books\n")
cat("Eigenvector diversity ratio:", round(nrow(unique_eigvecs) / nrow(eigvecs), 3), "\n")

# Check if the problem is in the graph construction
cat("\n=== GRAPH ANALYSIS ===\n")
cat("Books:", nrow(book_ids), "\n")
cat("Tags:", nrow(tag_names), "\n")
cat("Positive edges:", nrow(positive_edges), "\n")
cat("Graph edges (including multiples):", igraph::ecount(g), "\n")

cmp <- igraph::components(g)
cat("Connected components:", cmp$no, "\n")

# Edge diagnostics
edge_ends <- igraph::ends(g, igraph::E(g))
loops_count <- sum(edge_ends[, 1] == edge_ends[, 2])
multiples_count <- sum(igraph::which_multiple(g))
cat("Loops:", loops_count, "\n")
cat("Multiple edges:", multiples_count, "\n")

# Degree by node type
deg <- igraph::degree(g, mode = "all")
deg_df <- tibble::tibble(node_id = as.integer(names(deg)), degree = as.numeric(deg)) |>
  dplyr::left_join(node_types, by = "node_id")

cat("\nDegree summary (books):\n")
print(
  deg_df |>
    dplyr::filter(type == "book") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

cat("\nDegree summary (tags):\n")
print(
  deg_df |>
    dplyr::filter(type == "tag") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

# Top tags by connectivity
top_tags <- deg_df |>
  dplyr::filter(type == "tag") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(tag_names, by = "node_id") |>
  dplyr::select(tag_name, degree)
cat("\nTop 10 tags by degree:\n")
print(top_tags)

# Top books by connectivity
top_books <- deg_df |>
  dplyr::filter(type == "book") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(book_ids, by = "node_id") |>
  dplyr::select(book_id, degree)
cat("\nTop 10 books by degree:\n")
print(top_books)

# Check tag distribution
# Determine whether a tag value existed in the original metadata (pre-explosion)
original_tag_values <- md_db %>%
  load_eav() %>%
  dplyr::filter(feature == "tag") %>%
  dplyr::distinct(value) %>%
  dplyr::pull(value)

tag_counts <- eav %>% 
  dplyr::filter(feature == "tag") %>% 
  dplyr::count(value, sort = TRUE) %>%
  dplyr::mutate(tag_type = ifelse(value %in% original_tag_values, "original", "exploded"))

cat("Top 10 most common tags:\n")
print(tag_counts %>% head(10))
```

## Unified metadata builder

```{r}
# Build a single metadata tibble with unified lookups by node_id
# Columns: node_id, node_type, row_id, book_id, book_title, tag_name, tag_origin
build_unified_metadata <- function(eav, book_ids, tag_names, node_types, binary_embedding, md_db) {
  stopifnot(is.data.frame(eav), is.data.frame(book_ids), is.data.frame(tag_names),
            is.data.frame(node_types), is.matrix(binary_embedding))

  # Titles by book_id
  titles <- eav %>%
    dplyr::filter(feature == "title_original") %>%
    dplyr::distinct(id, .keep_all = TRUE) %>%
    dplyr::transmute(book_id = as.integer(id), book_title = as.character(value))

  # Original tag values from source DB (pre-explosion)
  original_tag_values <- md_db %>%
    load_eav() %>%
    dplyr::filter(feature == "tag") %>%
    dplyr::distinct(value) %>%
    dplyr::pull(value) %>%
    as.character()

  # Base node frame
  nodes <- tibble::tibble(
    node_id   = as.integer(node_types$node_id),
    node_type = as.character(node_types$type)
  )

  # row_id mapping: by construction row index/order equals node_id
  row_map <- tibble::tibble(
    node_id = as.integer(node_types$node_id),
    row_id  = as.integer(node_types$node_id)
  )

  # Book-side metadata
  books_meta <- book_ids %>%
    dplyr::transmute(node_id = as.integer(node_id), book_id = as.integer(book_id)) %>%
    dplyr::left_join(titles, by = "book_id")

  # Tag-side metadata
  tags_meta <- tag_names %>%
    dplyr::transmute(node_id = as.integer(node_id), tag_name = as.character(tag_name)) %>%
    dplyr::mutate(tag_origin = ifelse(tag_name %in% original_tag_values, "original", "exploded"))

  # Unified
  unified <- nodes %>%
    dplyr::left_join(row_map,  by = "node_id") %>%
    dplyr::left_join(books_meta, by = "node_id") %>%
    dplyr::left_join(tags_meta,  by = "node_id") %>%
    dplyr::arrange(node_id)

  unified
}

# Example:
unified_meta <- build_unified_metadata(eav, book_ids, tag_names, node_types, binary_embedding, md_db)
```

# Get new tags

## API design

* Given a db, produce a codebook that projects books and tags into a common embedding space
* Given a book, show me recommended tags at each margin (distance)
* Given a tag, show me which books it should be in at each margin
* Give me the optimal tags for a book
* Give me the books that should optimally have a specified tag (or tags)
* Recommend tag nesting (if it's a suffix, recommend the unexploded tag)
* Given a codebook, learn mappings from raw features to codes

## Fn: Get existing tags for each book

```{r}
get_existing_tags <- function(eav) {
  eav %>% 
    filter(feature == "tag") %>% 
    select(id, tag = value) %>% 
    group_by(id) %>% 
    summarize(existing_tags = list(tag), .groups = "drop")
}
```

## Fn: Get nearest neighbors

```{r}
# Simple Hamming-based nearest neighbors (pure R, no Python interop)
get_nearest_neighbors <- function(code_vector,
                                  index = NULL,
                                  binary_embedding = NULL,
                                  unified_meta,
                                  num_neighbors = NULL,
                                  margin = NULL,
                                  type = NULL) {
  stopifnot(is.numeric(code_vector))
  if (is.null(index)) stopifnot(is.matrix(binary_embedding))

  # If a prebuilt Hamming index is provided, use it
  if (!is.null(index)) {
    res <- hamming_search(
      index        = index,
      code_vector  = code_vector,
      k            = num_neighbors,
      margin       = margin,
      type         = type,
      unified_meta = unified_meta
    )
    return(res)
  }

  # Efficient Hamming distance for 0/1 data without large temporaries
  sx <- rowSums(binary_embedding)
  sy <- sum(code_vector)
  dot <- as.numeric(binary_embedding %*% code_vector)
  distances <- sx + sy - 2 * dot

  # Candidate set
  candidates <- seq_len(nrow(binary_embedding))

  # Type filter: row index equals node_id by construction
  if (!is.null(type)) {
    stopifnot(all(c("row_id", "node_type") %in% names(unified_meta)))
    row_types <- unified_meta$node_type[match(seq_len(nrow(binary_embedding)), unified_meta$row_id)]
    candidates <- candidates[!is.na(row_types) & row_types == type]
  }

  # Margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(node_ids = integer(0), distances = numeric(0)))

  # Order by distance then index
  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  # Top-k
  if (!is.null(num_neighbors) && length(candidates) > num_neighbors) {
    candidates <- candidates[seq_len(as.integer(num_neighbors))]
  }

  list(node_ids = candidates, distances = distances[candidates])
}
```

## Fn: Hamming index

```{r}
# Build a queryable flat Hamming index from a 0/1 integer matrix
# Returns a lightweight list with matrix and node_id mapping
build_hamming_index <- function(binary_embedding, unified_meta = NULL) {
  stopifnot(is.matrix(binary_embedding))
  node_ids <- if (!is.null(unified_meta) && all(c("node_id", "row_id") %in% names(unified_meta))) {
    unified_meta$node_id[match(seq_len(nrow(binary_embedding)), unified_meta$row_id)]
  } else {
    seq_len(nrow(binary_embedding))
  }
  list(
    data = binary_embedding,
    node_ids = as.integer(node_ids)
  )
}

# Query the flat Hamming index
hamming_search <- function(index,
                           code_vector,
                           k = NULL,
                           margin = NULL,
                           type = NULL,
                           unified_meta = NULL) {
  stopifnot(is.list(index), is.matrix(index$data), is.numeric(code_vector))
  X <- index$data
  # Efficient Hamming distances for 0/1 data:
  # d(x,y) = sum(x) + sum(y) - 2 * x·y
  sx <- rowSums(X)
  sy <- sum(code_vector)
  dot <- as.numeric(X %*% code_vector)
  distances <- sx + sy - 2 * dot

  candidates <- seq_len(nrow(X))

  # Optional type filter using unified metadata mapping
  if (!is.null(type)) {
    stopifnot(!is.null(unified_meta), all(c("node_id", "node_type") %in% names(unified_meta)))
    cand_node_ids <- index$node_ids[candidates]
    cand_types <- unified_meta$node_type[match(cand_node_ids, unified_meta$node_id)]
    keep <- !is.na(cand_types) & cand_types == type
    candidates <- candidates[keep]
  }

  # Optional margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(node_ids = integer(0), distances = numeric(0)))

  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  if (!is.null(k) && length(candidates) > k) {
    candidates <- candidates[seq_len(as.integer(k))]
  }

  list(node_ids = index$node_ids[candidates], distances = distances[candidates])
}

# Compute neighborhoods for all rows using the index (vectorized by blocks)
all_neighborhoods <- function(index,
                              k,
                              margin = NULL,
                              type = NULL,
                              unified_meta = NULL,
                              block_size = 512) {
  stopifnot(is.list(index), is.matrix(index$data))
  X <- index$data
  n <- nrow(X)

  # Precompute sums for fast Hamming
  sx <- rowSums(X)

  # Optional type mask
  type_mask <- rep(TRUE, n)
  if (!is.null(type)) {
    stopifnot(!is.null(unified_meta), all(c("node_id", "node_type") %in% names(unified_meta)))
    types <- unified_meta$node_type[match(index$node_ids, unified_meta$node_id)]
    type_mask <- !is.na(types) & types == type
  }

  results_ids <- vector("list", n)
  results_dist <- vector("list", n)

  for (start in seq(1, n, by = block_size)) {
    end <- min(start + block_size - 1L, n)
    Q <- X[start:end, , drop = FALSE]
    sy <- rowSums(Q)
    dot <- X %*% t(Q)                   # n x b
    # Hamming distances for all pairs in block
    # d = sx + sy - 2*dot, broadcasting sy by column
    D <- sweep(matrix(sx, nrow = n, ncol = ncol(dot)), 2, sy, `+`) - 2 * dot

    # Apply optional type and margin filters
    if (!all(type_mask)) {
      D[!type_mask, ] <- Inf
    }
    if (!is.null(margin)) {
      D[D > margin] <- Inf
    }

    # For each query in block, take top-k
    for (j in seq_len(ncol(D))) {
      dcol <- D[, j]
      ord <- order(dcol, seq_len(n), na.last = NA)
      if (is.finite(k)) {
        ord <- ord[seq_len(min(k, length(ord)))]
      }
      results_ids[[start + j - 1L]] <- index$node_ids[ord]
      results_dist[[start + j - 1L]] <- dcol[ord]
    }
  }

  list(node_ids = results_ids, distances = results_dist)
}
```

## Fn: Recommend tags
```{r}
# Assumes you have:
# - binary_embedding : matrix
# - book_ids         : data.frame/tibble with columns node_id, book_id
# - tag_names        : data.frame/tibble with columns node_id, tag_name
# - (optional) node_type : data.frame/tibble with columns node_id, type
# - get_nearest_neighbors() from the previous message

recommend_tags <- function(margin,
                           num_neighbors = NULL,
                           type = NULL,
                           unified_meta,
                           book_ids,
                           tag_names,
                           existing_tags_df,
                           binary_embedding,
                           index = NULL) {
  stopifnot(all(c("node_id", "book_id") %in% names(book_ids)))
  stopifnot(all(c("node_id", "tag_name") %in% names(tag_names)))
  stopifnot(!is.null(index))

  results_rows <- list()
  # existing_tags_df provided by caller

  # Helper: promote bare segment to its longest hierarchical path found in tag_names
  to_full_path <- function(tag_vector) {
    # Fast path: if all contain '.', return as-is
    if (all(grepl("\\.", tag_vector))) return(tag_vector)

    universe <- tag_names$tag_name
    # Precompute last segments and path depths for the universe
    universe_splits <- strsplit(universe, "\\.")
    universe_last   <- vapply(universe_splits, function(x) if (length(x)) x[length(x)] else "", character(1))
    universe_depth  <- vapply(universe_splits, length, integer(1))

    promote_one <- function(t) {
      if (is.na(t) || t == "") return(t)
      if (grepl("\\.", t)) return(t)
      idx <- which(universe_last == t)
      if (length(idx) == 0) return(t)
      # Choose deepest path; break ties by shortest string
      idx <- idx[order(-universe_depth[idx], nchar(universe[idx]))]
      universe[idx[1]]
    }
    vapply(tag_vector, promote_one, FUN.VALUE = character(1))
  }

  # Helper: keep only the most specific (leaf) paths; drop ancestors
  prune_to_leaf <- function(paths) {
    paths <- unique(paths)
    if (length(paths) <= 1) return(paths)
    keep <- !vapply(paths, function(t) any(paths != t & startsWith(paths, paste0(t, "."))), logical(1))
    paths[keep]
  }

  # Compute neighborhoods once using the index
  k_val <- if (is.null(num_neighbors)) nrow(index$data) else as.integer(num_neighbors)
  all_nbrs <- all_neighborhoods(
    index        = index,
    k            = k_val,
    margin       = margin,
    type         = type,
    unified_meta = unified_meta
  )

  # Map each book to its row_id (which aligns with index rows)
  book_row_ids <- unified_meta$row_id[match(book_ids$node_id, unified_meta$node_id)]

  for (i in seq_along(book_row_ids)) {
    row_idx <- book_row_ids[i]
    if (is.na(row_idx) || row_idx <= 0L) next
    this_book_id <- book_ids$book_id[i]

    nbr_ids <- all_nbrs$node_ids[[row_idx]]
    if (length(nbr_ids) == 0) next

    tag_lookup <- unified_meta$tag_name[match(nbr_ids, unified_meta$node_id)]
    rec_tags <- unique(stats::na.omit(tag_lookup))

    if (length(rec_tags) > 0) {
      rec_tags <- prune_to_leaf(to_full_path(rec_tags))
      # Remove tags already on this book
      idx_match <- which(existing_tags_df$id == this_book_id)
      this_existing <- if (length(idx_match) > 0) existing_tags_df$existing_tags[idx_match][[1]] else NULL
      existing_vec <- if (!is.null(this_existing)) unlist(this_existing, use.names = FALSE) else character(0)
      new_tags <- setdiff(rec_tags, existing_vec)

      if (length(new_tags) > 0) {
        results_rows[[length(results_rows) + 1L]] <- tibble::tibble(
          book_id = as.character(this_book_id),
          tag_name = as.character(new_tags)
        )
      }
    }
  }

  if (length(results_rows) == 0) {
    return(tibble::tibble(book_id = character(0), tag_name = character(0)))
  }

  dplyr::bind_rows(results_rows)
}
```
## Get recommendations
```{r}
existing_tags_df <- get_existing_tags(eav)
unified_meta <- build_unified_metadata(eav, book_ids, tag_names, node_types, binary_embedding, md_db)
# Optional: build flat Hamming index for faster queries
hamming_idx <- build_hamming_index(binary_embedding, unified_meta)
recs <- recommend_tags(
  margin             = 0,
  num_neighbors      = 25,
  type               = "tag",        # or NULL
  unified_meta       = unified_meta,
  book_ids           = book_ids,
  tag_names          = tag_names,
  existing_tags_df   = existing_tags_df,
  binary_embedding   = binary_embedding,
  index              = hamming_idx
)
```

### sanity checks
```{r}
cat("\n=== Recommendation sanity checks ===\n")
cat("Total recommendations:", nrow(recs), "\n")

books_with_recs <- if (nrow(recs) > 0) dplyr::n_distinct(recs$book_id) else 0L
cat(
  "Books with at least one rec:", books_with_recs,
  "of", nrow(book_ids),
  sprintf("(%.1f%%)", if (nrow(book_ids) > 0) 100 * books_with_recs / nrow(book_ids) else 0),
  "\n"
)

if (nrow(recs) > 0) {
  recs_per_book <- recs %>% dplyr::count(book_id, name = "n")
  cat("\nRecs per book summary:\n")
  print(summary(recs_per_book$n))

  cat("\nTop 5 books by rec count:\n")
  print(
    recs_per_book %>%
      dplyr::arrange(dplyr::desc(n)) %>%
      dplyr::slice_head(n = 5)
  )

  cat("\nTop 10 recommended tags:\n")
  print(
    recs %>%
      dplyr::count(tag_name, sort = TRUE, name = "n") %>%
      dplyr::slice_head(n = 10)
  )

  # Sample with titles if available
  if (exists("unified_meta")) {
    sample_rows <- recs %>%
      dplyr::left_join(
        unified_meta %>% dplyr::select(book_id, book_title) %>% dplyr::distinct() %>% dplyr::mutate(book_id = as.character(book_id)),
        by = "book_id"
      ) %>%
      dplyr::arrange(book_id, dplyr::desc(tag_name)) %>%
      dplyr::slice_head(n = 10)
    cat("\nSample recommendations (up to 10):\n")
    print(sample_rows)
  }
}
```
# End

# Refactor
Refactor:

1. Formalize entity metadata lookup (node_type, book_id, tag_name) from node_id
2. Remove global references throughout
3. Iterative Quantization must return uint8 matrix of 0s and 1s
4. Add a helper function to create a queryable index from the uint8 matrix
  a. must be a Hamming-based index, either LSH or flat
5. get_nearest_neighbors() should accept existing filtering arguments as well as the index and metadata lookup structures.
6. Recommend_tags() should not do individual NN searches, it should use an index-provided all_neighborhood() function

# Open questions:

* Given a tag, what books should it go on that don't currently have it?




