---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

## Options

```{r}
# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

rm(list = ls()) # remove everything in the global environment
gc() # reclaim memory

```

## Load libraries

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()
```

# Load data from SQLite to EAV table

```{r}
dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)

eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features() %>% 
  explode_tags()

# Debug: Print book count
cat("Total unique books in EAV:", length(unique(eav$id)), "\n")
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Build the graph

```{r}
# unique tags
# unique books
# number tags + book from 1 to n where n = #tags + #books
# provide node type and tag name lookup
# if book has tag, positive edge

# book-tag mapping
book_tags <- eav %>%
  filter(feature == "tag") %>%
  mutate(tag_name = value,
         book_id = id) %>%
  select(book_id, tag_name) %>%
  distinct() %>%
  arrange(book_id)

# distinct book_ids and add a node_id (simply 0-n)
book_ids <- book_tags %>% 
  select(book_id) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number())

# distinct tags and add a node_id (n+1 to...)
tag_names <- book_tags %>% 
  select(tag_name) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number() + length(book_ids$book_id))

# unified node_id to type lookup (answers is a node a book or a tag?)
node_types <- data_frame(node_id = seq(nrow(book_ids) + nrow(tag_names)),
                         type = dplyr::if_else(node_id <= length(book_ids$book_id), "book", "tag"))

positive_edges <- book_tags %>%               # book_id, tag name
  inner_join(book_ids,  by = "book_id")  %>%   # adds column `node_id`
  rename(node_a = node_id) %>%                # book → node_a
  inner_join(tag_names, by = "tag_name") %>%   # adds 2nd `node_id`
  rename(node_b = node_id) %>%                # tag  → node_b
  select(node_a, node_b)                      # keep only the edge list

g <- igraph::graph_from_data_frame(
        d         = positive_edges,
        vertices  = node_types$node_id,     # all nodes, even isolates
        directed  = FALSE
      )

cmp <- igraph::components(g)

reps <- tibble(
          vid  = seq_along(cmp$membership),   # vertex index (1…N)
          comp = cmp$membership               # component ID
        ) %>%
        group_by(comp) %>%
        summarise(rep = first(vid), .groups = "drop") %>%  # << first member >>
        pull(rep)                           # plain integer vector

new_edges <- as.vector(rbind(reps[-length(reps)], reps[-1]))

positive_edges <- bind_rows(positive_edges, tibble(
  node_a = new_edges[c(TRUE,  FALSE)],   # odd positions
  node_b = new_edges[c(FALSE, TRUE)]     # even positions
))

# Create graph with weights (all positive edges have weight 1)
g <- igraph::graph_from_data_frame(
  positive_edges %>% mutate(weight = 1), 
  vertices  = node_types$node_id,
  directed = FALSE
)
igraph::components(g)$no

```

## Adjacency Matrix
```{r}
# A <- Matrix::sparseMatrix(
#   i = symmetric_edges$node_a,
#   j = symmetric_edges$node_b,
#   x = 1.0,
#   dims = c(nrow(node_types), nrow(node_types)),
#   dimnames = list(node_types$node_id, node_types$node_id)
# )

A <- igraph::as_adjacency_matrix(
        g,
        type = "both",
        sparse = TRUE        # <- returns Matrix::dgCMatrix
      )

head(igraph::V(g)$name)
```

## Laplacian
```{r}
# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_norm <- Matrix::Diagonal(n = nrow(node_types)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```

## Get book embeddings

* think of these as ways to separate the space into increasingly higher resolution clusters
* they are binary cuts in the graph
* you need to collectively progress through them to get the finer level clustering

```{r}
# Set number of nontrivial eigenpairs desired
k <- 256 

eig <- RSpectra::eigs_sym(
  L_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

# Remove the trivial eigenpair (usually eigenvalue ~0)
# Optionally filter based on a tolerance if needed
eigvals <- eig$values[-1]
eigvecs <- eig$vectors[, -1]

eigvals

```
## Visualize graph

```{r}
library(igraph)

# Simplify graph for visualization (remove loops and multiple edges)
g_viz <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = "sum")

plot(g_viz, layout = eigvecs[, 1:2], vertex.size = 4, vertex.label = NA,
     edge.color = ifelse(E(g_viz)$weight > 0, "steelblue", "firebrick"),
     edge.width = abs(E(g_viz)$weight) + .2)
```

## Iterative quantization (optional)

* Rotate the embeddings to minimize quantization error (like PCA?)
* In PCA rotate feature space to explain the most variance
* rotate eigenvectors as if they were raw feature vectors, binarizing at each rotation, and
  trying to match the binary hamming distance to the dot product of the vectors
* you're mapping dot product to hamming distance here
* this allows you to obtain embeddings, continuous from eigenvectors, and binary, from the
  iterative quantization, where dot product (cosine similarity) and hamming distance approximate
  proximity in the graph
* This can be used to find similar books per the underlying graph

```{r}
iterative_quantization <- function(X, n_iter = 25) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: Optional PCA (skip if X already low-dimensional)
  # SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes and rotation
  list(binary_code = sign(X_pca %*% R), rotation = R)
}

# Option 1: Use iterative quantization (current approach)
itq_result <- iterative_quantization(eigvecs)
binary_embedding <- itq_result$binary_code  # contains -1/+1 codes per book (unpacked)
# rotation_matrix <- itq_result$rotation

# Option 2: Simple binary encoding (alternative approach)
# binary_embedding <- sign(eigvecs)  # Simple sign-based binary encoding

rownames(binary_embedding) <- node_types$node_id

# Simpler approach below uses Hamming distance directly in R; no packed embedding required

```

## Check binary embeddings
```{r}
cat("binary_embedding dimensions:", dim(binary_embedding), "\n")
cat("NA count:", sum(is.na(binary_embedding)), "\n")
cat("NaN count:", sum(is.nan(binary_embedding)), "\n")
cat("Inf count:", sum(is.infinite(binary_embedding)), "\n")
cat("Value range:", range(binary_embedding, na.rm = TRUE), "\n")
cat("First 10 rownames:", head(rownames(binary_embedding), 10), "\n")
cat("Total rownames:", length(rownames(binary_embedding)), "\n")
print(binary_embedding[1:5, 1:5])

# Check embedding diversity
cat("\n=== EMBEDDING DIVERSITY ANALYSIS ===\n")

# Count unique binary codes
unique_codes <- unique(as.data.frame(binary_embedding))
cat("Unique binary codes:", nrow(unique_codes), "out of", nrow(binary_embedding), "books\n")
cat("Diversity ratio:", round(nrow(unique_codes) / nrow(binary_embedding), 3), "\n")

# Check eigvecs diversity
unique_eigvecs <- unique(as.data.frame(round(eigvecs, 6)))
cat("Unique eigenvector codes (rounded to 6 digits):", nrow(unique_eigvecs), "out of", nrow(eigvecs), "books\n")
cat("Eigenvector diversity ratio:", round(nrow(unique_eigvecs) / nrow(eigvecs), 3), "\n")

# Check if the problem is in the graph construction
cat("\n=== GRAPH ANALYSIS ===\n")
cat("Books:", nrow(book_ids), "\n")
cat("Tags:", nrow(tag_names), "\n")
cat("Positive edges:", nrow(positive_edges), "\n")
cat("Graph edges (including multiples):", igraph::ecount(g), "\n")

cmp <- igraph::components(g)
cat("Connected components:", cmp$no, "\n")

# Edge diagnostics
edge_ends <- igraph::ends(g, igraph::E(g))
loops_count <- sum(edge_ends[, 1] == edge_ends[, 2])
multiples_count <- sum(igraph::which_multiple(g))
cat("Loops:", loops_count, "\n")
cat("Multiple edges:", multiples_count, "\n")

# Degree by node type
deg <- igraph::degree(g, mode = "all")
deg_df <- tibble::tibble(node_id = as.integer(names(deg)), degree = as.numeric(deg)) |>
  dplyr::left_join(node_types, by = "node_id")

cat("\nDegree summary (books):\n")
print(
  deg_df |>
    dplyr::filter(type == "book") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

cat("\nDegree summary (tags):\n")
print(
  deg_df |>
    dplyr::filter(type == "tag") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

# Top tags by connectivity
top_tags <- deg_df |>
  dplyr::filter(type == "tag") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(tag_names, by = "node_id") |>
  dplyr::select(tag_name, degree)
cat("\nTop 10 tags by degree:\n")
print(top_tags)

# Top books by connectivity
top_books <- deg_df |>
  dplyr::filter(type == "book") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(book_ids, by = "node_id") |>
  dplyr::select(book_id, degree)
cat("\nTop 10 books by degree:\n")
print(top_books)

# Check tag distribution
# Determine whether a tag value existed in the original metadata (pre-explosion)
original_tag_values <- md_db %>%
  load_eav() %>%
  dplyr::filter(feature == "tag") %>%
  dplyr::distinct(value) %>%
  dplyr::pull(value)

tag_counts <- eav %>% 
  dplyr::filter(feature == "tag") %>% 
  dplyr::count(value, sort = TRUE) %>%
  dplyr::mutate(tag_type = ifelse(value %in% original_tag_values, "original", "exploded"))

cat("Top 10 most common tags:\n")
print(tag_counts %>% head(10))
```

## Unified metadata builder (not yet integrated)

```{r}
# Build a single metadata tibble with unified lookups by node_id
# Columns: node_id, node_type, row_id, book_id, book_title, tag_name, tag_origin
build_unified_metadata <- function(eav, book_ids, tag_names, node_types, binary_embedding, md_db) {
  stopifnot(is.data.frame(eav), is.data.frame(book_ids), is.data.frame(tag_names),
            is.data.frame(node_types), is.matrix(binary_embedding))

  # Titles by book_id
  titles <- eav %>%
    dplyr::filter(feature == "title_original") %>%
    dplyr::distinct(id, .keep_all = TRUE) %>%
    dplyr::transmute(book_id = as.integer(id), book_title = as.character(value))

  # Original tag values from source DB (pre-explosion)
  original_tag_values <- md_db %>%
    load_eav() %>%
    dplyr::filter(feature == "tag") %>%
    dplyr::distinct(value) %>%
    dplyr::pull(value) %>%
    as.character()

  # Base node frame
  nodes <- tibble::tibble(
    node_id   = as.integer(node_types$node_id),
    node_type = as.character(node_types$type)
  )

  # row_id mapping: by construction rownames(binary_embedding) == node_id
  row_map <- tibble::tibble(
    node_id = suppressWarnings(as.integer(rownames(binary_embedding))),
    row_id  = suppressWarnings(as.integer(rownames(binary_embedding)))
  )

  # Book-side metadata
  books_meta <- book_ids %>%
    dplyr::transmute(node_id = as.integer(node_id), book_id = as.integer(book_id)) %>%
    dplyr::left_join(titles, by = "book_id")

  # Tag-side metadata
  tags_meta <- tag_names %>%
    dplyr::transmute(node_id = as.integer(node_id), tag_name = as.character(tag_name)) %>%
    dplyr::mutate(tag_origin = ifelse(tag_name %in% original_tag_values, "original", "exploded"))

  # Unified
  unified <- nodes %>%
    dplyr::left_join(row_map,  by = "node_id") %>%
    dplyr::left_join(books_meta, by = "node_id") %>%
    dplyr::left_join(tags_meta,  by = "node_id") %>%
    dplyr::arrange(node_id)

  unified
}

# Example:
unified_meta <- build_unified_metadata(eav, book_ids, tag_names, node_types, binary_embedding, md_db)
```

# Get new tags

## API design

* given a db, produce a codebook that projects books and tags into a common embedding space
* Given a book, show me recommended tags at each margin (distance)
* Given a tag, show me which books it should be in at each margin
* Give me the optimal tags for a book
* Give me the books that should optimally have this tag
* Recommend tag nesting (if it's a suffix, recommend the unexploded tag)
* Given a codebook, learn mappings from raw features to codes

Notes
* check classic version for functions
## Helper: Get existing tags for each book

```{r}
get_existing_tags <- function(eav) {
  eav %>% 
    filter(feature == "tag") %>% 
    select(id, tag = value) %>% 
    group_by(id) %>% 
    summarize(existing_tags = list(tag), .groups = "drop")
}
```

## Get nearest neighbors

Refactor:

1. Formalize entity metadata lookup (node_type, book_id, tag_name) from node_id
2. Remove global references throughout
3. Iterative Quantization must return uint8 matrix of 0s and 1s
4. Add a helper function to create a queryable index from the uint8 matrix
  a. must be a Hamming-based index, either LSH or flat
5. get_nearest_neighbors() should accept existing filtering arguments as well as the index and metadata lookup structures.
6. Recommend_tags() should not do individual NN searches, it should use an index-provided all_neighborhood() function


```{r}
# Simple Hamming-based nearest neighbors (pure R, no Python interop)
get_nearest_neighbors <- function(code_vector,
                                  num_neighbors = NULL,
                                  margin = NULL,
                                  type = NULL,
                                  node_type = NULL) {
  stopifnot(is.numeric(code_vector))
  stopifnot(exists("binary_embedding"), is.matrix(binary_embedding))

  # Hamming distance to all rows
  diffs <- binary_embedding != matrix(rep(code_vector, nrow(binary_embedding)), nrow(binary_embedding), byrow = TRUE)
  distances <- rowSums(diffs)

  # Candidate set
  candidates <- seq_len(nrow(binary_embedding))

  # Type filter: row index equals node_id by construction
  if (!is.null(type)) {
    stopifnot(!is.null(node_type), all(c("node_id", "type") %in% names(node_type)))
    same_type_ids <- node_type$node_id[node_type$type == type]
    candidates <- candidates[candidates %in% same_type_ids]
  }

  # Margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(node_ids = integer(0), distances = numeric(0)))

  # Order by distance then index
  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  # Top-k
  if (!is.null(num_neighbors) && length(candidates) > num_neighbors) {
    candidates <- candidates[seq_len(as.integer(num_neighbors))]
  }

  list(node_ids = candidates, distances = distances[candidates])
}
```

## Main: Recommend tags
```{r}
# Assumes you have:
# - binary_embedding : matrix
# - book_ids         : data.frame/tibble with columns node_id, book_id
# - tag_names        : data.frame/tibble with columns node_id, tag_name
# - (optional) node_type : data.frame/tibble with columns node_id, type
# - get_nearest_neighbors() from the previous message

recommend_tags <- function(margin,
                           num_neighbors = NULL,
                           type = NULL,
                           node_type = NULL,
                           faiss_index) {
  stopifnot(all(c("node_id", "book_id") %in% names(book_ids)))
  stopifnot(all(c("node_id", "tag_name") %in% names(tag_names)))

  results_rows <- list()
  # Precompute existing tags per book_id
  existing_tags_df <- get_existing_tags(eav)

  # Helper: promote bare segment to its longest hierarchical path found in tag_names
  to_full_path <- function(tag_vector) {
    # Fast path: if all contain '.', return as-is
    if (all(grepl("\\.", tag_vector))) return(tag_vector)

    universe <- tag_names$tag_name
    # Precompute last segments and path depths for the universe
    universe_splits <- strsplit(universe, "\\.")
    universe_last   <- vapply(universe_splits, function(x) if (length(x)) x[length(x)] else "", character(1))
    universe_depth  <- vapply(universe_splits, length, integer(1))

    promote_one <- function(t) {
      if (is.na(t) || t == "") return(t)
      if (grepl("\\.", t)) return(t)
      idx <- which(universe_last == t)
      if (length(idx) == 0) return(t)
      # Choose deepest path; break ties by shortest string
      idx <- idx[order(-universe_depth[idx], nchar(universe[idx]))]
      universe[idx[1]]
    }
    vapply(tag_vector, promote_one, FUN.VALUE = character(1))
  }

  # Helper: keep only the most specific (leaf) paths; drop ancestors
  prune_to_leaf <- function(paths) {
    paths <- unique(paths)
    if (length(paths) <= 1) return(paths)
    keep <- !vapply(paths, function(t) any(paths != t & startsWith(paths, paste0(t, "."))), logical(1))
    paths[keep]
  }

  for (i in seq_len(nrow(book_ids))) {
    this_node_id <- book_ids$node_id[i]
    this_book_id <- book_ids$book_id[i]

    # Locate query code vector by node_id
    row_idx <- which(rownames(binary_embedding) == as.character(this_node_id))
    if (length(row_idx) == 0) next
    code_vector <- binary_embedding[row_idx, ]

    nn <- get_nearest_neighbors(
      code_vector   = code_vector,
      num_neighbors = num_neighbors,
      margin        = margin,
      type          = type,
      node_type     = node_type
    )

    # Use neighbor node_ids to look up tag names
    if (length(nn$node_ids) == 0) {
      rec_tags <- character(0)
    } else {
      rec_tags <- tag_names %>%
        dplyr::filter(node_id %in% nn$node_ids) %>%
        dplyr::pull(tag_name) %>%
        unique()
    }

    if (length(rec_tags) > 0) {
      # Promote to full hierarchical paths and keep only deepest
      rec_tags <- prune_to_leaf(to_full_path(rec_tags))
      # Remove tags already on this book
      this_existing <- existing_tags_df %>%
        dplyr::filter(id == this_book_id) %>%
        dplyr::pull(existing_tags)
      existing_vec <- if (length(this_existing) > 0) unlist(this_existing, use.names = FALSE) else character(0)
      new_tags <- setdiff(rec_tags, existing_vec)

      if (length(new_tags) > 0) {
        results_rows[[length(results_rows) + 1L]] <- tibble::tibble(
          book_id = as.character(this_book_id),
          tag_name = as.character(new_tags)
        )
      }
    }
  }

  if (length(results_rows) == 0) {
    return(tibble::tibble(book_id = character(0), tag_name = character(0)))
  }

  dplyr::bind_rows(results_rows)
}
```
### test Recommend_tags
```{r}
recs <- recommend_tags(
  margin        = 0,
  num_neighbors = 25,
  type          = "tag",        # or NULL
  node_type     = node_types
)
```

## Given a tag, what books should it go on that don't currently have it?




# End