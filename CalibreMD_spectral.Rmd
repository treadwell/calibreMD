---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

## Options

```{r}
# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

rm(list = ls()) # remove everything in the global environment
gc() # reclaim memory

```

## Load libraries

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()
```

# Load data from SQLite to EAV table

```{r}
dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)

eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features() %>% 
  explode_tags()

# Debug: Print book count
cat("Total unique books in EAV:", length(unique(eav$id)), "\n")
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Build the training graph

```{r}
# unique tags
# unique books
# number tags + book from 1 to n where n = #tags + #books
# provide node type and tag name lookup
# if book has tag, positive edge

# book-tag mapping
book_tags_train <- eav %>%
  filter(feature == "tag") %>%
  mutate(tag_name = value,
         book_id = id) %>%
  select(book_id, tag_name) %>%
  distinct() %>%
  arrange(book_id)

# distinct book_ids and add a node_id (simply 0-n)
book_ids_test <- tibble(book_id = numeric())

book_ids_train <- book_tags_train %>% 
  select(book_id) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number())

# distinct tags and add a node_id (n+1 to...)
tag_names_train <- book_tags_train %>% 
  select(tag_name) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number() + length(book_ids_train$book_id))

# unified node_id to type lookup (answers is a node a book or a tag?)
node_types_train <- data_frame(node_id = seq(nrow(book_ids_train) + nrow(tag_names_train)),
                         type = dplyr::if_else(node_id <= length(book_ids_train$book_id), "book", "tag"))

positive_edges_train <- book_tags_train %>%               # book_id, tag name
  inner_join(book_ids_train,  by = "book_id")  %>%   # adds column `node_id`
  rename(node_a = node_id) %>%                # book → node_a
  inner_join(tag_names_train, by = "tag_name") %>%   # adds 2nd `node_id`
  rename(node_b = node_id) %>%                # tag  → node_b
  select(node_a, node_b)                      # keep only the edge list

g <- igraph::graph_from_data_frame(
        d         = positive_edges_train,
        vertices  = node_types_train$node_id,     # all nodes, even isolates
        directed  = FALSE
      )

cmp <- igraph::components(g)

reps <- tibble(
          vid  = seq_along(cmp$membership),   # vertex index (1…N)
          comp = cmp$membership               # component ID
        ) %>%
        group_by(comp) %>%
        summarise(rep = first(vid), .groups = "drop") %>%  # << first member >>
        pull(rep)                           # plain integer vector

new_edges <- as.vector(rbind(reps[-length(reps)], reps[-1]))

positive_edges_train <- bind_rows(positive_edges_train, tibble(
  node_a = new_edges[c(TRUE,  FALSE)],   # odd positions
  node_b = new_edges[c(FALSE, TRUE)]     # even positions
))

# Create graph with weights (all positive edges have weight 1)
g <- igraph::graph_from_data_frame(
  positive_edges_train %>% mutate(weight = 1), 
  vertices  = node_types_train$node_id,
  directed = FALSE
)
igraph::components(g)$no

# NOTE: abstract testing functions to avoid polluting global scope

```

## Adjacency Matrix
```{r}
# A <- Matrix::sparseMatrix(
#   i = symmetric_edges$node_a,
#   j = symmetric_edges$node_b,
#   x = 1.0,
#   dims = c(nrow(node_types), nrow(node_types)),
#   dimnames = list(node_types$node_id, node_types$node_id)
# )

A <- igraph::as_adjacency_matrix(
        g,
        type = "both",
        sparse = TRUE        # <- returns Matrix::dgCMatrix
      )

head(igraph::V(g)$name)
```

## Laplacian
```{r}
# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_norm <- Matrix::Diagonal(n = nrow(node_types_train)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```

## Get book embeddings

* think of these as ways to separate the space into increasingly higher resolution clusters
* they are binary cuts in the graph
* you need to collectively progress through them to get the finer level clustering

```{r}
# Set number of nontrivial eigenpairs desired
k <- 64

eig <- RSpectra::eigs_sym(
  L_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

eigvecs <- D_inv_sqrt %*% eig$vectors
eigvals <- eig$values

eigvecs <- eigvecs[,-(k+1)]
eigvals <- eigvals[-(k+1)]
ord <- order(eigvals)
eigvals <- eigvals[ord]
eigvecs <- eigvecs[, ord]
eigvals

# to do: build a line graph showing eigenvalues to help select the optimal count

plot(
  seq_along(eigvals),
  eigvals,
  type = "b",
  pch = 19,
  xlab = "Eigenvalue Index",
  ylab = "Eigenvalue",
  main = "Spectrum of Normalized Laplacian"
)
abline(v = which.max(diff(eigvals) > 0.05), col = "red", lty = 2) # optional heuristic marker

# Looks like 7, 10, 12
```
## Visualize graph

```{r}
# library(igraph)
# 
# # Simplify graph for visualization (remove loops and multiple edges)
# g_viz <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = "sum")
# 
# plot(g_viz, layout = as.matrix(eigvecs[, 1:2, drop = FALSE]),
#      vertex.size = 4, vertex.label = NA,
#      edge.color = ifelse(E(g_viz)$weight > 0, "steelblue", "firebrick"),
#      edge.width = abs(E(g_viz)$weight) + .2)
```

## Fn: Iterative quantization

* Rotate the embeddings to minimize quantization error (like PCA?)
* In PCA rotate feature space to explain the most variance
* rotate eigenvectors as if they were raw feature vectors, binarizing at each rotation, and
  trying to match the binary hamming distance to the dot product of the vectors
* you're mapping dot product to hamming distance here
* this allows you to obtain embeddings, continuous from eigenvectors, and binary, from the
  iterative quantization, where dot product (cosine similarity) and hamming distance approximate
  proximity in the graph
* This can be used to find similar books per the underlying graph

```{r}
iterative_quantization <- function(X, n_iter = 25) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: PCA via SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code in {-1, +1}
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes in {0,1} as integer matrix (uint8-like)
  B_final <- sign(X_pca %*% R)
  storage.mode(B_final) <- "integer"
  B01 <- ifelse(B_final > 0L, 1L, 0L)
  mode(B01) <- "integer"
  B01
}

binary_embedding_train <- iterative_quantization(eigvecs)  # uint8-like 0/1 integer matrix
```

### Check binary embeddings
```{r}
cat("binary_embedding dimensions:", dim(binary_embedding_train), "\n")
cat("NA count:", sum(is.na(binary_embedding_train)), "\n")
cat("NaN count:", sum(is.nan(binary_embedding_train)), "\n")
cat("Inf count:", sum(is.infinite(binary_embedding_train)), "\n")
cat("Value range:", range(binary_embedding_train, na.rm = TRUE), "\n")
cat("First 10 rownames:", head(rownames(binary_embedding_train), 10), "\n")
cat("Total rownames:", length(rownames(binary_embedding_train)), "\n")
print(binary_embedding_train[1:5, 1:5])

# Check embedding diversity
cat("\n=== EMBEDDING DIVERSITY ANALYSIS ===\n")

# Count unique binary codes
unique_codes <- unique(as.data.frame(binary_embedding_train))
cat("Unique binary codes:", nrow(unique_codes), "out of", nrow(binary_embedding_train), "books\n")
cat("Diversity ratio:", round(nrow(unique_codes) / nrow(binary_embedding_train), 3), "\n")

# Check eigvecs diversity
# unique_eigvecs <- unique(as.data.frame(round(eigvecs, 6)))
# cat("Unique eigenvector codes (rounded to 6 digits):", nrow(unique_eigvecs), "out of", nrow(eigvecs), "books\n")
# cat("Eigenvector diversity ratio:", round(nrow(unique_eigvecs) / nrow(eigvecs), 3), "\n")

# Check if the problem is in the graph construction
cat("\n=== GRAPH ANALYSIS ===\n")
cat("Books:", nrow(book_ids_train), "\n")
cat("Tags:", nrow(tag_names_train), "\n")
cat("Positive edges:", nrow(positive_edges_train), "\n")
cat("Graph edges (including multiples):", igraph::ecount(g), "\n")

cmp <- igraph::components(g)
cat("Connected components:", cmp$no, "\n")

# Edge diagnostics
edge_ends <- igraph::ends(g, igraph::E(g))
loops_count <- sum(edge_ends[, 1] == edge_ends[, 2])
multiples_count <- sum(igraph::which_multiple(g))
cat("Loops:", loops_count, "\n")
cat("Multiple edges:", multiples_count, "\n")

# Degree by node type
deg <- igraph::degree(g, mode = "all")
deg_df <- tibble::tibble(node_id = as.integer(names(deg)), degree = as.numeric(deg)) |>
  dplyr::left_join(node_types_train, by = "node_id")

cat("\nDegree summary (books):\n")
print(
  deg_df |>
    dplyr::filter(type == "book") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

cat("\nDegree summary (tags):\n")
print(
  deg_df |>
    dplyr::filter(type == "tag") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

# Top tags by connectivity
top_tags <- deg_df |>
  dplyr::filter(type == "tag") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(tag_names_train, by = "node_id") |>
  dplyr::select(tag_name, degree)
cat("\nTop 10 tags by degree:\n")
print(top_tags)

# Top books by connectivity
top_books <- deg_df |>
  dplyr::filter(type == "book") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(book_ids_train, by = "node_id") |>
  dplyr::select(book_id, degree)
cat("\nTop 10 books by degree:\n")
print(top_books)

# Check tag distribution
# Determine whether a tag value existed in the original metadata (pre-explosion)
original_tag_values <- md_db %>%
  load_eav() %>%
  dplyr::filter(feature == "tag") %>%
  dplyr::distinct(value) %>%
  dplyr::pull(value)

tag_counts <- eav %>% 
  dplyr::filter(feature == "tag") %>% 
  dplyr::count(value, sort = TRUE) %>%
  dplyr::mutate(tag_type = ifelse(value %in% original_tag_values, "original", "exploded"))

cat("Top 10 most common tags:\n")
print(tag_counts %>% head(10))
```

## Fn: Unified metadata builder

```{r}
# Build a single metadata tibble with unified lookups by node_id
# Columns: node_id, node_type, row_id, book_id, book_title, tag_name, tag_origin
build_unified_metadata <- function(eav, 
                                   book_ids, 
                                   tag_names = NULL, 
                                   node_types = NULL, 
                                   binary_embedding = NULL, 
                                   md_db = NULL) {
  
  # Titles by book_id
  titles <- eav %>%
    dplyr::filter(feature == "title_original") %>%
    dplyr::distinct(id, .keep_all = TRUE) %>%
    dplyr::transmute(book_id = as.integer(id), book_title = as.character(value))

  # Original tag values from source DB (pre-explosion)
  original_tag_values <- NULL
  if(!is.null(md_db)){
    original_tag_values <- md_db %>%
      load_eav() %>%
      dplyr::filter(feature == "tag") %>%
      dplyr::distinct(value) %>%
      dplyr::pull(value) %>%
      as.character()
  }
  if(is.null(node_types)){  # test data
    # Book-side metadata
    unified <- book_ids %>%
      dplyr::transmute(book_id = as.integer(book_id)) %>%
      dplyr::left_join(titles, by = "book_id")
    
  } else {  # training data
    nodes <- tibble::tibble(
      node_id   = as.integer(node_types$node_id),
      node_type = as.character(node_types$type))

    # row_id mapping: by construction row index/order equals node_id
    row_map <- tibble::tibble(
      node_id = as.integer(node_types$node_id),
      row_id  = as.integer(node_types$node_id))

    # Book-side metadata
    books_meta <- book_ids %>%
      dplyr::transmute(node_id = as.integer(node_id), book_id = as.integer(book_id)) %>%
      dplyr::left_join(titles, by = "book_id")
  
    # Tag-side metadata
    tags_meta <- tag_names %>%
      dplyr::transmute(node_id = as.integer(node_id), tag_name = as.character(tag_name)) %>%
      dplyr::mutate(tag_origin = ifelse(tag_name %in% original_tag_values, "original", "exploded"))
  
    # Unified
    unified <- nodes %>%
      dplyr::left_join(row_map,  by = "node_id") %>%
      dplyr::left_join(books_meta, by = "node_id") %>%
      dplyr::left_join(tags_meta,  by = "node_id") %>%
      dplyr::arrange(node_id)
  }

  unified
}

# Example:
unified_meta_train <- build_unified_metadata(eav, book_ids_train, tag_names_train, node_types_train, binary_embedding_train, md_db)
unified_meta_test <- build_unified_metadata(eav, book_ids_test)
```

# Get new tags

## Fn: Get existing tags for each book

```{r}
get_existing_tags <- function(eav) {
  eav %>% 
    filter(feature == "tag") %>% 
    select(book_id = id, tag_name = value)
}
```

## Fn: Get nearest neighbors

```{r}
# Simple Hamming-based nearest neighbors (pure R, no Python interop)
get_nearest_neighbors <- function(code_vector,
                                  index = NULL,
                                  binary_embedding = NULL,
                                  unified_meta,
                                  num_neighbors = NULL,
                                  margin = NULL,
                                  type = NULL) {
  stopifnot(is.numeric(code_vector))
  if (is.null(index)) stopifnot(is.matrix(binary_embedding))

  # If a prebuilt Hamming index is provided, use it
  if (!is.null(index)) {
    res <- hamming_search(
      index        = index,
      code_vector  = code_vector,
      k            = num_neighbors,
      margin       = margin,
      type         = type,
      unified_meta = unified_meta
    )
    return(res)
  }

  # Efficient Hamming distance for 0/1 data without large temporaries
  sx <- rowSums(binary_embedding)
  sy <- sum(code_vector)
  dot <- as.numeric(binary_embedding %*% code_vector)
  distances <- sx + sy - 2 * dot

  # Candidate set
  candidates <- seq_len(nrow(binary_embedding))

  # Type filter: row index equals node_id by construction
  if (!is.null(type)) {
    stopifnot(all(c("row_id", "node_type") %in% names(unified_meta)))
    row_types <- unified_meta$node_type[match(seq_len(nrow(binary_embedding)), unified_meta$row_id)]
    candidates <- candidates[!is.na(row_types) & row_types == type]
  }

  # Margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(row_ids = integer(0), distances = numeric(0)))

  # Order by distance then index
  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  # Top-k
  if (!is.null(num_neighbors) && length(candidates) > num_neighbors) {
    candidates <- candidates[seq_len(as.integer(num_neighbors))]
  }

  list(row_ids = candidates, distances = distances[candidates])
}
```

## Fn: Hamming index

```{r}
# Build a queryable flat Hamming index from a 0/1 integer matrix
# Returns a lightweight list with matrix and node_id mapping
build_hamming_index <- function(binary_embedding, unified_meta = NULL) {
  stopifnot(is.matrix(binary_embedding))
  if (!is.null(unified_meta)) {
    stopifnot("row_id" %in% names(unified_meta))
  }
  list(
    data = binary_embedding,
    metadata = unified_meta
  )
}

# Query the flat Hamming index
hamming_search <- function(index,
                           code_vector,
                           k = NULL,
                           margin = NULL,
                           type = NULL,
                           unified_meta = NULL) {
  stopifnot(is.list(index), is.matrix(index$data), is.numeric(code_vector))
  X <- index$data
  metadata <- if (!is.null(unified_meta)) unified_meta else index$metadata
  # Efficient Hamming distances for 0/1 data:
  # d(x,y) = sum(x) + sum(y) - 2 * x·y
  sx <- rowSums(X)
  sy <- sum(code_vector)
  dot <- as.numeric(X %*% code_vector)
  distances <- sx + sy - 2 * dot

  candidates <- seq_len(nrow(X))

  # Optional type filter using unified metadata mapping
  if (!is.null(type)) {
    stopifnot(!is.null(metadata), all(c("row_id", "node_type") %in% names(metadata)))
    cand_types <- metadata$node_type[match(candidates, metadata$row_id)]
    keep <- !is.na(cand_types) & cand_types == type
    candidates <- candidates[keep]
  }

  # Optional margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(row_ids = integer(0), distances = numeric(0)))

  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  if (!is.null(k) && length(candidates) > k) {
    candidates <- candidates[seq_len(as.integer(k))]
  }

  list(row_ids = candidates, distances = distances[candidates])
}

# Compute neighborhoods for all rows using the index (vectorized by blocks)
all_neighborhoods <- function(index,
                              k,
                              margin = NULL,
                              type = NULL,
                              unified_meta = NULL,
                              block_size = 512) {
  stopifnot(is.list(index), is.matrix(index$data))
  X <- index$data
  n <- nrow(X)
  metadata <- if (!is.null(unified_meta)) unified_meta else index$metadata

  # Precompute sums for fast Hamming
  sx <- rowSums(X)

  # Optional type mask
  type_mask <- rep(TRUE, n)
  if (!is.null(type)) {
    stopifnot(!is.null(metadata), all(c("row_id", "node_type") %in% names(metadata)))
    types <- metadata$node_type[match(seq_len(n), metadata$row_id)]
    type_mask <- !is.na(types) & types == type
  }

  results_ids <- vector("list", n)
  results_dist <- vector("list", n)

  for (start in seq(1, n, by = block_size)) {
    end <- min(start + block_size - 1L, n)
    Q <- X[start:end, , drop = FALSE]
    sy <- rowSums(Q)
    dot <- X %*% t(Q)                   # n x b
    # Hamming distances for all pairs in block
    # d = sx + sy - 2*dot, broadcasting sy by column
    D <- sweep(matrix(sx, nrow = n, ncol = ncol(dot)), 2, sy, `+`) - 2 * dot

    # Apply optional type and margin filters
    if (!all(type_mask)) {
      D[!type_mask, ] <- Inf
    }
    if (!is.null(margin)) {
      D[D > margin] <- Inf
    }

    # For each query in block, take top-k
    for (j in seq_len(ncol(D))) {
      dcol <- D[, j]
      ord <- order(dcol, seq_len(n), na.last = NA)
      if (is.finite(k)) {
      ord <- ord[seq_len(min(k, length(ord)))]
      }
      results_ids[[start + j - 1L]] <- ord
      results_dist[[start + j - 1L]] <- dcol[ord]
    }
  }

  list(row_ids = results_ids, distances = results_dist)
}
```

## Fn: Recommend tags
```{r}
# Assumes you have:
# - binary_embedding : matrix
# - book_ids         : data.frame/tibble with columns node_id, book_id
# - tag_names        : data.frame/tibble with columns node_id, tag_name
# - (optional) node_type : data.frame/tibble with columns node_id, type
# - get_nearest_neighbors() from the previous message

recommend_tags <- function(margin,
                           num_neighbors = NULL,
                           distance_cap = NULL,
                           type = NULL,
                           unified_meta,
                           book_ids,
                           tag_names,
                           existing_tags_df,
                           binary_embedding,
                           index = NULL) {
  stopifnot("book_id" %in% names(book_ids))
  stopifnot(all(c("node_id", "tag_name") %in% names(tag_names)))
  stopifnot(all(c("node_id", "tag_name") %in% names(tag_names)))
  stopifnot(!is.null(index))

  results_rows <- list()
  # existing_tags_df provided by caller

  # Helper: promote bare segment to its longest hierarchical path found in tag_names
  to_full_path <- function(tag_vector) {
    # Fast path: if all contain '.', return as-is
    if (all(grepl("\\.", tag_vector))) return(tag_vector)

    universe <- tag_names$tag_name
    # Precompute last segments and path depths for the universe
    universe_splits <- strsplit(universe, "\\.")
    universe_last   <- vapply(universe_splits, function(x) if (length(x)) x[length(x)] else "", character(1))
    universe_depth  <- vapply(universe_splits, length, integer(1))

    promote_one <- function(t) {
      if (is.na(t) || t == "") return(t)
      if (grepl("\\.", t)) return(t)
      idx <- which(universe_last == t)
      if (length(idx) == 0) return(t)
      # Choose deepest path; break ties by shortest string
      idx <- idx[order(-universe_depth[idx], nchar(universe[idx]))]
      universe[idx[1]]
    }
    vapply(tag_vector, promote_one, FUN.VALUE = character(1))
  }

  # Helper: keep only the most specific (leaf) paths; drop ancestors
  prune_to_leaf <- function(paths) {
    paths <- unique(paths)
    if (length(paths) <= 1) return(paths)
    keep <- !vapply(paths, function(t) any(paths != t & startsWith(paths, paste0(t, "."))), logical(1))
    paths[keep]
  }

  # Compute neighborhoods once using the index
  k_val <- if (is.null(num_neighbors)) nrow(index$data) else as.integer(num_neighbors)
  all_nbrs <- all_neighborhoods(
    index        = index,
    k            = k_val,
    margin       = margin,
    type         = type,
    unified_meta = unified_meta
  )

  # Map each book to its row_id (which aligns with index rows)
  if ("row_id" %in% names(book_ids)) {
    book_row_ids <- book_ids$row_id
  } else if ("node_id" %in% names(book_ids)) {
    book_row_ids <- unified_meta$row_id[match(book_ids$node_id, unified_meta$node_id)]
  } else {
    stop("`book_ids` must contain either `row_id` or `node_id`.")
  }

  existing_lookup <- existing_tags_df %>%
    dplyr::group_by(book_id) %>%
    dplyr::summarise(existing_tags = list(tag_name), .groups = "drop")

  for (i in seq_along(book_row_ids)) {
    row_idx <- book_row_ids[i]
    if (is.na(row_idx) || row_idx <= 0L) next
    this_book_id <- book_ids$book_id[i]
  
    nbr_rows   <- all_nbrs$row_ids[[row_idx]]
    nbr_dists <- all_nbrs$distances[[row_idx]]
    if (length(nbr_rows) == 0) next
  
    # Map neighbors to tags and carry distances
    df <- tibble::tibble(
      row_id   = nbr_rows,
      distance = nbr_dists
    )
  
    df$tag_raw <- unified_meta$tag_name[match(df$row_id, unified_meta$row_id)]
    df <- df[!is.na(df$tag_raw) & nzchar(df$tag_raw), , drop = FALSE]
    if (nrow(df) == 0) next
  
    # To full paths, prune to leaves
    df$tag_full <- to_full_path(df$tag_raw)
    pruned_set  <- prune_to_leaf(df$tag_full)
    df <- df[df$tag_full %in% pruned_set, , drop = FALSE]
    if (nrow(df) == 0) next
  
    # If multiple neighbors map to same tag, keep the closest (min distance)
    df_tags <- df |>
      dplyr::group_by(tag_name = .data$tag_full) |>
      dplyr::summarise(distance = min(.data$distance), .groups = "drop")
    if (!is.null(distance_cap)) {
      df_tags <- df_tags |>
        dplyr::filter(.data$distance <= distance_cap)
      if (nrow(df_tags) == 0) next
    }

    # Remove tags already on this book
    this_existing <- existing_lookup$existing_tags[existing_lookup$book_id == this_book_id]
    existing_vec  <- if (length(this_existing) > 0) unlist(this_existing[[1]], use.names = FALSE) else character(0)
  
    df_new <- dplyr::anti_join(
      df_tags,
      tibble::tibble(tag_name = existing_vec),
      by = "tag_name"
    )
  
    if (nrow(df_new) > 0) {
      results_rows[[length(results_rows) + 1L]] <- tibble::tibble(
        book_id  = as.character(this_book_id),
        tag_name = as.character(df_new$tag_name),
        distance = as.numeric(df_new$distance)
      )
    }
  }

  if (length(results_rows) == 0) {
    return(tibble::tibble(book_id = character(0), tag_name = character(0)))
  }

  dplyr::bind_rows(results_rows)
}
```
## FN: Summarize recommendations
```{r}
#' Summarize tag actions (add / remove / keep) per book
#'
#' @param recs           data.frame with at least: book_id, tag_name
#' @param existing_tags  data.frame with at least: book_id, tag_name (optional; may be NULL)
#'                       If NULL, assumes no existing tags for any books.
#' @param unified_meta   data.frame with at least: book_id, book_title
#'
#' @return data.frame with columns: book_id, book_title, tag_name, action
#'         where action ∈ { "add", "remove", "keep" }
#'
#' @examples
#' actions_df <- summarize_tag_actions(recs, existing_tags, unified_meta)
summarize_tag_actions <- function(recs, existing_tags = NULL, unified_meta) {
  stopifnot(is.data.frame(recs), is.data.frame(unified_meta))
  
  # --- Validate columns ---
  if (!all(c("book_id", "tag_name") %in% names(recs))) {
    stop("`recs` must contain columns: book_id, tag_name")
  }
  if (!all(c("book_id", "book_title") %in% names(unified_meta))) {
    stop("`unified_meta` must contain columns: book_id, book_title")
  }
  
  # --- Normalize ---
  recs_norm <- recs |>
    dplyr::transmute(
      book_id  = as.character(.data$book_id),
      tag_name = as.character(.data$tag_name)
    ) |>
    dplyr::distinct()
  
  if (is.null(existing_tags)) {
    # If no tags exist, all recommendations are "add"
    existing_norm <- dplyr::tibble(book_id = character(), tag_name = character())
  } else {
    if (!all(c("book_id", "tag_name") %in% names(existing_tags))) {
      stop("`existing_tags` must contain columns: book_id, tag_name")
    }
    existing_norm <- existing_tags |>
      dplyr::transmute(
        book_id  = as.character(.data$book_id),
        tag_name = as.character(.data$tag_name)
      ) |>
      dplyr::distinct()
  }
  
  # --- Compute sets ---
  keep_df <- dplyr::inner_join(existing_norm, recs_norm,
                               by = c("book_id", "tag_name")) |>
    dplyr::mutate(action = "keep")
  
  add_df <- dplyr::anti_join(recs_norm, existing_norm,
                             by = c("book_id", "tag_name")) |>
    dplyr::mutate(action = "add")
  
  remove_df <- dplyr::anti_join(existing_norm, recs_norm,
                                by = c("book_id", "tag_name")) |>
    dplyr::mutate(action = "remove")
  
  out <- dplyr::bind_rows(keep_df, add_df, remove_df)
  
  # --- Attach titles ---
  titles_norm <- unified_meta |>
    dplyr::transmute(
      book_id    = as.character(.data$book_id),
      book_title = as.character(.data$book_title)
    ) |>
    dplyr::distinct()
  
  out <- out |>
    dplyr::left_join(titles_norm, by = "book_id") |>
    dplyr::relocate(.data$book_title, .after = .data$book_id)
  
  # --- Order by book / action / tag ---
  action_order <- c(keep = 1L, add = 2L, remove = 3L)
  
  out |>
    dplyr::mutate(.action_rank = action_order[.data$action]) |>
    dplyr::arrange(.data$book_id, .data$.action_rank, .data$tag_name) |>
    dplyr::select(.data$book_id, .data$book_title, .data$tag_name, .data$action)
}
```

# Inference

## FN: Train encoder
```{r}
train_encoder <- function(X, Y){
  requireNamespace("xgboost", quietly = TRUE)
  models <- list()
  for (bit in seq_len(ncol(Y))) {
    print(paste("bit", bit, "training"))
    # Extract label vector and convert to numeric
    y <- as.numeric(Y[, bit])
    # Construct DMatrix using sparse feature matrix
    dtrain <- xgboost::xgb.DMatrix(data = X, label = y)
    models[[bit]] <- xgboost::xgboost(
      data = dtrain,
      objective = "binary:logistic",
      eval_metric = "logloss",
      nrounds = 50,
      verbose = 0
    )
  }
  models
}
```

## FN: Encode books
```{r}
encode_books <- function(models, X, threshold = 0.5) {
  requireNamespace("xgboost", quietly = TRUE)

  n_bits <- length(models)
  n_obs  <- nrow(X)

  dX <- xgboost::xgb.DMatrix(data = X)
  out <- matrix(0L, nrow = n_obs, ncol = n_bits)

  for (b in seq_len(n_bits)) {
    preds <- stats::predict(models[[b]], newdata = dX)

    # Convert logits to probabilities if outside [0,1]
    if (any(preds < 0 | preds > 1, na.rm = TRUE)) {
      preds <- plogis(preds)
    }

    out[, b] <- as.integer(preds >= threshold)
  }

  colnames(out) <- if (!is.null(names(models)) && all(nzchar(names(models)))) {
    names(models)
  } else {
    paste0("bit_", seq_len(n_bits))
  }

  out
}
```

## FN: Evaluate projections
```{r}
evaluate_projection <- function(Y0, Y1, prefix = 0){
  # Y0, Y1: integer (0/1) matrices with identical dims; rows = nodes, cols = bits
  if (!is.matrix(Y0) || !is.matrix(Y1)) stop("Y0 and Y1 must be matrices.")
  if (!all(dim(Y0) == dim(Y1))) stop("Y0 and Y1 must have the same dimensions.")
  if (anyNA(Y0) || anyNA(Y1)) stop("Y0 and Y1 must not contain NA values.")
  if (!all(Y0 %in% c(0L, 1L)) || !all(Y1 %in% c(0L, 1L)))
    stop("Y0 and Y1 must contain only 0/1 values.")

  p <- ncol(Y0)
  if (p == 0L) stop("Matrices must have at least one column (bit).")

  # determine number of bits to compare
  if (prefix <= 0) {
    k <- p
  } else {
    if (prefix > p) stop("prefix cannot be larger than the number of columns")
    k <- prefix
  }

  # restrict matrices to first k columns
  Y0_sub <- Y0[, seq_len(k), drop = FALSE]
  Y1_sub <- Y1[, seq_len(k), drop = FALSE]

  # Hamming distance per row = number of differing bits
  avg_frac_dist <- mean(rowSums(abs(Y0_sub - Y1_sub))) / k

  # Normalize to [0,1] so that 1.0 means avg Hamming distance = 0
  score <- 1 - avg_frac_dist
  as.numeric(score)
}

```

## FN: Prep data set
```{r}
prep_encoder_data <- function(eav, unified_meta, binary_embedding = NULL, indexed_features = NULL) {
  stopifnot("book_id" %in% names(unified_meta))
  
  if(is.null(binary_embedding)){
    stopifnot(!is.null(indexed_features))
    ids <- unified_meta %>%
      dplyr::distinct(book_id) %>%
      dplyr::pull(book_id)
    book_features <- unified_meta %>%
      dplyr::select(book_id) %>%
      dplyr::inner_join(
        eav,
        by = dplyr::join_by(book_id == id)
      ) %>%
      dplyr::filter(!.data$feature %in% c("tag", "title_original")) %>%
      dplyr::select(book_id, feature, value) %>%
      dplyr::distinct()
    row_index <- match(book_features$book_id, ids)
    col_index <- match(
      paste0(book_features$feature, "=", as.character(book_features$value)),
      indexed_features$key)
    Y <- NULL
  } else {
    ids <- unified_meta %>%
      dplyr::filter(node_type == "book") %>%
      dplyr::arrange(.data$node_id) %>%
      dplyr::pull(.data$node_id)
    Y <- binary_embedding[ids, , drop = FALSE]
    book_features <- eav %>%
      dplyr::inner_join(
        unified_meta %>% 
          dplyr::filter(node_type == "book") %>% 
          dplyr::select(book_id, node_id),
        by = dplyr::join_by(id == book_id)
      ) %>%
      dplyr::filter(!.data$feature %in% c("tag", "title_original")) %>%
      dplyr::select(node_id, feature, value) %>%
      dplyr::distinct()
    indexed_features <- book_features %>%
      dplyr::mutate(key = paste0(.data$feature, "=", as.character(.data$value))) %>%
      dplyr::distinct(.data$feature, .data$value, .data$key) %>%
      dplyr::arrange(.data$feature, .data$value) %>%
      dplyr::mutate(id_feature = dplyr::row_number()) %>%
      dplyr::select(.data$feature, .data$value, .data$key, .data$id_feature)
    row_index <- match(book_features$node_id, ids)
    col_index <- match(
      paste0(book_features$feature, "=", as.character(book_features$value)),
      indexed_features$key)
  }
  if (any(is.na(row_index))) {
    warning("Dropped ", sum(is.na(row_index)), " book-feature rows with unknown book_id.")
  }
  keep_idx <- !is.na(row_index) & !is.na(col_index)
  row_index <- row_index[keep_idx]
  col_index <- col_index[keep_idx]
  X <- Matrix::sparseMatrix(
    i = row_index,
    j = col_index,
    x = 1,
    dims = c(length(ids), nrow(indexed_features)),
    dimnames = list(ids, indexed_features$key))
  list(
    ids = ids,
    features = indexed_features,  # dictionary of columns in X
    X = X,                        # sparse one-hot feature matrix (ids x feature=value)
    Y = Y)
}
```


## Build X and Y
```{r}
print("Building train_data (expects node_id mapping)")
train_data <- prep_encoder_data(eav, unified_meta_train, binary_embedding_train)
# test_data <- prep_encoder_data(eav, unified_meta_test, NULL, train_data$features)
```
# Train Encoder
```{r}
# predict spectral codes from the raw features
model <- train_encoder(train_data$X, train_data$Y)  # Y is spectral codes

# project spectral codes from training data using the encoder
train_data$Y_projected <- encode_books(model, train_data$X)
# test_data$Y_projected <- encode_books(model, test_data$X)
```

## Evaluate (measure hamming between in-sample spectral and in-sample projected codes)

* How well do the codes we're predicting match the actual codes?
* This is self-taught hashing
* We need to increase the accuracy...

```{r}
train_acc <- evaluate_projection(train_data$Y, train_data$Y_projected, 1)
print(paste("Training accuracy", train_acc))

# Also build functions to do a manual check
```

## Cache encoder artifacts and tag codes
```{r}
encoder_features <- train_data$features
encoder_models <- model

tag_meta_combined <- unified_meta_train %>%
  dplyr::filter(node_type == "tag") %>%
  dplyr::arrange(row_id) %>%
  dplyr::mutate(
    row_id_train = row_id,
    row_id = dplyr::row_number(),
    source = "tag_spectral"
  )

bit_names <- paste0("bit_", seq_len(ncol(binary_embedding_train)))
colnames(binary_embedding_train) <- bit_names
tag_codes <- binary_embedding_train[tag_meta_combined$row_id_train, , drop = FALSE]
colnames(tag_codes) <- bit_names
```

## Build projected codes for all books
```{r}
book_ids_all <- eav %>%
  dplyr::filter(feature == "title_original") %>%
  dplyr::distinct(id) %>%
  dplyr::arrange(id) %>%
  dplyr::rename(book_id = id)

book_titles_all <- eav %>%
  dplyr::filter(feature == "title_original") %>%
  dplyr::transmute(book_id = as.integer(id), book_title = as.character(value)) %>%
  dplyr::distinct()

unified_meta_all_books <- book_ids_all %>%
  dplyr::mutate(book_id = as.integer(book_id)) %>%
  dplyr::left_join(book_titles_all, by = "book_id")

stopifnot("book_id" %in% names(unified_meta_all_books))

print("Building all_data (expects plain book_id metadata)")
all_data <- prep_encoder_data(
  eav              = eav,
  unified_meta     = unified_meta_all_books,
  binary_embedding = NULL,
  indexed_features = encoder_features
)

book_codes_projected <- encode_books(encoder_models, all_data$X)
colnames(book_codes_projected) <- bit_names
```

## Assemble unified codes and metadata
```{r}
codes_all <- rbind(tag_codes, book_codes_projected)

book_meta_all <- tibble::tibble(
  book_id = all_data$ids,
  row_id = seq_len(length(all_data$ids)) + nrow(tag_codes),
  node_type = "book",
  source = "book_projected"
) %>%
  dplyr::left_join(unified_meta_all_books, by = "book_id")

unified_meta_all <- dplyr::bind_rows(
  tag_meta_combined %>%
    dplyr::select(row_id, node_type, source, tag_name),
  book_meta_all %>%
    dplyr::select(row_id, node_type, source, book_id, book_title)
)
```

Done
1. generate graph
2. generate in-sample codes
3. learn projections
4. project codes
5. evaluate accuracy of projected codes

Next steps
6. find the margin that best captures ground-truth labeling for in-sample codes
7. find the margin that best captures ground-truth labeling for projected codes
8. for each in-sample book, considering the known labels as the first list, create a 2nd and 3rd labels list using the in-sample codes plus margin and projected codes plus margin
9. for each in-sample book compute Jaccard similarity between the ground truth label set and each of the two sets above (labels from the in-sample codes, and the projected codes). Display the worst offenders per list and overall Jaccard similarity per list.
10. for each in-sample tag, considering the known books as the first list, create a 2nd and 3rd books list using the in-sample codes plus margin and projected codes plus margin.
11. for each in-sample tag compute Jaccard similarity between the ground truth book set and each of the two sets above (books from the in-sample codes, and the projected codes). Display the worst offenders per list and overall Jaccard similarity per list.

## 6. find the margin that best captures ground-truth labeling for in-sample codes

* This works with the in-sample books only.
```{r}
## 6. Find the integer Hamming threshold that best captures ground-truth labeling (in-sample codes)

suppressPackageStartupMessages({
  library(dplyr); library(tibble); library(Matrix)
})

# ---- Guards & setup ---------------------------------------------------------
stopifnot(
  exists("train_data", inherits = FALSE),
  !is.null(train_data$Y), !is.null(train_data$ids),
  exists("book_tags_train", inherits = FALSE),
  exists("book_ids_train",  inherits = FALSE),
  exists("tag_names_train", inherits = FALSE)
)

Y <- as.matrix(train_data$Y)  # expected 0/1 codes (n x kbits)
if (!all(Y %in% c(0,1))) stop("train_data$Y must be binary (0/1) for integer Hamming search.")
storage.mode(Y) <- "integer"

n_books <- nrow(Y)
kbits   <- ncol(Y)            # e.g., 64

# ---- Ground truth: two books are same-label if they share ≥1 training tag ---
# Align rows of Y to node_ids
row_index_map <- tibble(node_id = as.integer(train_data$ids),
                        row_ix  = seq_len(n_books))

# Map training (book_id, tag_name) -> Y row_ix via node_id
bt_train <- book_tags_train %>%
  inner_join(book_ids_train, by = "book_id") %>%
  select(node_id, tag_name) %>%
  inner_join(row_index_map, by = "node_id")

# Stable tag order (not strictly required, but useful)
tag_levels <- tag_names_train %>% arrange(node_id) %>% pull(tag_name)

bt_train <- bt_train %>%
  mutate(col_ix = match(tag_name, tag_levels)) %>%
  filter(!is.na(row_ix), !is.na(col_ix))

n_tags <- length(tag_levels)

# Sparse book–tag incidence M (n_books x n_tags)
M <- Matrix::sparseMatrix(
  i = bt_train$row_ix, j = bt_train$col_ix, x = 1L,
  dims = c(n_books, n_tags)
)

# Co-membership matrix G: TRUE if share ≥ 1 tag (ignore diagonal)
G <- (M %*% Matrix::t(M)) > 0
diag(G) <- FALSE

# ---- Integer Hamming distances (bits that differ) ---------------------------
# For 0/1 Y: d_H(i,j) = ones_i + ones_j - 2 * (Y_i · Y_j)
ones <- Matrix::rowSums(Y)
S    <- Y %*% Matrix::t(Y)  # shared 1-bits (n x n)
D    <- (matrix(ones, n_books, n_books) +
         matrix(ones, n_books, n_books, byrow = TRUE)) - 2 * S
D    <- as.matrix(D)
diag(D) <- NA_integer_

# Upper triangle vectors for evaluation
upper_idx <- upper.tri(D, diag = FALSE)
dist_vec  <- as.numeric(D[upper_idx])         # integer in [0..kbits]
truth_vec <- as.logical(G[upper_idx])         # ground truth (same-label)
# Also keep similarity vector for downstream compatibility (NOT used here)
sim_vec   <- 1 - (dist_vec / kbits)

# ---- Sweep integer thresholds k = 0..kbits ----------------------------------
k_vals <- 0:kbits
prec <- rec <- f1 <- numeric(length(k_vals))

for (i in seq_along(k_vals)) {
  k_th <- k_vals[i]                 # threshold = max differing bits allowed
  pred <- dist_vec <= k_th
  TP <- sum(pred &  truth_vec, na.rm = TRUE)
  FP <- sum(pred & !truth_vec, na.rm = TRUE)
  FN <- sum(!pred &  truth_vec, na.rm = TRUE)

  prec[i] <- if ((TP + FP) > 0) TP / (TP + FP) else NA_real_
  rec[i]  <- if ((TP + FN) > 0) TP / (TP + FN) else NA_real_
  f1[i]   <- if (is.na(prec[i]) || is.na(rec[i]) || (prec[i] + rec[i]) == 0)
               NA_real_ else 2 * prec[i] * rec[i] / (prec[i] + rec[i])
}

best_i  <- which.max(f1)
best_k  <- k_vals[best_i]          # ≤ best_k differing bits
best_f1 <- f1[best_i]

# Accuracy at the chosen integer threshold
pred_best <- dist_vec <= best_k
acc_best  <- mean(pred_best == truth_vec)

# For later sections that expect a similarity margin in [0,1], provide equivalent:
best_margin <- 1 - (best_k / kbits)  # DO NOT use for plotting here

cat(sprintf("Best integer Hamming threshold: ≤ %d of %d bits differ | F1 = %.3f | Accuracy = %.3f\n",
            best_k, kbits, best_f1, acc_best))

# ---- Single chart: Precision / Recall / F1 vs Hamming distance (colored) ----
plot(k_vals, f1, type = "l", lwd = 2, col = "firebrick",
     ylim = c(0, 1),
     xlab = "Maximum Hamming distance (bits that may differ)",
     ylab = "Score",
     main = "Precision, Recall, and F1 vs Hamming Distance Threshold")
lines(k_vals, prec, lwd = 2, col = "steelblue")
lines(k_vals, rec,  lwd = 2, col = "darkgreen")
abline(v = best_k, lty = 2, col = "gray40")
legend("bottomright",
       legend = c("F1", "Precision", "Recall", sprintf("Best ≤ %d bits", best_k)),
       col    = c("firebrick", "steelblue", "darkgreen", "gray40"),
       lty    = c(1,1,1,2), lwd = c(2,2,2,1), bty = "n")

```
## 7. find the margin that best captures ground-truth labeling for projected codes

* Looks at projected codes, so for when we take new books and project them.
* Recognize that the margin will be larger and there will be more false positives.

```{r}
## 7. Find the integer Hamming threshold that best captures ground-truth labeling (PROJECTED in-sample codes)

suppressPackageStartupMessages({
  library(dplyr); library(tibble); library(Matrix)
})

# ---- Guards -----------------------------------------------------------------
stopifnot(
  exists("train_data", inherits = FALSE),
  !is.null(train_data$ids),
  !is.null(train_data$Y_projected)
)

# Use PROJECTED codes; binarize at 0.5 if needed for Hamming
Yp <- as.matrix(train_data$Y_projected)
if (!all(Yp %in% c(0,1))) {
  Yp <- (Yp >= 0.5) * 1L
}
storage.mode(Yp) <- "integer"

n_books_p <- nrow(Yp)
kbits_p   <- ncol(Yp)

# ---- Ground truth (reuse from Section 6 or rebuild) -------------------------
if (!exists("G", inherits = FALSE)) {
  stopifnot(
    exists("book_tags_train", inherits = FALSE),
    exists("book_ids_train",  inherits = FALSE),
    exists("tag_names_train", inherits = FALSE)
  )
  row_index_map <- tibble(node_id = as.integer(train_data$ids),
                          row_ix  = seq_len(n_books_p))

  bt_train <- book_tags_train %>%
    inner_join(book_ids_train, by = "book_id") %>%
    select(node_id, tag_name) %>%
    inner_join(row_index_map, by = "node_id")

  tag_levels <- tag_names_train %>% arrange(node_id) %>% pull(tag_name)

  bt_train <- bt_train %>%
    mutate(col_ix = match(tag_name, tag_levels)) %>%
    filter(!is.na(row_ix), !is.na(col_ix))

  n_tags <- length(tag_levels)

  M <- Matrix::sparseMatrix(
    i = bt_train$row_ix, j = bt_train$col_ix, x = 1L,
    dims = c(n_books_p, n_tags)
  )

  G <- (M %*% Matrix::t(M)) > 0
  diag(G) <- FALSE
}

# Pairwise upper-triangle ground-truth vector (same-label)
upper_idx_p <- upper.tri(G, diag = FALSE)
truth_vec   <- as.logical(G[upper_idx_p])   # reuse name for compatibility

# ---- Integer Hamming distances for PROJECTED codes --------------------------
# d_H(i,j) = ones_i + ones_j - 2 * (Y_i · Y_j)
ones_p <- Matrix::rowSums(Yp)
S_p    <- Yp %*% Matrix::t(Yp)
D_p    <- (matrix(ones_p, n_books_p, n_books_p) +
           matrix(ones_p, n_books_p, n_books_p, byrow = TRUE)) - 2 * S_p
D_p    <- as.matrix(D_p)
diag(D_p) <- NA_integer_

dist_vec_p <- as.numeric(D_p[upper_idx_p])          # integers 0..kbits_p
# Similarity kept only for downstream compatibility (not used for plotting)
sim_vec_p  <- 1 - (dist_vec_p / kbits_p)

stopifnot(is.numeric(dist_vec_p), all(dist_vec_p >= 0), all(dist_vec_p <= kbits_p))

# ---- Sweep integer thresholds k = 0..kbits_p --------------------------------
k_vals_p <- 0:kbits_p
prec_p <- rec_p <- f1_p <- numeric(length(k_vals_p))

for (i in seq_along(k_vals_p)) {
  k_th <- k_vals_p[i]
  pred <- dist_vec_p <= k_th
  TP <- sum(pred &  truth_vec, na.rm = TRUE)
  FP <- sum(pred & !truth_vec, na.rm = TRUE)
  FN <- sum(!pred &  truth_vec, na.rm = TRUE)

  prec_p[i] <- if ((TP + FP) > 0) TP / (TP + FP) else NA_real_
  rec_p[i]  <- if ((TP + FN) > 0) TP / (TP + FN) else NA_real_
  f1_p[i]   <- if (is.na(prec_p[i]) || is.na(rec_p[i]) || (prec_p[i] + rec_p[i]) == 0)
                 NA_real_ else 2 * prec_p[i] * rec_p[i] / (prec_p[i] + rec_p[i])
}

best_i_p   <- which.max(f1_p)
best_k_p   <- k_vals_p[best_i_p]           # ≤ best_k_p differing bits
best_f1_p  <- f1_p[best_i_p]

# Precision-constrained threshold (prefer high precision to limit false positives)
target_precision <- 0.9
eligible_idx <- which(!is.na(prec_p) & prec_p >= target_precision)
if (length(eligible_idx) > 0) {
  best_i_p_precision <- eligible_idx[which.max(f1_p[eligible_idx])]
  best_k_p_precision <- k_vals_p[best_i_p_precision]
  precision_at_best  <- prec_p[best_i_p_precision]
  recall_at_best     <- rec_p[best_i_p_precision]
} else {
  best_i_p_precision <- best_i_p
  best_k_p_precision <- best_k_p
  precision_at_best  <- prec_p[best_i_p_precision]
  recall_at_best     <- rec_p[best_i_p_precision]
  warning(sprintf("No Hamming thresholds meet precision ≥ %.2f; using unconstrained best_k_p.", target_precision))
}

# Accuracy at the chosen integer threshold
pred_best_p <- dist_vec_p <= best_k_p
acc_best_p  <- mean(pred_best_p == truth_vec)

# Provide equivalent similarity margin for later sections that expect it
best_margin_p <- 1 - (best_k_p / kbits_p)
best_margin_p_precision <- 1 - (best_k_p_precision / kbits_p)

best_k_p_precision_capped <- min(best_k_p_precision, 10L)
best_margin_p_precision_capped <- 1 - (best_k_p_precision_capped / kbits_p)

cat(sprintf("Projected codes — best integer Hamming threshold: ≤ %d of %d bits differ | F1 = %.3f | Accuracy = %.3f\n",
            best_k_p, kbits_p, best_f1_p, acc_best_p))
cat(sprintf("Projected codes — precision-constrained threshold: ≤ %d bits differ | precision = %.3f | recall = %.3f\n",
            best_k_p_precision, precision_at_best, recall_at_best))
cat(sprintf("Projected codes — capped precision threshold: ≤ %d bits differ (precision target %.2f with cap)\n",
            best_k_p_precision_capped, target_precision))

# ---- Single chart: Precision / Recall / F1 vs Hamming distance (colored) ----
plot(k_vals_p, f1_p, type = "l", lwd = 2, col = "firebrick",
     ylim = c(0, 1),
     xlab = "Maximum Hamming distance (bits that may differ)",
     ylab = "Score",
     main = "Precision, Recall, and F1 vs Hamming Distance (PROJECTED codes)")
lines(k_vals_p, prec_p, lwd = 2, col = "steelblue")
lines(k_vals_p, rec_p,  lwd = 2, col = "darkgreen")
abline(v = best_k_p, lty = 2, col = "gray40")
abline(v = best_k_p_precision, lty = 3, col = "steelblue")
legend("bottomright",
       legend = c("F1", "Precision", "Recall",
                  sprintf("Best F1 ≤ %d bits", best_k_p),
                  sprintf("Precision ≥ %.2f ≤ %d bits", target_precision, best_k_p_precision),
                  sprintf("Precision cap ≤ %d bits", best_k_p_precision_capped)),
       col    = c("firebrick", "steelblue", "darkgreen", "gray40", "steelblue", "steelblue"),
       lty    = c(1,1,1,2,3,3), lwd = c(2,2,2,1,1,1), bty = "n")
```
## 8. Get tag recommendations for in-sample books using in-sample codes
```{r}
existing_tags_df <- get_existing_tags(eav)

unified_meta_train <- build_unified_metadata(eav, 
                                             book_ids_train, 
                                             tag_names_train, 
                                             node_types_train, 
                                             binary_embedding_train, md_db)

book_ids_train_rows <- book_ids_train %>%
  dplyr::left_join(unified_meta_train %>% dplyr::select(node_id, row_id), by = "node_id")

book_ids_all_rows <- book_meta_all %>%
  dplyr::select(book_id, row_id)

hamming_idx_train <- build_hamming_index(binary_embedding_train, unified_meta_train)
hamming_idx_all   <- build_hamming_index(codes_all, unified_meta_all)

top_k_tags <- 3
distance_cap <- 5

recs_train <- recommend_tags(
  margin             = best_k,
  num_neighbors      = top_k_tags,
  distance_cap       = distance_cap,
  type               = "tag",
  unified_meta       = unified_meta_train,
  book_ids           = book_ids_train_rows,
  tag_names          = tag_names_train,
  existing_tags_df   = existing_tags_df,
  binary_embedding   = binary_embedding_train,
  index              = hamming_idx_train
)

recs_all <- recommend_tags(
  margin             = best_k_p_precision_capped,
  num_neighbors      = top_k_tags,
  distance_cap       = distance_cap,
  type               = "tag",
  unified_meta       = unified_meta_all,
  book_ids           = book_ids_all_rows,
  tag_names          = tag_names_train,
  existing_tags_df   = existing_tags_df,
  binary_embedding   = codes_all,
  index              = hamming_idx_all
)

if (!exists("original_tag_values", inherits = FALSE)) {
  original_tag_values <- md_db %>%
    load_eav() %>%
    dplyr::filter(feature == "tag") %>%
    dplyr::distinct(value) %>%
    dplyr::pull(value) %>%
    as.character()
}

existing_tags_original <- existing_tags_df %>%
  dplyr::filter(tag_name %in% original_tag_values)

existing_tags_for_union <- existing_tags_original %>%
  dplyr::mutate(
    book_id = as.character(book_id),
    tag_name = as.character(tag_name),
    distance = NA_real_
  )

prepare_actions <- function(recs_tbl, unified_meta_tbl) {
  recs_filtered <- recs_tbl %>%
    dplyr::filter(tag_name %in% original_tag_values) %>%
    dplyr::mutate(
      book_id = as.character(book_id),
      tag_name = as.character(tag_name)
    )

  recs_with_existing <- dplyr::bind_rows(
    recs_filtered,
    existing_tags_for_union
  ) %>%
    dplyr::distinct(book_id, tag_name, .keep_all = TRUE)

  summarize_tag_actions(
    recs = recs_with_existing,
    existing_tags = existing_tags_original,
    unified_meta = unified_meta_tbl %>%
      dplyr::filter(!is.na(book_id)) %>%
      dplyr::select(book_id, book_title) %>%
      dplyr::distinct()
  )
}

actions_df_train <- prepare_actions(recs_train, unified_meta_train)
actions_df_all   <- prepare_actions(recs_all, unified_meta_all)

existing_tag_lookup <- existing_tags_original %>%
  dplyr::mutate(book_id = as.character(book_id),
                tag_name = as.character(tag_name),
                tag_exists = TRUE)

reclassify_keep <- function(actions_df) {
  actions_df %>%
    dplyr::left_join(existing_tag_lookup,
                     by = c("book_id", "tag_name")) %>%
    dplyr::mutate(
      action = dplyr::case_when(
        action == "keep" & is.na(tag_exists) ~ "add",
        TRUE ~ action
      )
    ) %>%
    dplyr::select(-tag_exists)
}

actions_df_train <- reclassify_keep(actions_df_train)
actions_df_all   <- reclassify_keep(actions_df_all)

actions_df <- actions_df_train
actions_df_full <- actions_df_all
```

## 9. Get tag recommendations for all books using projected codes
```{r}
## 9. Get tag recommendations for all books using projected codes


```

## 10. Get recommendations for untagged books

```{r}
## 9. Get recommendations for untagged books

```

# End

# Refactor

4. Revise build_hamming to do a and b. Create a queryable index from the uint8 matrix
  a. must be a Hamming-based index, either LSH or flat
  b. index = annoy.create()
  c. index.add(binary_embedding)
  d. neighborhoods = index.all_neighborhoods()
  e. neighbors = index.search(vector)
5. Make all other calls use this (recommend_tags())

# API design

* Given a db, produce a codebook that projects books and tags into a common embedding space
* Given a book, show me recommended tags at each margin (distance)
* Given a tag, show me which books it should be in at each margin
* Give me the optimal tags for a book (from spectral codebook)
* Give me the books that should optimally have a specified tag(s) (from spectral codebook)

## To do
* Learn mappings from raw features to spectral codebook
* Reproject all books into a new codebook using learned mappings
* create unified metadata for the new codebook
* regenerate optimal tags
* (optional) enrich the graph with features

# Applying tags to untagged books

* get an untagged book
* use semantic embedding to find similar books
* get those neighbor codes
* interpolate the code from the untagged book from the neighbors (this is more involved than it sounds...)
  * self taught hashing
  * anchor graph hashing
  * self taught hashing with clever feature engineering (landmark-augmented self-taught hashing)
* find nearest tag using the same old approach above
