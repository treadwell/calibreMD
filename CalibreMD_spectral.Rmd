---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

## Options

```{r}
# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

rm(list = ls()) # remove everything in the global environment
gc() # reclaim memory

```

## Load libraries

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()
```

# Load data from SQLite to EAV table

```{r}
dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)

eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features() %>% 
  explode_tags()

# Debug: Print book count
cat("Total unique books in EAV:", length(unique(eav$id)), "\n")
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Build the training graph

```{r}
# unique tags
# unique books
# number tags + book from 1 to n where n = #tags + #books
# provide node type and tag name lookup
# if book has tag, positive edge

# book-tag mapping
book_tags_train <- eav %>%
  filter(feature == "tag") %>%
  mutate(tag_name = value,
         book_id = id) %>%
  select(book_id, tag_name) %>%
  distinct() %>%
  arrange(book_id)

# distinct book_ids and add a node_id (simply 0-n)
book_ids_test <- tibble(book_id = numeric())

book_ids_train <- book_tags_train %>% 
  select(book_id) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number())

# distinct tags and add a node_id (n+1 to...)
tag_names_train <- book_tags_train %>% 
  select(tag_name) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number() + length(book_ids_train$book_id))

# unified node_id to type lookup (answers is a node a book or a tag?)
node_types_train <- data_frame(node_id = seq(nrow(book_ids_train) + nrow(tag_names_train)),
                         type = dplyr::if_else(node_id <= length(book_ids_train$book_id), "book", "tag"))

positive_edges_train <- book_tags_train %>%               # book_id, tag name
  inner_join(book_ids_train,  by = "book_id")  %>%   # adds column `node_id`
  rename(node_a = node_id) %>%                # book → node_a
  inner_join(tag_names_train, by = "tag_name") %>%   # adds 2nd `node_id`
  rename(node_b = node_id) %>%                # tag  → node_b
  select(node_a, node_b)                      # keep only the edge list

g <- igraph::graph_from_data_frame(
        d         = positive_edges_train,
        vertices  = node_types_train$node_id,     # all nodes, even isolates
        directed  = FALSE
      )

cmp <- igraph::components(g)

reps <- tibble(
          vid  = seq_along(cmp$membership),   # vertex index (1…N)
          comp = cmp$membership               # component ID
        ) %>%
        group_by(comp) %>%
        summarise(rep = first(vid), .groups = "drop") %>%  # << first member >>
        pull(rep)                           # plain integer vector

new_edges <- as.vector(rbind(reps[-length(reps)], reps[-1]))

positive_edges_train <- bind_rows(positive_edges_train, tibble(
  node_a = new_edges[c(TRUE,  FALSE)],   # odd positions
  node_b = new_edges[c(FALSE, TRUE)]     # even positions
))

# Create graph with weights (all positive edges have weight 1)
g <- igraph::graph_from_data_frame(
  positive_edges_train %>% mutate(weight = 1), 
  vertices  = node_types_train$node_id,
  directed = FALSE
)
igraph::components(g)$no

# NOTE: abstract testing functions to avoid polluting global scope

```

## Adjacency Matrix
```{r}
# A <- Matrix::sparseMatrix(
#   i = symmetric_edges$node_a,
#   j = symmetric_edges$node_b,
#   x = 1.0,
#   dims = c(nrow(node_types), nrow(node_types)),
#   dimnames = list(node_types$node_id, node_types$node_id)
# )

A <- igraph::as_adjacency_matrix(
        g,
        type = "both",
        sparse = TRUE        # <- returns Matrix::dgCMatrix
      )

head(igraph::V(g)$name)
```

## Laplacian
```{r}
# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_norm <- Matrix::Diagonal(n = nrow(node_types_train)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```

## Get book embeddings

* think of these as ways to separate the space into increasingly higher resolution clusters
* they are binary cuts in the graph
* you need to collectively progress through them to get the finer level clustering

```{r}
# Set number of nontrivial eigenpairs desired
k <- 64

eig <- RSpectra::eigs_sym(
  L_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

eigvecs <- D_inv_sqrt %*% eig$vectors
eigvals <- eig$values

eigvecs <- eigvecs[,-(k+1)]
eigvals <- eigvals[-(k+1)]
ord <- order(eigvals)
eigvals <- eigvals[ord]
eigvecs <- eigvecs[, ord]
eigvals

# to do: build a line graph showing eigenvalues to help select the optimal count

plot(
  seq_along(eigvals),
  eigvals,
  type = "b",
  pch = 19,
  xlab = "Eigenvalue Index",
  ylab = "Eigenvalue",
  main = "Spectrum of Normalized Laplacian"
)
abline(v = which.max(diff(eigvals) > 0.05), col = "red", lty = 2) # optional heuristic marker

# Looks like 7, 10, 12
```
## Visualize graph

```{r}
# library(igraph)
# 
# # Simplify graph for visualization (remove loops and multiple edges)
# g_viz <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = "sum")
# 
# plot(g_viz, layout = as.matrix(eigvecs[, 1:2, drop = FALSE]),
#      vertex.size = 4, vertex.label = NA,
#      edge.color = ifelse(E(g_viz)$weight > 0, "steelblue", "firebrick"),
#      edge.width = abs(E(g_viz)$weight) + .2)
```

## Fn: Iterative quantization

* Rotate the embeddings to minimize quantization error (like PCA?)
* In PCA rotate feature space to explain the most variance
* rotate eigenvectors as if they were raw feature vectors, binarizing at each rotation, and
  trying to match the binary hamming distance to the dot product of the vectors
* you're mapping dot product to hamming distance here
* this allows you to obtain embeddings, continuous from eigenvectors, and binary, from the
  iterative quantization, where dot product (cosine similarity) and hamming distance approximate
  proximity in the graph
* This can be used to find similar books per the underlying graph

```{r}
iterative_quantization <- function(X, n_iter = 25) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: PCA via SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code in {-1, +1}
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes in {0,1} as integer matrix (uint8-like)
  B_final <- sign(X_pca %*% R)
  storage.mode(B_final) <- "integer"
  B01 <- ifelse(B_final > 0L, 1L, 0L)
  mode(B01) <- "integer"
  B01
}

binary_embedding_train <- iterative_quantization(eigvecs)  # uint8-like 0/1 integer matrix
```

### Check binary embeddings
```{r}
cat("binary_embedding dimensions:", dim(binary_embedding_train), "\n")
cat("NA count:", sum(is.na(binary_embedding_train)), "\n")
cat("NaN count:", sum(is.nan(binary_embedding_train)), "\n")
cat("Inf count:", sum(is.infinite(binary_embedding_train)), "\n")
cat("Value range:", range(binary_embedding_train, na.rm = TRUE), "\n")
cat("First 10 rownames:", head(rownames(binary_embedding_train), 10), "\n")
cat("Total rownames:", length(rownames(binary_embedding_train)), "\n")
print(binary_embedding_train[1:5, 1:5])

# Check embedding diversity
cat("\n=== EMBEDDING DIVERSITY ANALYSIS ===\n")

# Count unique binary codes
unique_codes <- unique(as.data.frame(binary_embedding_train))
cat("Unique binary codes:", nrow(unique_codes), "out of", nrow(binary_embedding_train), "books\n")
cat("Diversity ratio:", round(nrow(unique_codes) / nrow(binary_embedding_train), 3), "\n")

# Check eigvecs diversity
# unique_eigvecs <- unique(as.data.frame(round(eigvecs, 6)))
# cat("Unique eigenvector codes (rounded to 6 digits):", nrow(unique_eigvecs), "out of", nrow(eigvecs), "books\n")
# cat("Eigenvector diversity ratio:", round(nrow(unique_eigvecs) / nrow(eigvecs), 3), "\n")

# Check if the problem is in the graph construction
cat("\n=== GRAPH ANALYSIS ===\n")
cat("Books:", nrow(book_ids_train), "\n")
cat("Tags:", nrow(tag_names_train), "\n")
cat("Positive edges:", nrow(positive_edges_train), "\n")
cat("Graph edges (including multiples):", igraph::ecount(g), "\n")

cmp <- igraph::components(g)
cat("Connected components:", cmp$no, "\n")

# Edge diagnostics
edge_ends <- igraph::ends(g, igraph::E(g))
loops_count <- sum(edge_ends[, 1] == edge_ends[, 2])
multiples_count <- sum(igraph::which_multiple(g))
cat("Loops:", loops_count, "\n")
cat("Multiple edges:", multiples_count, "\n")

# Degree by node type
deg <- igraph::degree(g, mode = "all")
deg_df <- tibble::tibble(node_id = as.integer(names(deg)), degree = as.numeric(deg)) |>
  dplyr::left_join(node_types_train, by = "node_id")

cat("\nDegree summary (books):\n")
print(
  deg_df |>
    dplyr::filter(type == "book") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

cat("\nDegree summary (tags):\n")
print(
  deg_df |>
    dplyr::filter(type == "tag") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

# Top tags by connectivity
top_tags <- deg_df |>
  dplyr::filter(type == "tag") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(tag_names_train, by = "node_id") |>
  dplyr::select(tag_name, degree)
cat("\nTop 10 tags by degree:\n")
print(top_tags)

# Top books by connectivity
top_books <- deg_df |>
  dplyr::filter(type == "book") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(book_ids_train, by = "node_id") |>
  dplyr::select(book_id, degree)
cat("\nTop 10 books by degree:\n")
print(top_books)

# Check tag distribution
# Determine whether a tag value existed in the original metadata (pre-explosion)
original_tag_values <- md_db %>%
  load_eav() %>%
  dplyr::filter(feature == "tag") %>%
  dplyr::distinct(value) %>%
  dplyr::pull(value)

tag_counts <- eav %>% 
  dplyr::filter(feature == "tag") %>% 
  dplyr::count(value, sort = TRUE) %>%
  dplyr::mutate(tag_type = ifelse(value %in% original_tag_values, "original", "exploded"))

cat("Top 10 most common tags:\n")
print(tag_counts %>% head(10))
```

## Fn: Unified metadata builder

```{r}
# Build a single metadata tibble with unified lookups by node_id
# Columns: node_id, node_type, row_id, book_id, book_title, tag_name, tag_origin
build_unified_metadata <- function(eav, 
                                   book_ids, 
                                   tag_names = NULL, 
                                   node_types = NULL, 
                                   binary_embedding = NULL, 
                                   md_db = NULL) {
  
  # Titles by book_id
  titles <- eav %>%
    dplyr::filter(feature == "title_original") %>%
    dplyr::distinct(id, .keep_all = TRUE) %>%
    dplyr::transmute(book_id = as.integer(id), book_title = as.character(value))

  # Original tag values from source DB (pre-explosion)
  original_tag_values <- NULL
  if(!is.null(md_db)){
    original_tag_values <- md_db %>%
      load_eav() %>%
      dplyr::filter(feature == "tag") %>%
      dplyr::distinct(value) %>%
      dplyr::pull(value) %>%
      as.character()
  }
  if(is.null(node_types)){  # test data
    # Book-side metadata
    unified <- book_ids %>%
      dplyr::transmute(book_id = as.integer(book_id)) %>%
      dplyr::left_join(titles, by = "book_id")
    
  } else {  # training data
    nodes <- tibble::tibble(
      node_id   = as.integer(node_types$node_id),
      node_type = as.character(node_types$type))

    # row_id mapping: by construction row index/order equals node_id
    row_map <- tibble::tibble(
      node_id = as.integer(node_types$node_id),
      row_id  = as.integer(node_types$node_id))

    # Book-side metadata
    books_meta <- book_ids %>%
      dplyr::transmute(node_id = as.integer(node_id), book_id = as.integer(book_id)) %>%
      dplyr::left_join(titles, by = "book_id")
  
    # Tag-side metadata
    tags_meta <- tag_names %>%
      dplyr::transmute(node_id = as.integer(node_id), tag_name = as.character(tag_name)) %>%
      dplyr::mutate(tag_origin = ifelse(tag_name %in% original_tag_values, "original", "exploded"))
  
    # Unified
    unified <- nodes %>%
      dplyr::left_join(row_map,  by = "node_id") %>%
      dplyr::left_join(books_meta, by = "node_id") %>%
      dplyr::left_join(tags_meta,  by = "node_id") %>%
      dplyr::arrange(node_id)
  }

  unified
}

# Example:
unified_meta_train <- build_unified_metadata(eav, book_ids_train, tag_names_train, node_types_train, binary_embedding_train, md_db)
unified_meta_test <- build_unified_metadata(eav, book_ids_test)
```

# Get new tags

## Fn: Get existing tags for each book

```{r}
get_existing_tags <- function(eav) {
  eav %>% 
    filter(feature == "tag") %>% 
    select(book_id = id, tag_name = value)
}
```

## Fn: Get nearest neighbors

```{r}
# Simple Hamming-based nearest neighbors (pure R, no Python interop)
get_nearest_neighbors <- function(code_vector,
                                  index = NULL,
                                  binary_embedding = NULL,
                                  unified_meta,
                                  num_neighbors = NULL,
                                  margin = NULL,
                                  type = NULL) {
  stopifnot(is.numeric(code_vector))
  if (is.null(index)) stopifnot(is.matrix(binary_embedding))

  # If a prebuilt Hamming index is provided, use it
  if (!is.null(index)) {
    res <- hamming_search(
      index        = index,
      code_vector  = code_vector,
      k            = num_neighbors,
      margin       = margin,
      type         = type,
      unified_meta = unified_meta
    )
    return(res)
  }

  # Efficient Hamming distance for 0/1 data without large temporaries
  sx <- rowSums(binary_embedding)
  sy <- sum(code_vector)
  dot <- as.numeric(binary_embedding %*% code_vector)
  distances <- sx + sy - 2 * dot

  # Candidate set
  candidates <- seq_len(nrow(binary_embedding))

  # Type filter: row index equals node_id by construction
  if (!is.null(type)) {
    stopifnot(all(c("row_id", "node_type") %in% names(unified_meta)))
    row_types <- unified_meta$node_type[match(seq_len(nrow(binary_embedding)), unified_meta$row_id)]
    candidates <- candidates[!is.na(row_types) & row_types == type]
  }

  # Margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(node_ids = integer(0), distances = numeric(0)))

  # Order by distance then index
  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  # Top-k
  if (!is.null(num_neighbors) && length(candidates) > num_neighbors) {
    candidates <- candidates[seq_len(as.integer(num_neighbors))]
  }

  list(node_ids = candidates, distances = distances[candidates])
}
```

## Fn: Hamming index

```{r}
# Build a queryable flat Hamming index from a 0/1 integer matrix
# Returns a lightweight list with matrix and node_id mapping
build_hamming_index <- function(binary_embedding, unified_meta = NULL) {
  stopifnot(is.matrix(binary_embedding))
  node_ids <- if (!is.null(unified_meta) && all(c("node_id", "row_id") %in% names(unified_meta))) {
    unified_meta$node_id[match(seq_len(nrow(binary_embedding)), unified_meta$row_id)]
  } else {
    seq_len(nrow(binary_embedding))
  }
  list(
    data = binary_embedding,
    node_ids = as.integer(node_ids)
  )
}

# Query the flat Hamming index
hamming_search <- function(index,
                           code_vector,
                           k = NULL,
                           margin = NULL,
                           type = NULL,
                           unified_meta = NULL) {
  stopifnot(is.list(index), is.matrix(index$data), is.numeric(code_vector))
  X <- index$data
  # Efficient Hamming distances for 0/1 data:
  # d(x,y) = sum(x) + sum(y) - 2 * x·y
  sx <- rowSums(X)
  sy <- sum(code_vector)
  dot <- as.numeric(X %*% code_vector)
  distances <- sx + sy - 2 * dot

  candidates <- seq_len(nrow(X))

  # Optional type filter using unified metadata mapping
  if (!is.null(type)) {
    stopifnot(!is.null(unified_meta), all(c("node_id", "node_type") %in% names(unified_meta)))
    cand_node_ids <- index$node_ids[candidates]
    cand_types <- unified_meta$node_type[match(cand_node_ids, unified_meta$node_id)]
    keep <- !is.na(cand_types) & cand_types == type
    candidates <- candidates[keep]
  }

  # Optional margin filter
  if (!is.null(margin)) {
    stopifnot(is.numeric(margin), margin >= 0)
    candidates <- candidates[distances[candidates] <= margin]
  }

  if (length(candidates) == 0) return(list(node_ids = integer(0), distances = numeric(0)))

  ord <- order(distances[candidates], candidates)
  candidates <- candidates[ord]

  if (!is.null(k) && length(candidates) > k) {
    candidates <- candidates[seq_len(as.integer(k))]
  }

  list(node_ids = index$node_ids[candidates], distances = distances[candidates])
}

# Compute neighborhoods for all rows using the index (vectorized by blocks)
all_neighborhoods <- function(index,
                              k,
                              margin = NULL,
                              type = NULL,
                              unified_meta = NULL,
                              block_size = 512) {
  stopifnot(is.list(index), is.matrix(index$data))
  X <- index$data
  n <- nrow(X)

  # Precompute sums for fast Hamming
  sx <- rowSums(X)

  # Optional type mask
  type_mask <- rep(TRUE, n)
  if (!is.null(type)) {
    stopifnot(!is.null(unified_meta), all(c("node_id", "node_type") %in% names(unified_meta)))
    types <- unified_meta$node_type[match(index$node_ids, unified_meta$node_id)]
    type_mask <- !is.na(types) & types == type
  }

  results_ids <- vector("list", n)
  results_dist <- vector("list", n)

  for (start in seq(1, n, by = block_size)) {
    end <- min(start + block_size - 1L, n)
    Q <- X[start:end, , drop = FALSE]
    sy <- rowSums(Q)
    dot <- X %*% t(Q)                   # n x b
    # Hamming distances for all pairs in block
    # d = sx + sy - 2*dot, broadcasting sy by column
    D <- sweep(matrix(sx, nrow = n, ncol = ncol(dot)), 2, sy, `+`) - 2 * dot

    # Apply optional type and margin filters
    if (!all(type_mask)) {
      D[!type_mask, ] <- Inf
    }
    if (!is.null(margin)) {
      D[D > margin] <- Inf
    }

    # For each query in block, take top-k
    for (j in seq_len(ncol(D))) {
      dcol <- D[, j]
      ord <- order(dcol, seq_len(n), na.last = NA)
      if (is.finite(k)) {
        ord <- ord[seq_len(min(k, length(ord)))]
      }
      results_ids[[start + j - 1L]] <- index$node_ids[ord]
      results_dist[[start + j - 1L]] <- dcol[ord]
    }
  }

  list(node_ids = results_ids, distances = results_dist)
}
```

## Fn: Recommend tags
```{r}
# Assumes you have:
# - binary_embedding : matrix
# - book_ids         : data.frame/tibble with columns node_id, book_id
# - tag_names        : data.frame/tibble with columns node_id, tag_name
# - (optional) node_type : data.frame/tibble with columns node_id, type
# - get_nearest_neighbors() from the previous message

recommend_tags <- function(margin,
                           num_neighbors = NULL,
                           type = NULL,
                           unified_meta,
                           book_ids,
                           tag_names,
                           existing_tags_df,
                           binary_embedding,
                           index = NULL) {
  stopifnot(all(c("node_id", "book_id") %in% names(book_ids)))
  stopifnot(all(c("node_id", "tag_name") %in% names(tag_names)))
  stopifnot(!is.null(index))

  results_rows <- list()
  # existing_tags_df provided by caller

  # Helper: promote bare segment to its longest hierarchical path found in tag_names
  to_full_path <- function(tag_vector) {
    # Fast path: if all contain '.', return as-is
    if (all(grepl("\\.", tag_vector))) return(tag_vector)

    universe <- tag_names$tag_name
    # Precompute last segments and path depths for the universe
    universe_splits <- strsplit(universe, "\\.")
    universe_last   <- vapply(universe_splits, function(x) if (length(x)) x[length(x)] else "", character(1))
    universe_depth  <- vapply(universe_splits, length, integer(1))

    promote_one <- function(t) {
      if (is.na(t) || t == "") return(t)
      if (grepl("\\.", t)) return(t)
      idx <- which(universe_last == t)
      if (length(idx) == 0) return(t)
      # Choose deepest path; break ties by shortest string
      idx <- idx[order(-universe_depth[idx], nchar(universe[idx]))]
      universe[idx[1]]
    }
    vapply(tag_vector, promote_one, FUN.VALUE = character(1))
  }

  # Helper: keep only the most specific (leaf) paths; drop ancestors
  prune_to_leaf <- function(paths) {
    paths <- unique(paths)
    if (length(paths) <= 1) return(paths)
    keep <- !vapply(paths, function(t) any(paths != t & startsWith(paths, paste0(t, "."))), logical(1))
    paths[keep]
  }

  # Compute neighborhoods once using the index
  k_val <- if (is.null(num_neighbors)) nrow(index$data) else as.integer(num_neighbors)
  all_nbrs <- all_neighborhoods(
    index        = index,
    k            = k_val,
    margin       = margin,
    type         = type,
    unified_meta = unified_meta
  )

  # Map each book to its row_id (which aligns with index rows)
  book_row_ids <- unified_meta$row_id[match(book_ids$node_id, unified_meta$node_id)]

  for (i in seq_along(book_row_ids)) {
    row_idx <- book_row_ids[i]
    if (is.na(row_idx) || row_idx <= 0L) next
    this_book_id <- book_ids$book_id[i]
  
    nbr_ids   <- all_nbrs$node_ids[[row_idx]]
    nbr_dists <- all_nbrs$distances[[row_idx]]
    if (length(nbr_ids) == 0) next
  
    # Map neighbors to tags and carry distances
    df <- tibble::tibble(
      nbr_id   = nbr_ids,
      distance = nbr_dists
    )
  
    df$tag_raw <- unified_meta$tag_name[match(df$nbr_id, unified_meta$node_id)]
    df <- df[!is.na(df$tag_raw) & nzchar(df$tag_raw), , drop = FALSE]
    if (nrow(df) == 0) next
  
    # To full paths, prune to leaves
    df$tag_full <- to_full_path(df$tag_raw)
    pruned_set  <- prune_to_leaf(df$tag_full)
    df <- df[df$tag_full %in% pruned_set, , drop = FALSE]
    if (nrow(df) == 0) next
  
    # If multiple neighbors map to same tag, keep the closest (min distance)
    df_tags <- df |>
      dplyr::group_by(tag_name = .data$tag_full) |>
      dplyr::summarise(distance = min(.data$distance), .groups = "drop")
  
    # Remove tags already on this book
    idx_match    <- which(existing_tags_df$id == this_book_id)
    this_existing <- if (length(idx_match) > 0) existing_tags_df$existing_tags[idx_match][[1]] else NULL
    existing_vec  <- if (!is.null(this_existing)) unlist(this_existing, use.names = FALSE) else character(0)
  
    df_new <- dplyr::anti_join(
      df_tags,
      tibble::tibble(tag_name = existing_vec),
      by = "tag_name"
    )
  
    if (nrow(df_new) > 0) {
      results_rows[[length(results_rows) + 1L]] <- tibble::tibble(
        book_id  = as.character(this_book_id),
        tag_name = as.character(df_new$tag_name),
        distance = as.numeric(df_new$distance)
      )
    }
  }

  if (length(results_rows) == 0) {
    return(tibble::tibble(book_id = character(0), tag_name = character(0)))
  }

  dplyr::bind_rows(results_rows)
}
```
## FN: Summarize recommendations
```{r}
#' Summarize tag actions (add / remove / keep) per book
#'
#' @param recs           data.frame with at least: book_id, tag_name
#' @param existing_tags  data.frame with at least: book_id, tag_name (optional; may be NULL)
#'                       If NULL, assumes no existing tags for any books.
#' @param unified_meta   data.frame with at least: book_id, book_title
#'
#' @return data.frame with columns: book_id, book_title, tag_name, action
#'         where action ∈ { "add", "remove", "keep" }
#'
#' @examples
#' actions_df <- summarize_tag_actions(recs, existing_tags, unified_meta)
summarize_tag_actions <- function(recs, existing_tags = NULL, unified_meta) {
  stopifnot(is.data.frame(recs), is.data.frame(unified_meta))
  
  # --- Validate columns ---
  if (!all(c("book_id", "tag_name") %in% names(recs))) {
    stop("`recs` must contain columns: book_id, tag_name")
  }
  if (!all(c("book_id", "book_title") %in% names(unified_meta))) {
    stop("`unified_meta` must contain columns: book_id, book_title")
  }
  
  # --- Normalize ---
  recs_norm <- recs |>
    dplyr::transmute(
      book_id  = as.character(.data$book_id),
      tag_name = as.character(.data$tag_name)
    ) |>
    dplyr::distinct()
  
  if (is.null(existing_tags)) {
    # If no tags exist, all recommendations are "add"
    existing_norm <- dplyr::tibble(book_id = character(), tag_name = character())
  } else {
    if (!all(c("book_id", "tag_name") %in% names(existing_tags))) {
      stop("`existing_tags` must contain columns: book_id, tag_name")
    }
    existing_norm <- existing_tags |>
      dplyr::transmute(
        book_id  = as.character(.data$book_id),
        tag_name = as.character(.data$tag_name)
      ) |>
      dplyr::distinct()
  }
  
  # --- Compute sets ---
  keep_df <- dplyr::inner_join(existing_norm, recs_norm,
                               by = c("book_id", "tag_name")) |>
    dplyr::mutate(action = "keep")
  
  add_df <- dplyr::anti_join(recs_norm, existing_norm,
                             by = c("book_id", "tag_name")) |>
    dplyr::mutate(action = "add")
  
  remove_df <- dplyr::anti_join(existing_norm, recs_norm,
                                by = c("book_id", "tag_name")) |>
    dplyr::mutate(action = "remove")
  
  out <- dplyr::bind_rows(keep_df, add_df, remove_df)
  
  # --- Attach titles ---
  titles_norm <- unified_meta |>
    dplyr::transmute(
      book_id    = as.character(.data$book_id),
      book_title = as.character(.data$book_title)
    ) |>
    dplyr::distinct()
  
  out <- out |>
    dplyr::left_join(titles_norm, by = "book_id") |>
    dplyr::relocate(.data$book_title, .after = .data$book_id)
  
  # --- Order by book / action / tag ---
  action_order <- c(keep = 1L, add = 2L, remove = 3L)
  
  out |>
    dplyr::mutate(.action_rank = action_order[.data$action]) |>
    dplyr::arrange(.data$book_id, .data$.action_rank, .data$tag_name) |>
    dplyr::select(.data$book_id, .data$book_title, .data$tag_name, .data$action)
}
```

# Inference

## FN: Train encoder
```{r}
train_encoder <- function(X, Y){
  requireNamespace("xgboost", quietly = TRUE)
  models <- list()
  for (bit in seq_len(ncol(Y))) {
    print(paste("bit", bit, "training"))
    # Extract label vector and convert to numeric
    y <- as.numeric(Y[, bit])
    # Construct DMatrix using sparse feature matrix
    dtrain <- xgboost::xgb.DMatrix(data = X, label = y)
    models[[bit]] <- xgboost::xgboost(
      data = dtrain,
      objective = "binary:logistic",
      eval_metric = "logloss",
      nrounds = 50,
      verbose = 0
    )
  }
  models
}
```

## FN: Encode books
```{r}
encode_books <- function(models, X, threshold = 0.5) {
  requireNamespace("xgboost", quietly = TRUE)

  n_bits <- length(models)
  n_obs  <- nrow(X)

  dX <- xgboost::xgb.DMatrix(data = X)
  out <- matrix(0L, nrow = n_obs, ncol = n_bits)

  for (b in seq_len(n_bits)) {
    preds <- stats::predict(models[[b]], newdata = dX)

    # Convert logits to probabilities if outside [0,1]
    if (any(preds < 0 | preds > 1, na.rm = TRUE)) {
      preds <- plogis(preds)
    }

    out[, b] <- as.integer(preds >= threshold)
  }

  colnames(out) <- if (!is.null(names(models)) && all(nzchar(names(models)))) {
    names(models)
  } else {
    paste0("bit_", seq_len(n_bits))
  }

  out
}
```

## FN: Evaluate projections
```{r}
evaluate_projection <- function(Y0, Y1, prefix = 0){
  # Y0, Y1: integer (0/1) matrices with identical dims; rows = nodes, cols = bits
  if (!is.matrix(Y0) || !is.matrix(Y1)) stop("Y0 and Y1 must be matrices.")
  if (!all(dim(Y0) == dim(Y1))) stop("Y0 and Y1 must have the same dimensions.")
  if (anyNA(Y0) || anyNA(Y1)) stop("Y0 and Y1 must not contain NA values.")
  if (!all(Y0 %in% c(0L, 1L)) || !all(Y1 %in% c(0L, 1L)))
    stop("Y0 and Y1 must contain only 0/1 values.")

  p <- ncol(Y0)
  if (p == 0L) stop("Matrices must have at least one column (bit).")

  # determine number of bits to compare
  if (prefix <= 0) {
    k <- p
  } else {
    if (prefix > p) stop("prefix cannot be larger than the number of columns")
    k <- prefix
  }

  # restrict matrices to first k columns
  Y0_sub <- Y0[, seq_len(k), drop = FALSE]
  Y1_sub <- Y1[, seq_len(k), drop = FALSE]

  # Hamming distance per row = number of differing bits
  avg_frac_dist <- mean(rowSums(abs(Y0_sub - Y1_sub))) / k

  # Normalize to [0,1] so that 1.0 means avg Hamming distance = 0
  score <- 1 - avg_frac_dist
  as.numeric(score)
}

```

## FN: Prep data set
```{r}
prep_encoder_data <- function(eav, unified_meta, binary_embedding = NULL, indexed_features = NULL) {
  if(is.null(binary_embedding)){
    book_features <- eav %>%
      dplyr::inner_join(
        unified_meta %>%
          dplyr::select(book_id),
        by = dplyr::join_by(id == book_id)
      ) %>%
      dplyr::filter(!.data$feature %in% c("tag", "title_original")) %>%
      dplyr::select(book_id, feature, value) %>%
      dplyr::distinct()
    row_index <- unified_meta$book_id
    col_index <- match(
      paste0(book_features$feature, "=", as.character(book_features$value)),
      indexed_features$key)
  } else {
    ids <- unified_meta %>%
      dplyr::filter(node_type == "book") %>%
      dplyr::arrange(.data$node_id) %>%
      dplyr::pull(.data$node_id)
    Y <- binary_embedding[ids, , drop = FALSE]
    book_features <- eav %>%
      dplyr::inner_join(
        unified_meta %>% 
          dplyr::filter(node_type == "book") %>% 
          dplyr::select(book_id, node_id),
        by = dplyr::join_by(id == book_id)
      ) %>%
      dplyr::filter(!.data$feature %in% c("tag", "title_original")) %>%
      dplyr::select(node_id, feature, value) %>%
      dplyr::distinct()
    indexed_features <- book_features %>%
      dplyr::mutate(key = paste0(.data$feature, "=", as.character(.data$value))) %>%
      dplyr::distinct(.data$feature, .data$value, .data$key) %>%
      dplyr::arrange(.data$feature, .data$value) %>%
      dplyr::mutate(id_feature = dplyr::row_number()) %>%
      dplyr::select(.data$feature, .data$value, .data$key, .data$id_feature)
    row_index <- match(book_features$node_id, ids)
    col_index <- match(
      paste0(book_features$feature, "=", as.character(book_features$value)),
      indexed_features$key)
  }
  X <- Matrix::sparseMatrix(
    i = row_index,
    j = col_index,
    x = 1,
    dims = c(length(ids), nrow(indexed_features)),
    dimnames = list(ids, indexed_features$key))
  list(
    ids = ids,
    features = indexed_features,  # dictionary of columns in X
    X = X,                        # sparse one-hot feature matrix (ids x feature=value)
    Y = Y)
}
```


## Build X and Y
```{r}
train_data <- prep_encoder_data(eav, unified_meta_train, binary_embedding_train)
# test_data <- prep_encoder_data(eav, unified_meta_test, NULL, train_data$features)
```
# Train Encoder
```{r}
# predict spectral codes from the raw features
model <- train_encoder(train_data$X, train_data$Y)  # Y is spectral codes

# project spectral codes from training data using the encoder
train_data$Y_projected <- encode_books(model, train_data$X)
# test_data$Y_projected <- encode_books(model, test_data$X)
```

## Evaluate (measure hamming between in-sample spectral and in-sample projected codes)

* How well do the codes we're predicting match the actual codes?
* This is self-taught hashing
* We need to increase the accuracy...

```{r}
train_acc <- evaluate_projection(train_data$Y, train_data$Y_projected, 1)
print(paste("Training accuracy", train_acc))

# Also build functions to do a manual check
```

Done
1. generate graph
2. generate in-sample codes
3. learn projections
4. project codes
5. evaluate accuracy of projected codes

Next steps
6. find the margin that best captures ground-truth labeling for in-sample codes
7. find the margin that best captures ground-truth labeling for projected codes
8. for each in-sample book, considering the known labels as the first list, create a 2nd and 3rd labels list using the in-sample codes plus margin and projected codes plus margin
9. for each in-sample book compute Jaccard similarity between the ground truth label set and each of the two sets above (labels from the in-sample codes, and the projected codes). Display the worst offenders per list and overall Jaccard similarity per list.
10. for each in-sample tag, considering the known books as the first list, create a 2nd and 3rd books list using the in-sample codes plus margin and projected codes plus margin.
11. for each in-sample tag compute Jaccard similarity between the ground truth book set and each of the two sets above (books from the in-sample codes, and the projected codes). Display the worst offenders per list and overall Jaccard similarity per list.

## 6. find the margin that best captures ground-truth labeling for in-sample codes

* This works with the in-sample books only.
```{r}
## 6. Find the integer Hamming threshold that best captures ground-truth labeling (in-sample codes)

suppressPackageStartupMessages({
  library(dplyr); library(tibble); library(Matrix)
})

# ---- Guards & setup ---------------------------------------------------------
stopifnot(
  exists("train_data", inherits = FALSE),
  !is.null(train_data$Y), !is.null(train_data$ids),
  exists("book_tags_train", inherits = FALSE),
  exists("book_ids_train",  inherits = FALSE),
  exists("tag_names_train", inherits = FALSE)
)

Y <- as.matrix(train_data$Y)  # expected 0/1 codes (n x kbits)
if (!all(Y %in% c(0,1))) stop("train_data$Y must be binary (0/1) for integer Hamming search.")
storage.mode(Y) <- "integer"

n_books <- nrow(Y)
kbits   <- ncol(Y)            # e.g., 64

# ---- Ground truth: two books are same-label if they share ≥1 training tag ---
# Align rows of Y to node_ids
row_index_map <- tibble(node_id = as.integer(train_data$ids),
                        row_ix  = seq_len(n_books))

# Map training (book_id, tag_name) -> Y row_ix via node_id
bt_train <- book_tags_train %>%
  inner_join(book_ids_train, by = "book_id") %>%
  select(node_id, tag_name) %>%
  inner_join(row_index_map, by = "node_id")

# Stable tag order (not strictly required, but useful)
tag_levels <- tag_names_train %>% arrange(node_id) %>% pull(tag_name)

bt_train <- bt_train %>%
  mutate(col_ix = match(tag_name, tag_levels)) %>%
  filter(!is.na(row_ix), !is.na(col_ix))

n_tags <- length(tag_levels)

# Sparse book–tag incidence M (n_books x n_tags)
M <- Matrix::sparseMatrix(
  i = bt_train$row_ix, j = bt_train$col_ix, x = 1L,
  dims = c(n_books, n_tags)
)

# Co-membership matrix G: TRUE if share ≥ 1 tag (ignore diagonal)
G <- (M %*% Matrix::t(M)) > 0
diag(G) <- FALSE

# ---- Integer Hamming distances (bits that differ) ---------------------------
# For 0/1 Y: d_H(i,j) = ones_i + ones_j - 2 * (Y_i · Y_j)
ones <- Matrix::rowSums(Y)
S    <- Y %*% Matrix::t(Y)  # shared 1-bits (n x n)
D    <- (matrix(ones, n_books, n_books) +
         matrix(ones, n_books, n_books, byrow = TRUE)) - 2 * S
D    <- as.matrix(D)
diag(D) <- NA_integer_

# Upper triangle vectors for evaluation
upper_idx <- upper.tri(D, diag = FALSE)
dist_vec  <- as.numeric(D[upper_idx])         # integer in [0..kbits]
truth_vec <- as.logical(G[upper_idx])         # ground truth (same-label)
# Also keep similarity vector for downstream compatibility (NOT used here)
sim_vec   <- 1 - (dist_vec / kbits)

# ---- Sweep integer thresholds k = 0..kbits ----------------------------------
k_vals <- 0:kbits
prec <- rec <- f1 <- numeric(length(k_vals))

for (i in seq_along(k_vals)) {
  k_th <- k_vals[i]                 # threshold = max differing bits allowed
  pred <- dist_vec <= k_th
  TP <- sum(pred &  truth_vec, na.rm = TRUE)
  FP <- sum(pred & !truth_vec, na.rm = TRUE)
  FN <- sum(!pred &  truth_vec, na.rm = TRUE)

  prec[i] <- if ((TP + FP) > 0) TP / (TP + FP) else NA_real_
  rec[i]  <- if ((TP + FN) > 0) TP / (TP + FN) else NA_real_
  f1[i]   <- if (is.na(prec[i]) || is.na(rec[i]) || (prec[i] + rec[i]) == 0)
               NA_real_ else 2 * prec[i] * rec[i] / (prec[i] + rec[i])
}

best_i  <- which.max(f1)
best_k  <- k_vals[best_i]          # ≤ best_k differing bits
best_f1 <- f1[best_i]

# Accuracy at the chosen integer threshold
pred_best <- dist_vec <= best_k
acc_best  <- mean(pred_best == truth_vec)

# For later sections that expect a similarity margin in [0,1], provide equivalent:
best_margin <- 1 - (best_k / kbits)  # DO NOT use for plotting here

cat(sprintf("Best integer Hamming threshold: ≤ %d of %d bits differ | F1 = %.3f | Accuracy = %.3f\n",
            best_k, kbits, best_f1, acc_best))

# ---- Single chart: Precision / Recall / F1 vs Hamming distance (colored) ----
plot(k_vals, f1, type = "l", lwd = 2, col = "firebrick",
     ylim = c(0, 1),
     xlab = "Maximum Hamming distance (bits that may differ)",
     ylab = "Score",
     main = "Precision, Recall, and F1 vs Hamming Distance Threshold")
lines(k_vals, prec, lwd = 2, col = "steelblue")
lines(k_vals, rec,  lwd = 2, col = "darkgreen")
abline(v = best_k, lty = 2, col = "gray40")
legend("bottomright",
       legend = c("F1", "Precision", "Recall", sprintf("Best ≤ %d bits", best_k)),
       col    = c("firebrick", "steelblue", "darkgreen", "gray40"),
       lty    = c(1,1,1,2), lwd = c(2,2,2,1), bty = "n")

```
## 7. find the margin that best captures ground-truth labeling for projected codes

* Looks at projected codes, so for when we take new books and project them.
* Recognize that the margin will be larger and there will be more false positives.

```{r}
## 7. Find the integer Hamming threshold that best captures ground-truth labeling (PROJECTED in-sample codes)

suppressPackageStartupMessages({
  library(dplyr); library(tibble); library(Matrix)
})

# ---- Guards -----------------------------------------------------------------
stopifnot(
  exists("train_data", inherits = FALSE),
  !is.null(train_data$ids),
  !is.null(train_data$Y_projected)
)

# Use PROJECTED codes; binarize at 0.5 if needed for Hamming
Yp <- as.matrix(train_data$Y_projected)
if (!all(Yp %in% c(0,1))) {
  Yp <- (Yp >= 0.5) * 1L
}
storage.mode(Yp) <- "integer"

n_books_p <- nrow(Yp)
kbits_p   <- ncol(Yp)

# ---- Ground truth (reuse from Section 6 or rebuild) -------------------------
if (!exists("G", inherits = FALSE)) {
  stopifnot(
    exists("book_tags_train", inherits = FALSE),
    exists("book_ids_train",  inherits = FALSE),
    exists("tag_names_train", inherits = FALSE)
  )
  row_index_map <- tibble(node_id = as.integer(train_data$ids),
                          row_ix  = seq_len(n_books_p))

  bt_train <- book_tags_train %>%
    inner_join(book_ids_train, by = "book_id") %>%
    select(node_id, tag_name) %>%
    inner_join(row_index_map, by = "node_id")

  tag_levels <- tag_names_train %>% arrange(node_id) %>% pull(tag_name)

  bt_train <- bt_train %>%
    mutate(col_ix = match(tag_name, tag_levels)) %>%
    filter(!is.na(row_ix), !is.na(col_ix))

  n_tags <- length(tag_levels)

  M <- Matrix::sparseMatrix(
    i = bt_train$row_ix, j = bt_train$col_ix, x = 1L,
    dims = c(n_books_p, n_tags)
  )

  G <- (M %*% Matrix::t(M)) > 0
  diag(G) <- FALSE
}

# Pairwise upper-triangle ground-truth vector (same-label)
upper_idx_p <- upper.tri(G, diag = FALSE)
truth_vec   <- as.logical(G[upper_idx_p])   # reuse name for compatibility

# ---- Integer Hamming distances for PROJECTED codes --------------------------
# d_H(i,j) = ones_i + ones_j - 2 * (Y_i · Y_j)
ones_p <- Matrix::rowSums(Yp)
S_p    <- Yp %*% Matrix::t(Yp)
D_p    <- (matrix(ones_p, n_books_p, n_books_p) +
           matrix(ones_p, n_books_p, n_books_p, byrow = TRUE)) - 2 * S_p
D_p    <- as.matrix(D_p)
diag(D_p) <- NA_integer_

dist_vec_p <- as.numeric(D_p[upper_idx_p])          # integers 0..kbits_p
# Similarity kept only for downstream compatibility (not used for plotting)
sim_vec_p  <- 1 - (dist_vec_p / kbits_p)

stopifnot(is.numeric(dist_vec_p), all(dist_vec_p >= 0), all(dist_vec_p <= kbits_p))

# ---- Sweep integer thresholds k = 0..kbits_p --------------------------------
k_vals_p <- 0:kbits_p
prec_p <- rec_p <- f1_p <- numeric(length(k_vals_p))

for (i in seq_along(k_vals_p)) {
  k_th <- k_vals_p[i]
  pred <- dist_vec_p <= k_th
  TP <- sum(pred &  truth_vec, na.rm = TRUE)
  FP <- sum(pred & !truth_vec, na.rm = TRUE)
  FN <- sum(!pred &  truth_vec, na.rm = TRUE)

  prec_p[i] <- if ((TP + FP) > 0) TP / (TP + FP) else NA_real_
  rec_p[i]  <- if ((TP + FN) > 0) TP / (TP + FN) else NA_real_
  f1_p[i]   <- if (is.na(prec_p[i]) || is.na(rec_p[i]) || (prec_p[i] + rec_p[i]) == 0)
                 NA_real_ else 2 * prec_p[i] * rec_p[i] / (prec_p[i] + rec_p[i])
}

best_i_p   <- which.max(f1_p)
best_k_p   <- k_vals_p[best_i_p]           # ≤ best_k_p differing bits
best_f1_p  <- f1_p[best_i_p]

# Accuracy at the chosen integer threshold
pred_best_p <- dist_vec_p <= best_k_p
acc_best_p  <- mean(pred_best_p == truth_vec)

# Provide equivalent similarity margin for later sections that expect it
best_margin_p <- 1 - (best_k_p / kbits_p)

cat(sprintf("Projected codes — best integer Hamming threshold: ≤ %d of %d bits differ | F1 = %.3f | Accuracy = %.3f\n",
            best_k_p, kbits_p, best_f1_p, acc_best_p))

# ---- Single chart: Precision / Recall / F1 vs Hamming distance (colored) ----
plot(k_vals_p, f1_p, type = "l", lwd = 2, col = "firebrick",
     ylim = c(0, 1),
     xlab = "Maximum Hamming distance (bits that may differ)",
     ylab = "Score",
     main = "Precision, Recall, and F1 vs Hamming Distance (PROJECTED codes)")
lines(k_vals_p, prec_p, lwd = 2, col = "steelblue")
lines(k_vals_p, rec_p,  lwd = 2, col = "darkgreen")
abline(v = best_k_p, lty = 2, col = "gray40")
legend("bottomright",
       legend = c("F1", "Precision", "Recall", sprintf("Best ≤ %d bits", best_k_p)),
       col    = c("firebrick", "steelblue", "darkgreen", "gray40"),
       lty    = c(1,1,1,2), lwd = c(2,2,2,1), bty = "n")
```
## Get recommendations
```{r}
existing_tags_df <- get_existing_tags(eav)
unified_meta_train <- build_unified_metadata(eav, 
                                             book_ids_train, 
                                             tag_names_train, 
                                             node_types_train, 
                                             binary_embedding_train, md_db)
# Optional: build flat Hamming index for faster queries
hamming_idx <- build_hamming_index(binary_embedding_train, unified_meta_train)
recs <- recommend_tags(
  margin             = 64, #best_k_p,
  num_neighbors      = 25,
  type               = "tag",        # or NULL
  unified_meta       = unified_meta_train,
  book_ids           = book_ids_train,
  tag_names          = tag_names_train,
  existing_tags_df   = existing_tags_df,
  binary_embedding   = binary_embedding_train,
  index              = hamming_idx
)

actions_df <- summarize_tag_actions(
  recs = recs,
  existing_tags = existing_tags_df,                # all books considered untagged
  unified_meta = unified_meta_train    # has book_id + book_title
)
```
TODO:  check on filtering by exploded tags.  eg. magic and magic.card and maybe 

## 8. for each in-sample book, considering the known labels as the first list, create a 2nd and 3rd labels list using the in-sample codes plus margin and projected codes plus margin
```{r}
## 8. Neighbor tag lists by book (integer Hamming; robust + self-check)

suppressPackageStartupMessages({
  library(dplyr); library(tibble); library(Matrix); library(purrr); library(tidyr); library(jsonlite)
})

# ------------------------ Guards & inputs ------------------------------------
req_ok <- all(
  exists("book_tags_train", inherits = FALSE),
  exists("book_ids_train",  inherits = FALSE)
)
if (!req_ok) stop("Section 8 requires book_tags_train and book_ids_train in memory.")

# Optional: try to load thresholds from calibration if not present
if (!exists("best_k", inherits = FALSE) || !is.numeric(best_k)) {
  if (file.exists("calibremd_margins_calibration.rds")) {
    calibration <- readRDS("calibremd_margins_calibration.rds")
    if (!is.null(calibration$in_sample$best_k)) best_k <- calibration$in_sample$best_k
    if (is.null(best_k) || is.na(best_k))
      stop("best_k not found; run Section 6 or provide calibremd_margins_calibration.rds.")
  } else {
    stop("best_k not found and calibremd_margins_calibration.rds is missing. Run Section 6 first.")
  }
}

# Projected threshold is optional
have_projected_threshold <- exists("best_k_p", inherits = FALSE) && is.numeric(best_k_p)

# ------------------------ Align rows (books) ---------------------------------
# We need the row order we will use throughout Section 8. Prefer train_data if present.
if (exists("train_data", inherits = FALSE) && !is.null(train_data$ids)) {
  node_ids <- as.integer(train_data$ids)
} else if (exists("all_data", inherits = FALSE) && !is.null(all_data$ids)) {
  node_ids <- as.integer(all_data$ids)  # fallback to a global codebook if you use one
} else {
  stop("Neither train_data$ids nor all_data$ids is available to define row order.")
}

row_map <- tibble(row_ix = seq_along(node_ids), node_id = node_ids) %>%
  left_join(book_ids_train, by = "node_id") %>%
  distinct(row_ix, node_id, book_id)

if (any(is.na(row_map$book_id))) {
  # Some node_ids in the code matrix are not present in book_ids_train (training mapping).
  # Keep only rows we can map back to a book_id.
  keep <- which(!is.na(row_map$book_id))
  if (length(keep) == 0) stop("No overlap between code rows and training book_ids.")
  row_map <- row_map[keep, , drop = FALSE]
}

n_books <- nrow(row_map)

# ------------------------ Get code matrices ----------------------------------
# In-sample codes (Y): must be binary 0/1
# Prefer train_data$Y in the same node_id order
get_codes <- function(candidate_Y, candidate_ids, target_node_ids) {
  if (is.null(candidate_Y) || is.null(candidate_ids)) return(NULL)
  idx <- match(target_node_ids, as.integer(candidate_ids))
  if (all(is.na(idx))) return(NULL)
  candidate_Y[idx[!is.na(idx)], , drop = FALSE]
}

Y <- NULL
if (exists("train_data", inherits = FALSE) && !is.null(train_data$Y)) {
  Y <- get_codes(train_data$Y, train_data$ids, row_map$node_id)
}
if (is.null(Y) && exists("all_data", inherits = FALSE) && !is.null(all_data$Y)) {
  Y <- get_codes(all_data$Y, all_data$ids, row_map$node_id)
}
if (is.null(Y)) stop("Cannot find a code matrix (Y). Ensure train_data$Y or all_data$Y is available.")

Y <- as.matrix(Y)
if (!all(Y %in% c(0,1))) stop("Y must be binary 0/1 for integer Hamming.")
storage.mode(Y) <- "integer"
kbits <- ncol(Y)

# Optional projected codes (Yp)
Yp <- NULL
if (exists("train_data", inherits = FALSE) && !is.null(train_data$Y_projected)) {
  Yp <- get_codes(train_data$Y_projected, train_data$ids, row_map$node_id)
} else if (exists("all_data", inherits = FALSE) && !is.null(all_data$Y_projected)) {
  Yp <- get_codes(all_data$Y_projected, all_data$ids, row_map$node_id)
}
if (!is.null(Yp)) {
  Yp <- as.matrix(Yp)
  if (!all(Yp %in% c(0,1))) Yp <- (Yp >= 0.5) * 1L
  storage.mode(Yp) <- "integer"
}

# ------------------------ Build tag incidence M ------------------------------
# If tag_names_train exists we use it to stabilize columns; else infer from book_tags_train
if (exists("tag_names_train", inherits = FALSE)) {
  tag_levels <- tag_names_train %>% arrange(node_id) %>% pull(tag_name)
} else {
  tag_levels <- book_tags_train %>% distinct(tag_name) %>% arrange(tag_name) %>% pull(tag_name)
}
n_tags <- length(tag_levels)

bt_train <- book_tags_train %>%
  inner_join(row_map %>% select(book_id, row_ix), by = "book_id") %>%
  mutate(col_ix = match(tag_name, tag_levels)) %>%
  filter(!is.na(row_ix), !is.na(col_ix))

M <- Matrix::sparseMatrix(
  i = bt_train$row_ix,
  j = bt_train$col_ix,
  x = 1L,
  dims = c(n_books, n_tags)
)

# ------------------------ Compute integer Hamming distances ------------------
# d_H(i,j) = ones_i + ones_j - 2 * (Y_i · Y_j)
compute_D <- function(B) {
  ones <- Matrix::rowSums(B)
  S    <- B %*% Matrix::t(B)
  D    <- (matrix(ones, nrow(B), nrow(B)) +
           matrix(ones, nrow(B), nrow(B), byrow = TRUE)) - 2 * S
  D <- as.matrix(D)
  diag(D) <- NA_integer_
  D
}

D <- compute_D(Y)

D_p <- NULL
if (!is.null(Yp) && have_projected_threshold) {
  D_p <- compute_D(Yp)
}

# ------------------------ Adjacency (neighbors within best_k / best_k_p) ----
L_in <- Matrix((D <= best_k) * 1L, sparse = TRUE)
diag(L_in) <- 0L

L_pr <- NULL
if (!is.null(D_p) && have_projected_threshold) {
  L_pr <- Matrix((D_p <= best_k_p) * 1L, sparse = TRUE)
  diag(L_pr) <- 0L
}

# ------------------------ Aggregate neighbor tag counts ----------------------
counts_in <- L_in %*% M
counts_pr <- if (!is.null(L_pr)) L_pr %*% M else NULL

# Helper serializers
topN_to_string <- function(x, tag_names, N = 20L) {
  if (all(x == 0)) return("")
  ix <- which(x > 0)
  ord <- ix[order(x[ix], decreasing = TRUE)]
  ord <- head(ord, N)
  paste0(tag_names[ord], ":", as.integer(x[ord]), collapse = " | ")
}
vec_to_json <- function(x, tag_names, N = 200L) {
  if (all(x == 0)) return("[]")
  ix <- which(x > 0)
  ord <- ix[order(x[ix], decreasing = TRUE)]
  ord <- head(ord, N)
  jsonlite::toJSON(tag_names[ord], auto_unbox = TRUE)
}

# Own tag columns per book (for minus-own)
own_cols_by_row <- split(bt_train$col_ix, bt_train$row_ix)

make_output <- function(L, counts, label) {
  if (is.null(L) || is.null(counts)) return(NULL)
  neigh_ct <- Matrix::rowSums(L)

  # counts minus own tags
  counts_mo <- counts
  if (length(own_cols_by_row) > 0) {
    for (i in seq_len(n_books)) {
      cols <- own_cols_by_row[[as.character(i)]]
      if (!is.null(cols) && length(cols)) counts_mo[i, cols] <- 0
    }
  }

  tibble(
    row_ix         = seq_len(n_books),
    book_id        = row_map$book_id,
    node_id        = row_map$node_id,
    neighbor_count = as.integer(neigh_ct),
    neighbor_tags          = purrr::map_chr(seq_len(n_books), ~ vec_to_json(counts[.x, ],  tag_levels)),
    neighbor_top20         = purrr::map_chr(seq_len(n_books), ~ topN_to_string(counts[.x, ],  tag_levels, 20L)),
    neighbor_tags_minus_own= purrr::map_chr(seq_len(n_books), ~ vec_to_json(counts_mo[.x, ], tag_levels)),
    neighbor_top20_minus_own= purrr::map_chr(seq_len(n_books), ~ topN_to_string(counts_mo[.x, ], tag_levels, 20L)),
    source = label
  )
}

out_in  <- make_output(L_in, counts_in, "in_sample")
out_pr  <- if (!is.null(counts_pr)) make_output(L_pr, counts_pr, "projected") else NULL

neighbors_tags <- bind_rows(out_in, out_pr)

# Optional: attach titles
if (exists("catalog_df", inherits = FALSE)) {
  neighbors_tags <- neighbors_tags %>%
    left_join(catalog_df %>% select(book_id = id, title), by = "book_id") %>%
    relocate(title, .after = book_id)
} else if (exists("titles_all", inherits = FALSE)) {
  neighbors_tags <- neighbors_tags %>%
    left_join(titles_all, by = "book_id") %>%
    relocate(title, .after = book_id)
}

# ------------------------ Save & self-check ----------------------------------
write.csv(neighbors_tags %>% arrange(source, desc(neighbor_count)),
          "calibremd_neighbor_tags_by_book.csv", row.names = FALSE)
if (!is.null(out_in))  write.csv(out_in, "calibremd_neighbor_tags_in_sample.csv", row.names = FALSE)
if (!is.null(out_pr))  write.csv(out_pr, "calibremd_neighbor_tags_projected.csv", row.names = FALSE)

cat(sprintf(
  "Section 8 complete.\nBooks: %d | Tags: %d | best_k: %s%s\nSaved:\n - calibremd_neighbor_tags_by_book.csv%s%s\n",
  n_books, n_tags, as.character(best_k),
  if (have_projected_threshold) sprintf(" | best_k_p: %s", as.character(best_k_p)) else "",
  if (!is.null(out_in))  "\n - calibremd_neighbor_tags_in_sample.csv" else "",
  if (!is.null(out_pr))  "\n - calibremd_neighbor_tags_projected.csv" else ""
))

# Lightweight self-checks
stopifnot(nrow(neighbors_tags) %in% c(n_books, 2L * n_books))
stopifnot(all(neighbors_tags$neighbor_count >= 0))
```

## 9. for each in-sample book compute Jaccard similarity between the ground truth label set and each of the two sets above (labels from the in-sample codes, and the projected codes). Display the worst offenders per list and overall Jaccard similarity per list.

```{r}

```
## 9a. Take any book by id, project the code for the book, compare labels from the projected code with ground truth labels if any exist for that book, and recommend labels that should be either added or removed, ranked by most important to add or delete.


```{r}

```

## 10. for each in-sample tag, considering the known books as the first list, create a 2nd and 3rd books list using the in-sample codes plus margin and projected codes plus margin.
```{r}

```

## 11. for each in-sample tag compute Jaccard similarity between the ground truth book set and each of the two sets above (books from the in-sample codes, and the projected codes). Display the worst offenders per list and overall Jaccard similarity per list.
```{r}

```

# End

# Refactor

4. Revise build_hamming to do a and b. Create a queryable index from the uint8 matrix
  a. must be a Hamming-based index, either LSH or flat
  b. index = annoy.create()
  c. index.add(binary_embedding)
  d. neighborhoods = index.all_neighborhoods()
  e. neighbors = index.search(vector)
5. Make all other calls use this (recommend_tags())

# API design

* Given a db, produce a codebook that projects books and tags into a common embedding space
* Given a book, show me recommended tags at each margin (distance)
* Given a tag, show me which books it should be in at each margin
* Give me the optimal tags for a book (from spectral codebook)
* Give me the books that should optimally have a specified tag(s) (from spectral codebook)

## To do
* Learn mappings from raw features to spectral codebook
* Reproject all books into a new codebook using learned mappings
* create unified metadata for the new codebook
* regenerate optimal tags
* (optional) enrich the graph with features

# Applying tags to untagged books

* get an untagged book
* use semantic embedding to find similar books
* get those neighbor codes
* interpolate the code from the untagged book from the neighbors (this is more involved than it sounds...)
  * self taught hashing
  * anchor graph hashing
  * self taught hashing with clever feature engineering (landmark-augmented self-taught hashing)
* find nearest tag using the same old approach above




