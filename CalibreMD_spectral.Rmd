---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

## Options

```{r}
# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

rm(list = ls()) # remove everything in the global environment
gc() # reclaim memory

```

## Load libraries

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()
```

# Load data from SQLite to EAV table

```{r}
dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)

eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features() %>% 
  explode_tags()

# Debug: Print book count
cat("Total unique books in EAV:", length(unique(eav$id)), "\n")
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Build the graph

```{r}
# unique tags
# unique books
# number tags + book from 1 to n where n = #tags + #books
# provide node type and tag name lookup
# if book has tag, positive edge

# book-tag mapping
book_tags <- eav %>%
  filter(feature == "tag") %>%
  mutate(tag_name = value,
         book_id = id) %>%
  select(book_id, tag_name) %>%
  distinct() %>%
  arrange(book_id)

# distinct book_ids and add a node_id (simply 0-n)
book_ids <- book_tags %>% 
  select(book_id) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number())

# distinct tags and add a node_id (n+1 to...)
tag_names <- book_tags %>% 
  select(tag_name) %>% 
  distinct() %>% 
  mutate(node_id = dplyr::row_number() + length(book_ids$book_id))

# unified node_id to type lookup (answers is a node a book or a tag?)
node_types <- data_frame(node_id = seq(nrow(book_ids) + nrow(tag_names)),
                         type = dplyr::if_else(node_id <= length(book_ids$book_id), "book", "tag"))

positive_edges <- book_tags %>%               # book_id, tag name
  inner_join(book_ids,  by = "book_id")  %>%   # adds column `node_id`
  rename(node_a = node_id) %>%                # book → node_a
  inner_join(tag_names, by = "tag_name") %>%   # adds 2nd `node_id`
  rename(node_b = node_id) %>%                # tag  → node_b
  select(node_a, node_b)                      # keep only the edge list

g <- igraph::graph_from_data_frame(
        d         = positive_edges,
        vertices  = node_types$node_id,     # all nodes, even isolates
        directed  = FALSE
      )

cmp <- igraph::components(g)

reps <- tibble(
          vid  = seq_along(cmp$membership),   # vertex index (1…N)
          comp = cmp$membership               # component ID
        ) %>%
        group_by(comp) %>%
        summarise(rep = first(vid), .groups = "drop") %>%  # << first member >>
        pull(rep)                           # plain integer vector

new_edges <- as.vector(rbind(reps[-length(reps)], reps[-1]))

positive_edges <- bind_rows(positive_edges, tibble(
  node_a = new_edges[c(TRUE,  FALSE)],   # odd positions
  node_b = new_edges[c(FALSE, TRUE)]     # even positions
))

# Create graph with weights (all positive edges have weight 1)
g <- igraph::graph_from_data_frame(
  positive_edges %>% mutate(weight = 1), 
  vertices  = node_types$node_id,
  directed = FALSE
)
igraph::components(g)$no

```

## Adjacency Matrix
```{r}
# A <- Matrix::sparseMatrix(
#   i = symmetric_edges$node_a,
#   j = symmetric_edges$node_b,
#   x = 1.0,
#   dims = c(nrow(node_types), nrow(node_types)),
#   dimnames = list(node_types$node_id, node_types$node_id)
# )

A <- igraph::as_adjacency_matrix(
        g,
        type = "both",
        sparse = TRUE        # <- returns Matrix::dgCMatrix
      )

head(igraph::V(g)$name)
```

## Laplacian
```{r}
# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_norm <- Matrix::Diagonal(n = nrow(node_types)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```

## Get book embeddings

* think of these as ways to separate the space into increasingly higher resolution clusters
* they are binary cuts in the graph
* you need to collectively progress through them to get the finer level clustering

```{r}
# Set number of nontrivial eigenpairs desired
k <- 256 

eig <- RSpectra::eigs_sym(
  L_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

# Remove the trivial eigenpair (usually eigenvalue ~0)
# Optionally filter based on a tolerance if needed
eigvals <- eig$values[-1]
eigvecs <- eig$vectors[, -1]

eigvals

```
## visualize

```{r}
library(igraph)

# Simplify graph for visualization (remove loops and multiple edges)
g_viz <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = "sum")

plot(g_viz, layout = eigvecs[, 1:2], vertex.size = 4, vertex.label = NA,
     edge.color = ifelse(E(g_viz)$weight > 0, "steelblue", "firebrick"),
     edge.width = abs(E(g_viz)$weight) + .2)
```

## Iterative quantization (optional)

* Rotate the embeddings to minimize quantization error (like PCA?)
* In PCA rotate feature space to explain the most variance
* rotate eigenvectors as if they were raw feature vectors, binarizing at each rotation, and
  trying to match the binary hamming distance to the dot product of the vectors
* you're mapping dot product to hamming distance here
* this allows you to obtain embeddings, continuous from eigenvectors, and binary, from the
  iterative quantization, where dot product (cosine similarity) and hamming distance approximate
  proximity in the graph
* This can be used to find similar books per the underlying graph

```{r}
iterative_quantization <- function(X, n_iter = 25) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: Optional PCA (skip if X already low-dimensional)
  # SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes and rotation
  list(binary_code = sign(X_pca %*% R), rotation = R)
}

# Option 1: Use iterative quantization (current approach)
itq_result <- iterative_quantization(eigvecs)
binary_embedding <- itq_result$binary_code  # contains -1/+1 codes per book
# rotation_matrix <- itq_result$rotation

# Option 2: Simple binary encoding (alternative approach)
# binary_embedding <- sign(eigvecs)  # Simple sign-based binary encoding

rownames(binary_embedding) <- node_types$node_id

```

## Check binary embeddings
```{r}
cat("binary_embedding dimensions:", dim(binary_embedding), "\n")
cat("NA count:", sum(is.na(binary_embedding)), "\n")
cat("NaN count:", sum(is.nan(binary_embedding)), "\n")
cat("Inf count:", sum(is.infinite(binary_embedding)), "\n")
cat("Value range:", range(binary_embedding, na.rm = TRUE), "\n")
cat("First 10 rownames:", head(rownames(binary_embedding), 10), "\n")
cat("Total rownames:", length(rownames(binary_embedding)), "\n")
print(binary_embedding[1:5, 1:5])

# Check embedding diversity
cat("\n=== EMBEDDING DIVERSITY ANALYSIS ===\n")

# Count unique binary codes
unique_codes <- unique(as.data.frame(binary_embedding))
cat("Unique binary codes:", nrow(unique_codes), "out of", nrow(binary_embedding), "books\n")
cat("Diversity ratio:", round(nrow(unique_codes) / nrow(binary_embedding), 3), "\n")

# Check eigvecs diversity
unique_eigvecs <- unique(as.data.frame(round(eigvecs, 6)))
cat("Unique eigenvector codes (rounded to 6 digits):", nrow(unique_eigvecs), "out of", nrow(eigvecs), "books\n")
cat("Eigenvector diversity ratio:", round(nrow(unique_eigvecs) / nrow(eigvecs), 3), "\n")

# Check if the problem is in the graph construction
cat("\n=== GRAPH ANALYSIS ===\n")
cat("Books:", nrow(book_ids), "\n")
cat("Tags:", nrow(tag_names), "\n")
cat("Positive edges:", nrow(positive_edges), "\n")
cat("Graph edges (including multiples):", igraph::ecount(g), "\n")

cmp <- igraph::components(g)
cat("Connected components:", cmp$no, "\n")

# Edge diagnostics
edge_ends <- igraph::ends(g, igraph::E(g))
loops_count <- sum(edge_ends[, 1] == edge_ends[, 2])
multiples_count <- sum(igraph::which_multiple(g))
cat("Loops:", loops_count, "\n")
cat("Multiple edges:", multiples_count, "\n")

# Degree by node type
deg <- igraph::degree(g, mode = "all")
deg_df <- tibble::tibble(node_id = as.integer(names(deg)), degree = as.numeric(deg)) |>
  dplyr::left_join(node_types, by = "node_id")

cat("\nDegree summary (books):\n")
print(
  deg_df |>
    dplyr::filter(type == "book") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

cat("\nDegree summary (tags):\n")
print(
  deg_df |>
    dplyr::filter(type == "tag") |>
    dplyr::summarize(
      min = min(degree),
      q1 = quantile(degree, 0.25),
      median = median(degree),
      mean = mean(degree),
      q3 = quantile(degree, 0.75),
      max = max(degree)
    )
)

# Top tags by connectivity
top_tags <- deg_df |>
  dplyr::filter(type == "tag") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(tag_names, by = "node_id") |>
  dplyr::select(tag_name, degree)
cat("\nTop 10 tags by degree:\n")
print(top_tags)

# Top books by connectivity
top_books <- deg_df |>
  dplyr::filter(type == "book") |>
  dplyr::arrange(dplyr::desc(degree)) |>
  dplyr::slice(1:10) |>
  dplyr::left_join(book_ids, by = "node_id") |>
  dplyr::select(book_id, degree)
cat("\nTop 10 books by degree:\n")
print(top_books)

# Check tag distribution
tag_counts <- eav %>% 
  filter(feature == "tag") %>% 
  count(value, sort = TRUE) %>%
  head(10)
cat("Top 10 most common tags:\n")
print(tag_counts)
```

# Get new tags
## Helper: Get existing tags for each book

```{r}
get_existing_tags <- function(eav) {
  eav %>% 
    filter(feature == "tag") %>% 
    select(id, tag = value) %>% 
    group_by(id) %>% 
    summarize(existing_tags = list(tag), .groups = "drop")
}
```

## Helper: Get nearest neighbors for a book

```{r}
# Hamming distance version (current)
get_nearest_neighbors <- function(binary_embedding, book_idx, num_neighbors) {
  book_code <- binary_embedding[book_idx, , drop = FALSE]
  distances <- apply(binary_embedding, 1, function(other_code) sum(book_code != other_code))
  distances[book_idx] <- Inf  # exclude self
  nearest_indices <- order(distances)[1:num_neighbors]
  list(
    indices   = nearest_indices,
    ids       = rownames(binary_embedding)[nearest_indices],
    distances = distances[nearest_indices]
  )
}

# Test direct feature similarity (alternative approach)
# Try a book with more common tags
test_book_id <- "12243"  # This book had "@Ken Brooks" tag from earlier tests

# Find the book in the spectral dataset
# test_book_idx <- book_ids[test_book_id]

# book_id -> node_id 
test_book_idx <- book_ids %>% 
  dplyr::filter(book_id == as.numeric(test_book_id)) %>%
  dplyr::pull(node_id)

neighbors <- get_nearest_neighbors(binary_embedding, test_book_idx, 100)

# 1) Build helper tables (all node_id as character to avoid type mismatches)
node_types_chr <- node_types %>% mutate(node_id = as.character(node_id))
book_ids_chr   <- book_ids   %>% mutate(node_id = as.character(node_id))
tag_names_chr  <- tag_names  %>% mutate(node_id = as.character(node_id))

# Existing tags per *book_id*
existing_tags_df <- get_existing_tags(eav)   # cols: id (book_id), existing_tags (list)

# Map *node_id* -> (book_id, collapsed_tags)
book_tags_by_node <- book_ids_chr %>%
  left_join(existing_tags_df, by = c("book_id" = "id")) %>%
  mutate(tags_str = vapply(existing_tags, function(x) paste(x, collapse = ", "),
                           FUN.VALUE = character(1L))) %>%
  select(node_id, book_id, tags_str)

# 2) Build a tidy neighbors table (ranked)
nbrs <- neighbors  # from get_nearest_neighbors(...)
nbrs_df <- tibble(
  rank     = seq_along(nbrs$indices),
  node_id  = if (!is.null(nbrs$ids)) as.character(nbrs$ids) else as.character(nbrs$indices),
  distance = nbrs$distances
) %>%
  left_join(node_types_chr, by = "node_id") %>%                  # adds 'type'
  left_join(book_tags_by_node, by = "node_id") %>%               # adds 'book_id', 'tags_str'
  left_join(tag_names_chr %>% rename(tag_label = tag_name), by = "node_id") %>%  # adds 'tag_label'
  mutate(label = dplyr::case_when(
    type == "book" ~ paste0("book_id=", book_id,
                            ifelse(is.na(tags_str) | tags_str == "", "",
                                   paste0(" : tags=[", tags_str, "]"))),
    type == "tag"  ~ paste0("tag=", tag_label),
    TRUE           ~ paste0("type=", coalesce(type, "NA"))
  )) %>%
  select(rank, node_id, type, book_id, tag_label, tags_str, label, distance) %>%
  arrange(rank)

# 3) Print (no global row index)
for (i in seq_len(nrow(nbrs_df))) {
  cat(sprintf("%3d: %s : distance=%s\n",
              nbrs_df$rank[i],
              nbrs_df$label[i],
              nbrs_df$distance[i]))
}
```
## API design

* given a db, produce a codebook that projects books and tags into a common embedding space
* Given a book, show me recommended tags at each margin (distance)
* Given a tag, show me which books it should be in at each margin
* Give me the optimal tags for a book
* Give me the books that should optimally have this tag
* Recommend tag nesting (if it's a suffix, recommend the unexploded tag)
* Given a codebook, learn mappings from raw features to codes

Notes
* check classic version for functions

```{r}
# (Optional) quick sanity check: see how many books vs tags in the neighbor list
print(table(nbrs_df$type, useNA = "ifany"))

if (length(test_book_idx) > 0) {
  cat("Testing book ID:", test_book_id, "at index:", test_book_idx, "\n")
  
  # Get tags for test book
  test_book_tags <- eav %>% 
    filter(feature == "tag", id == as.numeric(test_book_id)) %>%
    pull(value)
  cat("\nTest book tags:", paste(test_book_tags, collapse = ", "), "\n")
  
  # Get tags for neighbor BOOKS only, preserving neighbor order
  neighbor_books_df <- nbrs_df %>%
    dplyr::filter(type == "book", !is.na(book_id)) %>%
    dplyr::select(book_id, distance)

  neighbor_tags <- eav %>% 
    dplyr::filter(feature == "tag", id %in% as.numeric(neighbor_books_df$book_id)) %>%
    dplyr::group_by(id) %>%
    dplyr::summarize(tags = paste(value, collapse = ", "), .groups = "drop") %>%
    dplyr::mutate(id = as.numeric(id)) %>%
    dplyr::inner_join(
      neighbor_books_df %>% dplyr::transmute(id = as.numeric(book_id), distance),
      by = "id"
    ) %>%
    dplyr::rename(book_id = id) %>%
    dplyr::select(book_id, distance, tags) %>%
    dplyr::arrange(match(book_id, neighbor_books_df$book_id))

  cat("\nNeighbor tags:\n")
  for (i in seq_len(nrow(neighbor_tags))) {
    cat("Book", neighbor_tags$book_id[i], "(distance:", round(neighbor_tags$distance[i], 3), "):", neighbor_tags$tags[i], "\n")
  }
  
  # Check shared tags
  shared_tags <- intersect(test_book_tags, unlist(strsplit(neighbor_tags$tags, ", ")))
  cat("\nShared tags:", paste(shared_tags, collapse = ", "), "\n")
  
  # Count how many neighbors share at least one tag
  neighbor_tag_lists <- strsplit(neighbor_tags$tags, ", ")
  neighbors_with_shared_tags <- sum(sapply(neighbor_tag_lists, function(tags) length(intersect(test_book_tags, tags)) > 0))
  cat("Neighbors with shared tags:", neighbors_with_shared_tags, "out of", length(neighbors$ids), "\n")
  
} else {
  cat("Book ID", test_book_id, "not found in spectral dataset!\n")
}

```

## Helper: Get tags from nearest neighbors

```{r}
get_neighbor_tags <- function(existing_tags, neighbor_indices, binary_embedding) {
  # Map neighbor indices (rows in binary_embedding) -> node_ids -> book_ids
  neighbor_node_ids <- rownames(binary_embedding)[neighbor_indices]
  if (is.null(neighbor_node_ids)) neighbor_node_ids <- as.character(neighbor_indices)
  neighbor_node_ids_int <- suppressWarnings(as.integer(neighbor_node_ids))

  # Keep only neighbors that correspond to book nodes
  neighbor_book_ids <- book_ids %>%
    dplyr::filter(node_id %in% neighbor_node_ids_int) %>%
    dplyr::pull(book_id)

  if (length(neighbor_book_ids) == 0) return(character(0))

  # Collect existing tags for those neighbor books
  existing_tags %>%
    dplyr::filter(id %in% neighbor_book_ids) %>%
    dplyr::pull(existing_tags) %>%
    unlist(use.names = FALSE)
}
```

## Helper: Score tags and filter recommendations

```{r}
score_tags <- function(all_neighbor_tags, nearest_distances, book_existing_tags, threshold, max_recommendations) {
  if (length(all_neighbor_tags) == 0) return(NULL)
  all_neighbor_tags <- as.character(all_neighbor_tags)
  # Drop NA and blank tags after trimming whitespace
  all_neighbor_tags <- trimws(all_neighbor_tags)
  all_neighbor_tags <- all_neighbor_tags[!is.na(all_neighbor_tags) & nzchar(all_neighbor_tags)]
  if (length(all_neighbor_tags) == 0) return(NULL)
  unique_tags <- unique(all_neighbor_tags)
  tag_counts <- setNames(
    sapply(unique_tags, function(tag) sum(all_neighbor_tags == tag)),
    unique_tags
  )
  avg_distance <- mean(nearest_distances)
  tag_scores <- tag_counts / (avg_distance + 1)
  new_tags <- names(tag_scores)[!names(tag_scores) %in% book_existing_tags]
  if (length(new_tags) == 0) return(NULL)
  new_tag_scores <- tag_scores[new_tags]
  new_tag_scores <- sort(new_tag_scores, decreasing = TRUE)
  high_score_tags <- new_tag_scores[new_tag_scores >= threshold]
  if (length(high_score_tags) > max_recommendations) {
    high_score_tags <- high_score_tags[1:max_recommendations]
  }
  if (length(high_score_tags) == 0) return(NULL)
  high_score_tags
}
```

## Main: Recommend new tags using spectral analysis

```{r}
add_new_tags_spectral <- function(eav, binary_embedding, 
                                  threshold = 0.7, min_neighbors = 3, max_recommendations = 10,
                                  max_books = NULL) {
  cat("Recommending new tags using spectral analysis...\n")
  cat("Debug: binary_embedding dimensions:", dim(binary_embedding), "\n")
  existing_tags <- get_existing_tags(eav)
  # Iterate only over book nodes; map node_id -> book_id
  n_books <- nrow(book_ids)
  if (is.null(max_books)) {
    book_indices <- seq_len(n_books)
  } else {
    book_indices <- seq_len(min(n_books, as.integer(max_books)))
  }
  recommendations <- list()
  for (i in book_indices) {
    node_id <- book_ids$node_id[i]
    book_id <- book_ids$book_id[i]

    # Row index in the embedding for this book node
    book_row_idx <- which(rownames(binary_embedding) == as.character(node_id))
    if (length(book_row_idx) == 0) next

    book_existing <- existing_tags %>% dplyr::filter(id == book_id)
    book_existing_tags <- if (nrow(book_existing) > 0) book_existing$existing_tags[[1]] else character(0)

    nn <- get_nearest_neighbors(binary_embedding, book_row_idx, min_neighbors)

    # Keep only neighbor indices that correspond to book nodes
    neighbor_node_ids <- rownames(binary_embedding)[nn$indices]
    neighbor_node_ids_int <- suppressWarnings(as.integer(neighbor_node_ids))
    book_mask <- neighbor_node_ids_int %in% book_ids$node_id
    indices_book_only <- nn$indices[book_mask]
    distances_book_only <- nn$distances[book_mask]

    all_neighbor_tags <- get_neighbor_tags(existing_tags, indices_book_only, binary_embedding)
    high_score_tags <- score_tags(all_neighbor_tags, distances_book_only, book_existing_tags, threshold, max_recommendations)
    if (!is.null(high_score_tags)) {
      # Map neighbor node_ids -> neighbor book_ids for reporting
      neighbor_book_ids <- {
        nbr_ids_int <- neighbor_node_ids_int[book_mask]
        matched <- match(nbr_ids_int, book_ids$node_id)
        book_ids$book_id[matched]
      }
      recommendations[[as.character(book_id)]] <- list(
        book_id = as.character(book_id),
        existing_tags = book_existing_tags,
        recommended_tags = names(high_score_tags),
        tag_scores = high_score_tags,
        nearest_neighbors = neighbor_book_ids,
        neighbor_distances = distances_book_only
      )
    }
  }
  cat("3. Formatting results...\n")
  if (length(recommendations) > 0) {
    results <- do.call(rbind, lapply(recommendations, function(rec) {
      data.frame(
        book_id = rec$book_id,
        recommended_tag = rec$recommended_tags,
        score = rec$tag_scores,
        existing_tags = paste(rec$existing_tags, collapse = ", "),
        stringsAsFactors = FALSE
      )
    }))
    results <- results[order(results$score, decreasing = TRUE), ]
    cat("✓ Found", nrow(results), "tag recommendations for", length(recommendations), "books\n")
    return(results)
  } else {
    cat("✓ No tag recommendations found\n")
    return(data.frame(
      book_id = character(),
      recommended_tag = character(),
      score = numeric(),
      existing_tags = character(),
      stringsAsFactors = FALSE
    ))
  }
}
```

# Function to show tag additions in a clear format
```{r}
show_tag_additions <- function(spectral_results, top_n = 20) {
  if (nrow(spectral_results) == 0) {
    return(data.frame(
      book_id = character(),
      tag_name = character(),
      score = numeric(),
      stringsAsFactors = FALSE
    ))
  }
  
  # Create the result data frame with the desired structure
  result_df <- spectral_results %>%
    select(
      book_id = book_id,
      tag_name = recommended_tag,
      score = score
    ) %>%
    arrange(book_id, desc(score))  # Sort by book_id, then by score descending
  
  return(result_df)
}
```

## Main: Recommend new tags using feature similarity (improved approach)

```{r}
add_new_tags_feature_similarity <- function(eav, 
                                           threshold = 0.7, min_neighbors = 5, max_recommendations = 10) {
  cat("Recommending new tags using feature similarity...\n")
  
  # Get the spectral dataset (contains actual book features)
  spectral_data <- prep_dataset_spectral(eav)
  cat("Working with", nrow(spectral_data$X), "books and", ncol(spectral_data$X), "features\n")
  
  # Get existing tags
  existing_tags <- get_existing_tags(eav)
  
  # Convert sparse matrix to dense for similarity calculation
  cat("Converting to dense matrix for similarity calculation...\n")
  X_dense <- as.matrix(spectral_data$X)
  
  recommendations <- list()
  n_books <- nrow(X_dense)
  
  # Process books in batches to avoid memory issues
  batch_size <- 100
  for (batch_start in seq(1, n_books, batch_size)) {
    batch_end <- min(batch_start + batch_size - 1, n_books)
    cat("Processing books", batch_start, "to", batch_end, "of", n_books, "\n")
    
    for (book_idx in batch_start:batch_end) {
      book_id <- rownames(X_dense)[book_idx]
      
      # Get existing tags for this book
      book_existing <- existing_tags %>% filter(id == as.numeric(book_id))
      book_existing_tags <- if (nrow(book_existing) > 0) book_existing$existing_tags[[1]] else character(0)
      
      # Calculate cosine similarities for this book
      book_vec <- X_dense[book_idx, ]
      similarities <- apply(X_dense, 1, function(other_vec) {
        sum(book_vec * other_vec) / (sqrt(sum(book_vec^2)) * sqrt(sum(other_vec^2)))
      })
      similarities[book_idx] <- -Inf  # Exclude self
      
      # Get nearest neighbors
      nearest_indices <- order(similarities, decreasing = TRUE)[1:min_neighbors]
      nearest_similarities <- similarities[nearest_indices]
      neighbor_book_ids <- rownames(X_dense)[nearest_indices]
      
      # Get tags from neighbors
      neighbor_tags <- eav %>% 
        filter(feature == "tag", id %in% as.numeric(neighbor_book_ids)) %>%
        group_by(id) %>%
        summarize(tags = list(value), .groups = "drop")
      
      all_neighbor_tags <- unlist(neighbor_tags$tags)
      
      if (length(all_neighbor_tags) > 0) {
        # Score tags based on frequency and similarity
        unique_tags <- unique(all_neighbor_tags)
        tag_scores <- numeric(length(unique_tags))
        names(tag_scores) <- unique_tags
        
        for (i in seq_along(unique_tags)) {
          tag <- unique_tags[i]
          # Count frequency and weight by similarity
          tag_count <- sum(all_neighbor_tags == tag)
          avg_similarity <- mean(nearest_similarities[which(all_neighbor_tags == tag)])
          tag_scores[i] <- tag_count * avg_similarity
        }
        
        # Filter out existing tags
        new_tags <- names(tag_scores)[!names(tag_scores) %in% book_existing_tags]
        
        if (length(new_tags) > 0) {
          new_tag_scores <- tag_scores[new_tags]
          new_tag_scores <- sort(new_tag_scores, decreasing = TRUE)
          
          # Apply threshold and limit
          high_score_tags <- new_tag_scores[new_tag_scores >= threshold]
          if (length(high_score_tags) > max_recommendations) {
            high_score_tags <- high_score_tags[1:max_recommendations]
          }
          
          if (length(high_score_tags) > 0) {
            recommendations[[book_id]] <- list(
              book_id = book_id,
              existing_tags = book_existing_tags,
              recommended_tags = names(high_score_tags),
              tag_scores = high_score_tags,
              nearest_neighbors = neighbor_book_ids,
              neighbor_similarities = nearest_similarities
            )
          }
        }
      }
    }
  }
  
  cat("Formatting results...\n")
  if (length(recommendations) > 0) {
    results <- do.call(rbind, lapply(recommendations, function(rec) {
      data.frame(
        book_id = rec$book_id,
        recommended_tag = rec$recommended_tags,
        score = rec$tag_scores,
        existing_tags = paste(rec$existing_tags, collapse = ", "),
        stringsAsFactors = FALSE
      )
    }))
    results <- results[order(results$score, decreasing = TRUE), ]
    cat("✓ Found", nrow(results), "tag recommendations for", length(recommendations), "books\n")
    return(results)
  } else {
    cat("✓ No tag recommendations found\n")
    return(data.frame(
      book_id = character(),
      recommended_tag = character(),
      score = numeric(),
      existing_tags = character(),
      stringsAsFactors = FALSE
    ))
  }
}
```

# Test the spectral tag recommendation function

```{r}
cat("\n=== Testing spectral tag recommendations ===\n")
spectral_recommendations <- add_new_tags_spectral(
  eav = eav,
  binary_embedding = binary_embedding,
  threshold = 0.5,  # Lower threshold for testing
  min_neighbors = 3,
  max_recommendations = 5,
  max_books = 200
)
```

# Get clean data frame of tag additions

```{r}
tag_additions_df <- show_tag_additions(spectral_recommendations)
```

# Display the results
```{r}
cat("Tag additions data frame:\n")
print(tag_additions_df)

#saveRDS(tag_additions_df, "~/downloads/tag_additions_df.rds")
```

# Test the improved feature similarity tag recommendation function

```{r}
cat("\n=== Testing improved feature similarity tag recommendations ===\n")
feature_similarity_recommendations <- add_new_tags_feature_similarity(
  eav = eav,
  threshold = 0.5,  # Lower threshold for testing
  min_neighbors = 5,
  max_recommendations = 5
)
```

# Get clean data frame of feature similarity tag additions

```{r}
feature_similarity_tag_additions_df <- show_tag_additions(feature_similarity_recommendations)
```

# Display the feature similarity results
```{r}
cat("Feature similarity tag additions data frame:\n")
print(feature_similarity_tag_additions_df)
```

# End