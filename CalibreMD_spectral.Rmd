---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()

# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)
```

# Load data from SQLite to EAV table

```{r}
eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features()
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Spectral Analysis Functions (No Threshold)
```{r}
# Spectral version of prep_dataset - no minimum book threshold
prep_dataset_spectral <- function(eav){
  book_tags <- eav %>% 
    filter(feature == "tag") %>% 
    mutate(tag = value) %>% 
    select(id, tag) %>% 
    distinct() %>%
    arrange(id)
  
  # No filtering by tag frequency - use all tags
  valid_tags <- unique(book_tags$tag)
  
  book_tags <- book_tags %>%
    filter(tag %in% valid_tags)
  
  book_features <- eav %>% 
    filter(!feature %in% c("tag", "title_original")) %>% 
    mutate(feature = paste(feature, value, sep = ": ")) %>% 
    select(id, feature) %>% 
    distinct() %>% 
    arrange(id)
  
  common_ids <- sort(intersect(book_tags$id, book_features$id))
  
  book_tags <- book_tags %>%
    filter(id %in% common_ids)
  
  book_features <- book_features %>%
    filter(id %in% common_ids)
  
  # Create index mappings for row (id) and columns (tag or feature)
  id_levels <- sort(unique(common_ids)) # ensures same order in both matrices
  
  # Sparse tag matrix
  tag_levels <- sort(unique(book_tags$tag))
  tag_matrix <- Matrix::sparseMatrix(
    i = match(book_tags$id, id_levels),
    j = match(book_tags$tag, tag_levels),
    x = 1L,
    dims = c(length(id_levels), length(tag_levels)),
    dimnames = list(id_levels, tag_levels)
  )
  
  # Sparse feature matrix
  feature_levels <- sort(unique(book_features$feature))
  feature_matrix <- Matrix::sparseMatrix(
    i = match(book_features$id, id_levels),
    j = match(book_features$feature, feature_levels),
    x = 1L,
    dims = c(length(id_levels), length(feature_levels)),
    dimnames = list(id_levels, feature_levels)
  )
  
  list(
    X = feature_matrix,
    Y = tag_matrix,
    book_features = book_features
  )
}
```

## Build the graph (anchor graph)

* change star graph to anchor graph

```{r}
tag_membership <- eav %>% 
  filter(feature == "tag") %>% 
  mutate(tag = value) %>% 
  select(id, tag) %>% 
  group_by(tag) %>% 
  summarize(membership = list(id), .groups = "drop")

# Step 1: Create star graph + record center nodes per tag
star_graph_df <- tag_membership %>%
  mutate(center = purrr::map_int(membership, ~ sample(.x, 1))) %>%
  mutate(pairs = purrr::map2(membership, center, function(ids, center) {
    others <- setdiff(ids, center)
    if (length(others) == 0) return(tibble(from = integer(), to = integer()))
    bind_rows(
      tibble(from = center, to = others),
      tibble(from = others, to = center)
    )
  }))

# Step 2: Collect positive edges
positive_edges <- star_graph_df %>%
  select(pairs) %>%
  tidyr::unnest(pairs) %>%
  mutate(weight = 1)

# Step 3: Create negative edges by pairing each center with another random center
centers <- star_graph_df$center
n_centers <- length(centers)

negative_edges <- tibble(
  from = centers,
  to = purrr::map_int(centers, function(center) {
    sample(setdiff(centers, center), 1)
  })
) %>%
  bind_rows(transmute(., from = to, to = from)) %>%  # symmetric
  mutate(weight = -0.1)

# Step 4: Combine
star_graph_pairs <- bind_rows(positive_edges, negative_edges)
```

## Laplacian
```{r}

# Use signed weights directly
edge_list <- star_graph_pairs %>%
  select(from, to, weight)

# Step 1: Build node index
nodes <- sort(unique(c(edge_list$from, edge_list$to)))
node_index <- setNames(seq_along(nodes), nodes)

# Step 2: Build signed adjacency matrix A
A <- Matrix::sparseMatrix(
  i = node_index[as.character(edge_list$from)],
  j = node_index[as.character(edge_list$to)],
  x = edge_list$weight,
  dims = c(length(nodes), length(nodes)),
  dimnames = list(nodes, nodes)
)

# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_signed_norm <- Matrix::Diagonal(n = length(nodes)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```

## Get book embeddings

* think of these as ways to separate the space into increasingly higher resolution clusters
* they are binary cuts in the graph
* you need to collectively progress through them to get the finer level clustering

```{r}
# Set number of nontrivial eigenpairs desired
k <- 38  # example value

eig <- RSpectra::eigs_sym(
  L_signed_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

# Remove the trivial eigenpair (usually eigenvalue ~0)
# Optionally filter based on a tolerance if needed
eigvals <- eig$values[-1]
eigvecs <- eig$vectors[, -1]

eigvals

```
## visualize

```{r}
library(igraph)

g  <- graph_from_data_frame(star_graph_pairs, directed = FALSE) |>
        simplify(remove.multiple = TRUE,
                 remove.loops    = TRUE,
                 edge.attr.comb  = list(weight = "sum"))

plot(g, layout = eigvecs[, 3:5], vertex.size = 4, vertex.label = NA,
     edge.color = ifelse(E(g)$weight > 0, "steelblue", "firebrick"),
     edge.width = abs(E(g)$weight) + .2)
```

## Iterative quantization (optional)

* Rotate the embeddings to minimize quantization error (like PCA?)
* In PCA rotate feature space to explain the most variance
* rotate eigenvectors as if they were raw feature vectors, binarizing at each rotation, and
  trying to match the binary hamming distance to the dot product of the vectors
* you're mapping dot product to hamming distance here
* this allows you to obtain embeddings, continuous from eigenvectors, and binary, from the
  iterative quantization, where dot product (cosine similarity) and hamming distance approximate
  proximity in the graph
* This can be used to find similar books per the underlying graph

```{r}
iterative_quantization <- function(X, n_iter = 50) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: Optional PCA (skip if X already low-dimensional)
  # SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes and rotation
  list(binary_code = sign(X_pca %*% R), rotation = R)
}

# eigvecs: matrix of size n_books × k from Laplacian
itq_result <- iterative_quantization(eigvecs)

binary_embedding <- itq_result$binary_code  # contains -1/+1 codes per book
rotation_matrix <- itq_result$rotation

```

## Recommend tag additions using spectral analysis
```{r}
# Function to recommend new tags using spectral analysis
add_new_tags_spectral <- function(eav, binary_classifiers, dataset, binary_embedding, 
                                  threshold = 0.7, min_neighbors = 3, max_recommendations = 10) {
  cat("Recommending new tags using spectral analysis...\n")
  
  # Encode all books in the dataset using trained classifiers
  cat("1. Encoding all books...\n")
  all_books_encoded <- encode_new_books(binary_classifiers, dataset$X)
  
  # Get existing tag assignments
  cat("2. Getting existing tag assignments...\n")
  existing_tags <- eav %>% 
    filter(feature == "tag") %>% 
    select(id, tag = value) %>% 
    group_by(id) %>% 
    summarize(existing_tags = list(tag), .groups = "drop")
  
  # Get all unique tags
  all_tags <- eav %>% 
    filter(feature == "tag") %>% 
    pull(value) %>% 
    unique()
  
  cat("3. Analyzing nearest neighbors for each book...\n")
  
  # Calculate pairwise distances between all books
  # Using Hamming distance for binary codes (number of different bits)
  n_books <- nrow(all_books_encoded)
  recommendations <- list()
  
  for (book_idx in 1:n_books) {
    book_id <- rownames(all_books_encoded)[book_idx]
    if (is.null(book_id)) book_id <- paste0("book_", book_idx)
    
    # Get existing tags for this book
    book_existing <- existing_tags %>% filter(id == book_id)
    book_existing_tags <- if (nrow(book_existing) > 0) book_existing$existing_tags[[1]] else character(0)
    
    # Calculate distances to all other books
    book_code <- all_books_encoded[book_idx, ]
    distances <- apply(all_books_encoded, 1, function(other_code) {
      sum(book_code != other_code)  # Hamming distance
    })
    
    # Find nearest neighbors (excluding self)
    distances[book_idx] <- Inf  # Exclude self
    nearest_indices <- order(distances)[1:min_neighbors]
    nearest_distances <- distances[nearest_indices]
    
    # Get tags from nearest neighbors
    neighbor_tags <- list()
    for (i in seq_along(nearest_indices)) {
      neighbor_id <- rownames(all_books_encoded)[nearest_indices[i]]
      if (is.null(neighbor_id)) neighbor_id <- paste0("book_", nearest_indices[i])
      
      neighbor_existing <- existing_tags %>% filter(id == neighbor_id)
      if (nrow(neighbor_existing) > 0) {
        neighbor_tags[[i]] <- neighbor_existing$existing_tags[[1]]
      } else {
        neighbor_tags[[i]] <- character(0)
      }
    }
    
    # Calculate tag frequencies among neighbors
    all_neighbor_tags <- unlist(neighbor_tags)
    if (length(all_neighbor_tags) > 0) {
      # Ensure tag names are properly handled as character strings
      all_neighbor_tags <- as.character(all_neighbor_tags)
      
      # Use manual counting instead of table() to avoid tag name issues
      unique_tags <- unique(all_neighbor_tags)
      tag_counts <- setNames(
        sapply(unique_tags, function(tag) sum(all_neighbor_tags == tag)),
        unique_tags
      )
      
      # Calculate tag scores based on frequency and distance
      # Use a loop instead of sapply to avoid name duplication issues
      tag_scores <- numeric(length(tag_counts))
      names(tag_scores) <- names(tag_counts)
      
      for (i in seq_along(tag_counts)) {
        tag <- names(tag_counts)[i]
        count <- tag_counts[i]
        # Weight by inverse distance and frequency
        avg_distance <- mean(nearest_distances)
        tag_scores[i] <- count / (avg_distance + 1)  # Add 1 to avoid division by zero
      }
      
      # Filter out existing tags and sort by score
      new_tags <- names(tag_scores)[!names(tag_scores) %in% book_existing_tags]
      if (length(new_tags) > 0) {
        new_tag_scores <- tag_scores[new_tags]
        new_tag_scores <- sort(new_tag_scores, decreasing = TRUE)
        
        # Apply threshold and limit recommendations
        high_score_tags <- new_tag_scores[new_tag_scores >= threshold]
        if (length(high_score_tags) > max_recommendations) {
          high_score_tags <- high_score_tags[1:max_recommendations]
        }
        
        if (length(high_score_tags) > 0) {
          recommendations[[book_id]] <- list(
            book_id = book_id,
            existing_tags = book_existing_tags,
            recommended_tags = names(high_score_tags),
            tag_scores = high_score_tags,
            nearest_neighbors = rownames(all_books_encoded)[nearest_indices],
            neighbor_distances = nearest_distances
          )
        }
      }
    }
  }
  
  cat("4. Formatting results...\n")
  
  # Convert to data frame
  if (length(recommendations) > 0) {
    results <- do.call(rbind, lapply(recommendations, function(rec) {
      data.frame(
        book_id = rec$book_id,
        recommended_tag = rec$recommended_tags,
        score = rec$tag_scores,
        existing_tags = paste(rec$existing_tags, collapse = ", "),
        stringsAsFactors = FALSE
      )
    }))
    
    # Sort by score
    results <- results[order(results$score, decreasing = TRUE), ]
    
    cat("✓ Found", nrow(results), "tag recommendations for", length(recommendations), "books\n")
    return(results)
  } else {
    cat("✓ No tag recommendations found\n")
    return(data.frame(
      book_id = character(),
      recommended_tag = character(),
      score = numeric(),
      existing_tags = character(),
      stringsAsFactors = FALSE
    ))
  }
}

# Function to show tag additions in a clear format
show_tag_additions <- function(spectral_results, top_n = 20) {
  if (nrow(spectral_results) == 0) {
    return(data.frame(
      book_id = character(),
      tag_name = character(),
      score = numeric(),
      stringsAsFactors = FALSE
    ))
  }
  
  # Create the result data frame with the desired structure
  result_df <- spectral_results %>%
    select(
      book_id = book_id,
      tag_name = recommended_tag,
      score = score
    ) %>%
    arrange(book_id, desc(score))  # Sort by book_id, then by score descending
  
  return(result_df)
}

# Test the spectral tag recommendation function
cat("\n=== Testing spectral tag recommendations ===\n")
spectral_recommendations <- add_new_tags_spectral(
  eav = eav,
  binary_classifiers = binary_classifiers_spectral,
  dataset = dataset_spectral,
  binary_embedding = binary_embedding_spectral,
  threshold = 0.5,  # Lower threshold for testing
  min_neighbors = 3,
  max_recommendations = 5
)

# Get clean data frame of tag additions
tag_additions_df <- show_tag_additions(spectral_recommendations)

# Display the results
cat("Tag additions data frame:\n")
print(tag_additions_df)

#saveRDS(tag_additions_df, "~/downloads/tag_additions_df.rds")
``` 

# End