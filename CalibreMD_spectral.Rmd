---
title: "CalibreMD - Spectral Analysis"
author: "Ken Brooks"
date: "5/11/2025"
output: html_document
---

# Setup

```{r setup, include=FALSE}
library(CalibreMD)
setup_packages()

# Set knitr options directly
knitr::opts_chunk$set(
  fig.pos = 'h',
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  autodep = TRUE,
  cache = TRUE,
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  fig.align = "center",
  fig.show = "hold"
)

#dataDir <- '/Users/kbrooks/Dropbox/Books/Calibre Travel Library'
#dataDir <- '/Users/kbrooks/Dropbox/Books/AI Travel Library'
dataDir <- '/Users/kbrooks/Dropbox/Books/calbreGPT'

md_db <- find_md_db(dataDir)
```

# Load data from SQLite to EAV table

```{r}
eav <- md_db %>% 
  load_eav() %>% 
  explode_text_features()
```

# Spectral Analysis

Goals:
1. Recommend existing tags for a set of books (and those that should be removed)
2. Discover book groupings and recommend tags that describe them.

Process for #1:

1. Form a graph based on existing tags
  a. build the stars (connect books with the same tag)
  b. connect star centers to a random other star centers for different tags (can experiment with this)
2. Spectral hashing to get a perfect code book for existing tag graph
3. Use k binary classifiers to learn the codebook (xgboost, etc.) (maps any book to codebook space
   using iterative quantization)
4. Encode a book and find nearest neighbors - use those to indicate what tags should be
  a. Assume the book has the same tags as its nearest neighbor
  b. Use distance to other clusters for multi-tags

Process for #2:

1. Using encoders from step 1 encode all unseen books
2. Scan through all of possible clustering and see what's grouped (silhouette coeff?)
  a. One will likely hit the way we've tagged with unseen or misclassified books as noise
  b. Other clusters will look good (subsets, supersets, or other)
3. Find representative features for the clusters (chi-Sq or mutual information)

## Spectral Analysis Functions (No Threshold)
```{r}
# Spectral version of prep_dataset - no minimum book threshold
prep_dataset_spectral <- function(eav){
  book_tags <- eav %>% 
    filter(feature == "tag") %>% 
    mutate(tag = value) %>% 
    select(id, tag) %>% 
    distinct() %>%
    arrange(id)
  
  # No filtering by tag frequency - use all tags
  valid_tags <- unique(book_tags$tag)
  
  book_tags <- book_tags %>%
    filter(tag %in% valid_tags)
  
  book_features <- eav %>% 
    filter(!feature %in% c("tag", "title_original")) %>% 
    mutate(feature = paste(feature, value, sep = ": ")) %>% 
    select(id, feature) %>% 
    distinct() %>% 
    arrange(id)
  
  common_ids <- sort(intersect(book_tags$id, book_features$id))
  
  book_tags <- book_tags %>%
    filter(id %in% common_ids)
  
  book_features <- book_features %>%
    filter(id %in% common_ids)
  
  # Create index mappings for row (id) and columns (tag or feature)
  id_levels <- sort(unique(common_ids)) # ensures same order in both matrices
  
  # Sparse tag matrix
  tag_levels <- sort(unique(book_tags$tag))
  tag_matrix <- Matrix::sparseMatrix(
    i = match(book_tags$id, id_levels),
    j = match(book_tags$tag, tag_levels),
    x = 1L,
    dims = c(length(id_levels), length(tag_levels)),
    dimnames = list(id_levels, tag_levels)
  )
  
  # Sparse feature matrix
  feature_levels <- sort(unique(book_features$feature))
  feature_matrix <- Matrix::sparseMatrix(
    i = match(book_features$id, id_levels),
    j = match(book_features$feature, feature_levels),
    x = 1L,
    dims = c(length(id_levels), length(feature_levels)),
    dimnames = list(id_levels, feature_levels)
  )
  
  list(
    X = feature_matrix,
    Y = tag_matrix,
    book_features = book_features
  )
}

# Spectral version of train_models
train_models_spectral <- function(dataset){
  requireNamespace("xgboost", quietly = TRUE)
  X <- dataset$X
  Y <- dataset$Y
  models <- list()
  for (tag in colnames(Y)) {
    # Extract label vector and convert to numeric
    y <- as.numeric(Y[, tag])
    # Construct DMatrix using sparse feature matrix
    dtrain <- xgboost::xgb.DMatrix(data = X, label = y)
    models[[tag]] <- xgboost::xgboost(
      data = dtrain,
      objective = "binary:logistic",
      eval_metric = "logloss",
      nrounds = 50,
      verbose = 0
    )
  }
  models
}

# Spectral version of predict_tags
predict_tags_spectral <- function(models, dataset){
  requireNamespace("xgboost", quietly = TRUE)
  book_features <- dataset$book_features
  # Assume predict_ids and book_features are defined
  predict_ids <- unique(book_features$id)  # books to predict for
  
  # Add row_index to book_features dynamically
  book_features <- book_features %>%
    mutate(row_index = match(id, predict_ids))
  
  # Initialize predictions output
  pred_probs_new <- data.frame(book_id = predict_ids)
  
  # Loop over all models/tags
  for (tag in names(models)) {
    model <- models[[tag]]
    used_features <- model$feature_names
    
    # Align features to model-specific ones
    filtered_book_features <- book_features %>%
      filter(feature %in% used_features) %>%
      mutate(col_index = match(feature, used_features)) %>%
      filter(!is.na(row_index) & !is.na(col_index))
    
    # Build sparse matrix
    new_feature_matrix <- Matrix::sparseMatrix(
      i = filtered_book_features$row_index,
      j = filtered_book_features$col_index,
      x = 1L,
      dims = c(length(predict_ids), length(used_features)),
      dimnames = list(NULL, used_features)
    )
    
    # Predict and store
    preds <- stats::predict(model, newdata = new_feature_matrix)
    pred_probs_new[[tag]] <- preds
  }
  
  pred_probs_new
}

# Create spectral dataset (no threshold)
cat("Creating spectral dataset (no minimum book threshold)...\n")
dataset_spectral <- prep_dataset_spectral(eav)
cat("Spectral dataset dimensions:", nrow(dataset_spectral$X), "books x", ncol(dataset_spectral$X), "features\n")
cat("Spectral dataset tags:", ncol(dataset_spectral$Y), "tags\n")

# Train spectral models
cat("Training spectral models...\n")
models_spectral <- train_models_spectral(dataset_spectral)
```

## Build Star Graph
```{r}
tag_membership <- eav %>% 
  filter(feature == "tag") %>% 
  mutate(tag = value) %>% 
  select(id, tag) %>% 
  group_by(tag) %>% 
  summarize(membership = list(id), .groups = "drop")

# TODO: produce a list of pairs encoding a star graph over these tags. Pick a random book id from each membership list and produce a pair connecting that book id to every other book id in the membership list. This needs to be symmetric, when emitting a pair [a, b] also emit [b, a]

# Step 1: Create star graph + record center nodes per tag
star_graph_df <- tag_membership %>%
  mutate(center = purrr::map_int(membership, ~ sample(.x, 1))) %>%
  mutate(pairs = purrr::map2(membership, center, function(ids, center) {
    others <- setdiff(ids, center)
    if (length(others) == 0) return(tibble(from = integer(), to = integer()))
    bind_rows(
      tibble(from = center, to = others),
      tibble(from = others, to = center)
    )
  }))

# Step 2: Collect positive edges
positive_edges <- star_graph_df %>%
  select(pairs) %>%
  tidyr::unnest(pairs) %>%
  mutate(weight = 1)

# Step 3: Create negative edges by pairing each center with another random center
centers <- star_graph_df$center
n_centers <- length(centers)

negative_edges <- tibble(
  from = centers,
  to = purrr::map_int(centers, function(center) {
    sample(setdiff(centers, center), 1)
  })
) %>%
  bind_rows(transmute(., from = to, to = from)) %>%  # symmetric
  mutate(weight = -1)

# Step 4: Combine
star_graph_pairs <- bind_rows(positive_edges, negative_edges)
```

## Laplacian
```{r}

# Use signed weights directly
edge_list <- star_graph_pairs %>%
  select(from, to, weight)

# Step 1: Build node index
nodes <- sort(unique(c(edge_list$from, edge_list$to)))
node_index <- setNames(seq_along(nodes), nodes)

# Step 2: Build signed adjacency matrix A
A <- Matrix::sparseMatrix(
  i = node_index[as.character(edge_list$from)],
  j = node_index[as.character(edge_list$to)],
  x = edge_list$weight,
  dims = c(length(nodes), length(nodes)),
  dimnames = list(nodes, nodes)
)

# Step 3: Compute signed degree matrix D (sum of absolute edge weights)
d_vals <- Matrix::rowSums(abs(A))
D_inv_sqrt <- Matrix::Diagonal(x = 1 / sqrt(d_vals))

# Step 4: Compute signed normalized Laplacian
L_signed_norm <- Matrix::Diagonal(n = length(nodes)) - D_inv_sqrt %*% A %*% D_inv_sqrt

```
## Eigenvectors and values
```{r}
# Set number of nontrivial eigenpairs desired
k <- 5  # example value

eig <- RSpectra::eigs_sym(
  L_signed_norm,
  k       = k + 1,  # request one extra to drop trivial eigenvalue
  which   = "SM",
  target  = 0
)

# Remove the trivial eigenpair (usually eigenvalue ~0)
# Optionally filter based on a tolerance if needed
eigvals <- eig$values[-1]
eigvecs <- eig$vectors[, -1]

eigvals

```
## Iterative quantization
```{r}
iterative_quantization <- function(X, n_iter = 50) {
  # Step 1: Zero-center
  X_centered <- scale(X, center = TRUE, scale = FALSE)

  # Step 2: Optional PCA (skip if X already low-dimensional)
  # SVD of centered matrix
  svd_X <- svd(X_centered)
  V <- svd_X$v  # principal directions

  # Step 3: Project X to PCA space
  X_pca <- X_centered %*% V

  # Step 4: Initialize rotation matrix R as identity
  R <- diag(ncol(X))

  for (i in 1:n_iter) {
    # Step 5: Compute binary code
    B <- sign(X_pca %*% R)

    # Step 6: Solve for optimal R using SVD
    C <- t(B) %*% X_pca
    svd_C <- svd(C)
    R <- svd_C$u %*% t(svd_C$v)
  }

  # Final binary codes and rotation
  list(binary_code = sign(X_pca %*% R), rotation = R)
}

# eigvecs: matrix of size n_books × k from Laplacian
itq_result <- iterative_quantization(eigvecs)

binary_embedding <- itq_result$binary_code  # contains -1/+1 codes per book
rotation_matrix <- itq_result$rotation

# For spectral analysis, use the spectral dataset
if (!is.null(dataset_spectral$X)) {
  n_spectral_books <- nrow(dataset_spectral$X)
  if (nrow(binary_embedding) >= n_spectral_books) {
    # Take only the first n_spectral_books rows
    binary_embedding_spectral <- binary_embedding[1:n_spectral_books, , drop = FALSE]
    rownames(binary_embedding_spectral) <- rownames(dataset_spectral$X)
    cat("Aligned binary_embedding to spectral dataset: ", nrow(binary_embedding_spectral), "x", ncol(binary_embedding_spectral), "\n")
  } else {
    cat("Warning: binary_embedding has fewer rows than dataset_spectral$X\n")
    binary_embedding_spectral <- binary_embedding
  }
} else {
  binary_embedding_spectral <- binary_embedding
}

```
## Train binary classifiers
```{r}
# Helper function to fix data alignment issues
fix_data_alignment <- function(dataset, binary_embedding) {
  cat("Fixing data alignment...\n")
  
  # Check if binary_embedding has row names
  if (is.null(rownames(binary_embedding))) {
    cat("  Binary embedding missing row names. Using sequential IDs...\n")
    rownames(binary_embedding) <- paste0("book_", 1:nrow(binary_embedding))
  }
  
  # Check if dataset$X has row names
  if (is.null(rownames(dataset$X))) {
    cat("  Dataset X missing row names. Using sequential IDs...\n")
    rownames(dataset$X) <- paste0("book_", 1:nrow(dataset$X))
  }
  
  # Get book IDs that are in both the dataset and binary embedding
  dataset_book_ids <- rownames(dataset$X)
  binary_book_ids <- rownames(binary_embedding)
  common_book_ids <- intersect(dataset_book_ids, binary_book_ids)
  
  cat("  Dataset has", length(dataset_book_ids), "books\n")
  cat("  Binary embedding has", length(binary_book_ids), "books\n")
  cat("  Common books:", length(common_book_ids), "\n")
  
  if (length(common_book_ids) == 0) {
    # If no common IDs, align by position (assuming same order and same number of books)
    min_books <- min(nrow(dataset$X), nrow(binary_embedding))
    cat("  No common IDs found. Aligning by position for first", min_books, "books...\n")
    
    # Use the dataset's row names for both
    common_book_ids <- rownames(dataset$X)[1:min_books]
    rownames(binary_embedding)[1:min_books] <- common_book_ids
  }
  
  return(list(
    dataset = dataset,
    binary_embedding = binary_embedding,
    common_book_ids = common_book_ids
  ))
}

train_binary_classifiers <- function(dataset, binary_embedding, n_rounds = 50) {
  # Fix data alignment issues
  aligned_data <- fix_data_alignment(dataset, binary_embedding)
  dataset <- aligned_data$dataset
  binary_embedding <- aligned_data$binary_embedding
  common_book_ids <- aligned_data$common_book_ids
  
  # Get book features and binary codes
  X <- dataset$X  # feature matrix
  Y_binary <- binary_embedding  # binary codes from iterative quantization
  
  # Align the matrices using common book IDs
  X_aligned <- X[common_book_ids, ]
  Y_aligned <- Y_binary[common_book_ids, ]
  
  cat("Training", ncol(Y_aligned), "binary classifiers on", nrow(X_aligned), "books...\n")
  
  # Train k binary classifiers (one for each bit)
  k <- ncol(Y_aligned)
  classifiers <- list()
  
  for (bit in 1:k) {
    # Convert -1/+1 to 0/1 for binary classification
    y_bit <- (Y_aligned[, bit] + 1) / 2  # converts -1,1 to 0,1
    
    # Train XGBoost classifier for this bit
    dtrain <- xgboost::xgb.DMatrix(data = X_aligned, label = y_bit)
    
    classifier <- xgboost::xgboost(
      data = dtrain,
      objective = "binary:logistic",
      eval_metric = "logloss",
      nrounds = n_rounds,
      verbose = 0
    )
    
    classifiers[[bit]] <- classifier
  }
  
  # Return trained classifiers and alignment info
  list(
    classifiers = classifiers,
    feature_names = colnames(X),
    common_book_ids = common_book_ids,
    n_bits = k
  )
}

# Train the spectral binary classifiers
binary_classifiers_spectral <- train_binary_classifiers(dataset_spectral, binary_embedding_spectral)
```

## Test binary classifiers
```{r}
# Test the binary classifiers
test_binary_classifiers <- function(binary_classifiers, dataset, binary_embedding) {
  cat("Testing binary classifiers...\n")
  
  # Test 1: Check structure
  cat("1. Checking function output structure...\n")
  if (!is.list(binary_classifiers)) {
    cat("   ❌ ERROR: binary_classifiers is not a list\n")
    return(list(error = "binary_classifiers is not a list"))
  }
  if (!all(c("classifiers", "feature_names", "common_book_ids", "n_bits") %in% names(binary_classifiers))) {
    cat("   ❌ ERROR: binary_classifiers missing required components\n")
    return(list(error = "binary_classifiers missing required components"))
  }
  cat("   ✓ Structure is correct\n")
  
  # Test 2: Check number of classifiers matches binary embedding
  cat("2. Checking number of classifiers...\n")
  n_classifiers <- length(binary_classifiers$classifiers)
  n_bits <- ncol(binary_embedding)
  if (n_classifiers != n_bits) {
    cat("   ❌ ERROR: Number of classifiers (", n_classifiers, ") doesn't match number of bits (", n_bits, ")\n", sep = "")
    return(list(error = "Number of classifiers doesn't match number of bits"))
  }
  cat("   ✓ Number of classifiers (", n_classifiers, ") matches number of bits (", n_bits, ")\n", sep = "")
  
  # Test 3: Check data alignment and row names
  cat("3. Checking data alignment...\n")
  
  # Check if binary_embedding has row names
  binary_has_rownames <- !is.null(rownames(binary_embedding))
  cat("   Binary embedding has row names:", binary_has_rownames, "\n")
  
  # Check if dataset$X has row names
  dataset_has_rownames <- !is.null(rownames(dataset$X))
  cat("   Dataset X has row names:", dataset_has_rownames, "\n")
  
  # Check dimensions
  cat("   Binary embedding dimensions:", nrow(binary_embedding), "x", ncol(binary_embedding), "\n")
  cat("   Dataset X dimensions:", nrow(dataset$X), "x", ncol(dataset$X), "\n")
  
  # Check if we have common book IDs
  n_common <- length(binary_classifiers$common_book_ids)
  cat("   Number of common book IDs:", n_common, "\n")
  
  if (n_common == 0) {
    cat("   ⚠️  WARNING: No common book IDs found! This indicates a data alignment problem.\n")
    cat("   This might be because:\n")
    cat("   - Binary embedding doesn't have row names\n")
    cat("   - Dataset X doesn't have row names\n")
    cat("   - Row names don't match between the two datasets\n")
    return(list(error = "No common book IDs found"))
  }
  
  cat("   ✓ Data alignment looks good\n")
  
  # Test 4: Test predictions on training data
  cat("4. Testing predictions on training data...\n")
  
  tryCatch({
    # Check if common_book_ids exist in dataset$X
    missing_in_dataset <- setdiff(binary_classifiers$common_book_ids, rownames(dataset$X))
    if (length(missing_in_dataset) > 0) {
      cat("   ⚠️  WARNING: Some book IDs missing from dataset$X:", paste(missing_in_dataset, collapse = ", "), "\n")
    }
    
    # Check if common_book_ids exist in binary_embedding
    missing_in_embedding <- setdiff(binary_classifiers$common_book_ids, rownames(binary_embedding))
    if (length(missing_in_embedding) > 0) {
      cat("   ⚠️  WARNING: Some book IDs missing from binary_embedding:", paste(missing_in_embedding, collapse = ", "), "\n")
    }
    
    # Use only book IDs that exist in both datasets
    available_book_ids <- intersect(binary_classifiers$common_book_ids, rownames(dataset$X))
    available_book_ids <- intersect(available_book_ids, rownames(binary_embedding))
    
    if (length(available_book_ids) == 0) {
      cat("   ❌ ERROR: No common book IDs found between datasets\n")
      return(list(error = "No common book IDs found"))
    }
    
    cat("   Using", length(available_book_ids), "common book IDs for testing\n")
    
    X_test <- dataset$X[available_book_ids, ]
    
    # Get predictions from all classifiers
    predictions <- matrix(0, nrow = nrow(X_test), ncol = n_classifiers)
    for (i in 1:n_classifiers) {
      pred_probs <- predict(binary_classifiers$classifiers[[i]], newdata = X_test)
      predictions[, i] <- ifelse(pred_probs > 0.5, 1, 0)  # Convert to binary
    }
    
    # Convert back to -1/+1 format
    predictions_binary <- 2 * predictions - 1
    
    # Compare with original binary embedding
    original_binary <- binary_embedding[available_book_ids, ]
    
    # Calculate accuracy for each bit
    bit_accuracies <- colMeans(predictions_binary == original_binary)
    mean_accuracy <- mean(bit_accuracies)
    
    cat("   ✓ Mean prediction accuracy:", round(mean_accuracy, 3), "\n")
    cat("   ✓ Individual bit accuracies:", round(bit_accuracies, 3), "\n")
    
    # Test 5: Check that all classifiers are XGBoost models
    cat("5. Checking classifier types...\n")
    all_xgboost <- all(sapply(binary_classifiers$classifiers, function(x) inherits(x, "xgb.Booster")))
    if (!all_xgboost) {
      cat("   ❌ ERROR: Not all classifiers are XGBoost models\n")
      return(list(error = "Not all classifiers are XGBoost models"))
    }
    cat("   ✓ All classifiers are XGBoost models\n")
    
    # Test 6: Check feature names consistency
    cat("6. Checking feature names...\n")
    if (!identical(binary_classifiers$feature_names, colnames(dataset$X))) {
      cat("   ❌ ERROR: Feature names are not consistent\n")
      return(list(error = "Feature names are not consistent"))
    }
    cat("   ✓ Feature names are consistent\n")
    
    # Test 7: Check for reasonable accuracy
    cat("7. Checking prediction quality...\n")
    if (mean_accuracy < 0.5) {
      cat("   ⚠️  WARNING: Low prediction accuracy (", round(mean_accuracy, 3), "). This might indicate:\n", sep = "")
      cat("   - Data alignment issues\n")
      cat("   - Insufficient training data\n")
      cat("   - Binary codes are too noisy\n")
    } else {
      cat("   ✓ Prediction accuracy is reasonable\n")
    }
    
    cat("All tests passed! ✓\n")
    
    # Return test results
    list(
      mean_accuracy = mean_accuracy,
      bit_accuracies = bit_accuracies,
      n_classifiers = n_classifiers,
      n_training_samples = length(binary_classifiers$common_book_ids),
      data_alignment_ok = TRUE
    )
    
  }, error = function(e) {
    cat("   ❌ ERROR in prediction testing:", e$message, "\n")
    cat("   This might be due to data alignment issues.\n")
    list(error = e$message, data_alignment_ok = FALSE)
  })
}

# Run the tests
test_results <- test_binary_classifiers(binary_classifiers_spectral, dataset_spectral, binary_embedding_spectral)
```

## Encode new books
```{r}
# Function to encode new books using trained classifiers
encode_new_books <- function(binary_classifiers, new_books_features) {
  cat("Encoding new books using trained binary classifiers...\n")
  
  # Ensure new_books_features has the same feature names as training data
  if (!all(binary_classifiers$feature_names %in% colnames(new_books_features))) {
    missing_features <- setdiff(binary_classifiers$feature_names, colnames(new_books_features))
    cat("  ⚠️  WARNING: Missing features:", paste(missing_features, collapse = ", "), "\n")
    cat("  Adding missing features with zeros...\n")
    
    # Add missing features with zeros
    for (feature in missing_features) {
      new_books_features[[feature]] <- 0
    }
  }
  
  # Reorder columns to match training data
  new_books_features <- new_books_features[, binary_classifiers$feature_names, drop = FALSE]
  
  # Get predictions from all classifiers
  n_books <- nrow(new_books_features)
  n_bits <- binary_classifiers$n_bits
  encoded_books <- matrix(0, nrow = n_books, ncol = n_bits)
  
  for (bit in 1:n_bits) {
    pred_probs <- predict(binary_classifiers$classifiers[[bit]], newdata = new_books_features)
    encoded_books[, bit] <- ifelse(pred_probs > 0.5, 1, 0)  # Convert to binary
  }
  
  # Convert to -1/+1 format
  encoded_books_binary <- 2 * encoded_books - 1
  
  # Add row names if available
  if (!is.null(rownames(new_books_features))) {
    rownames(encoded_books_binary) <- rownames(new_books_features)
  }
  
  cat("  ✓ Encoded", n_books, "books into", n_bits, "binary dimensions\n")
  
  return(encoded_books_binary)
}

# Test encoding on a subset of the training data
cat("\n=== Testing encoding on training data ===\n")
test_books <- dataset_spectral$X[1:5, , drop = FALSE]  # First 5 books
encoded_test_books <- encode_new_books(binary_classifiers_spectral, test_books)
cat("Encoded test books shape:", nrow(encoded_test_books), "x", ncol(encoded_test_books), "\n")
print(head(encoded_test_books))
```

## Add new tags using spectral analysis
```{r}
# Function to recommend new tags using spectral analysis
add_new_tags_spectral <- function(eav, binary_classifiers, dataset, binary_embedding, 
                                  threshold = 0.7, min_neighbors = 3, max_recommendations = 10) {
  cat("Recommending new tags using spectral analysis...\n")
  
  # Encode all books in the dataset using trained classifiers
  cat("1. Encoding all books...\n")
  all_books_encoded <- encode_new_books(binary_classifiers, dataset$X)
  
  # Get existing tag assignments
  cat("2. Getting existing tag assignments...\n")
  existing_tags <- eav %>% 
    filter(feature == "tag") %>% 
    select(id, tag = value) %>% 
    group_by(id) %>% 
    summarize(existing_tags = list(tag), .groups = "drop")
  
  # Get all unique tags
  all_tags <- eav %>% 
    filter(feature == "tag") %>% 
    pull(value) %>% 
    unique()
  
  cat("3. Analyzing nearest neighbors for each book...\n")
  
  # Calculate pairwise distances between all books
  # Using Hamming distance for binary codes (number of different bits)
  n_books <- nrow(all_books_encoded)
  recommendations <- list()
  
  for (book_idx in 1:n_books) {
    book_id <- rownames(all_books_encoded)[book_idx]
    if (is.null(book_id)) book_id <- paste0("book_", book_idx)
    
    # Get existing tags for this book
    book_existing <- existing_tags %>% filter(id == book_id)
    book_existing_tags <- if (nrow(book_existing) > 0) book_existing$existing_tags[[1]] else character(0)
    
    # Calculate distances to all other books
    book_code <- all_books_encoded[book_idx, ]
    distances <- apply(all_books_encoded, 1, function(other_code) {
      sum(book_code != other_code)  # Hamming distance
    })
    
    # Find nearest neighbors (excluding self)
    distances[book_idx] <- Inf  # Exclude self
    nearest_indices <- order(distances)[1:min_neighbors]
    nearest_distances <- distances[nearest_indices]
    
    # Get tags from nearest neighbors
    neighbor_tags <- list()
    for (i in seq_along(nearest_indices)) {
      neighbor_id <- rownames(all_books_encoded)[nearest_indices[i]]
      if (is.null(neighbor_id)) neighbor_id <- paste0("book_", nearest_indices[i])
      
      neighbor_existing <- existing_tags %>% filter(id == neighbor_id)
      if (nrow(neighbor_existing) > 0) {
        neighbor_tags[[i]] <- neighbor_existing$existing_tags[[1]]
      } else {
        neighbor_tags[[i]] <- character(0)
      }
    }
    
    # Calculate tag frequencies among neighbors
    all_neighbor_tags <- unlist(neighbor_tags)
    if (length(all_neighbor_tags) > 0) {
      # Ensure tag names are properly handled as character strings
      all_neighbor_tags <- as.character(all_neighbor_tags)
      
      # Use manual counting instead of table() to avoid tag name issues
      unique_tags <- unique(all_neighbor_tags)
      tag_counts <- setNames(
        sapply(unique_tags, function(tag) sum(all_neighbor_tags == tag)),
        unique_tags
      )
      
      # Calculate tag scores based on frequency and distance
      # Use a loop instead of sapply to avoid name duplication issues
      tag_scores <- numeric(length(tag_counts))
      names(tag_scores) <- names(tag_counts)
      
      for (i in seq_along(tag_counts)) {
        tag <- names(tag_counts)[i]
        count <- tag_counts[i]
        # Weight by inverse distance and frequency
        avg_distance <- mean(nearest_distances)
        tag_scores[i] <- count / (avg_distance + 1)  # Add 1 to avoid division by zero
      }
      
      # Filter out existing tags and sort by score
      new_tags <- names(tag_scores)[!names(tag_scores) %in% book_existing_tags]
      if (length(new_tags) > 0) {
        new_tag_scores <- tag_scores[new_tags]
        new_tag_scores <- sort(new_tag_scores, decreasing = TRUE)
        
        # Apply threshold and limit recommendations
        high_score_tags <- new_tag_scores[new_tag_scores >= threshold]
        if (length(high_score_tags) > max_recommendations) {
          high_score_tags <- high_score_tags[1:max_recommendations]
        }
        
        if (length(high_score_tags) > 0) {
          recommendations[[book_id]] <- list(
            book_id = book_id,
            existing_tags = book_existing_tags,
            recommended_tags = names(high_score_tags),
            tag_scores = high_score_tags,
            nearest_neighbors = rownames(all_books_encoded)[nearest_indices],
            neighbor_distances = nearest_distances
          )
        }
      }
    }
  }
  
  cat("4. Formatting results...\n")
  
  # Convert to data frame
  if (length(recommendations) > 0) {
    results <- do.call(rbind, lapply(recommendations, function(rec) {
      data.frame(
        book_id = rec$book_id,
        recommended_tag = rec$recommended_tags,
        score = rec$tag_scores,
        existing_tags = paste(rec$existing_tags, collapse = ", "),
        stringsAsFactors = FALSE
      )
    }))
    
    # Sort by score
    results <- results[order(results$score, decreasing = TRUE), ]
    
    cat("✓ Found", nrow(results), "tag recommendations for", length(recommendations), "books\n")
    return(results)
  } else {
    cat("✓ No tag recommendations found\n")
    return(data.frame(
      book_id = character(),
      recommended_tag = character(),
      score = numeric(),
      existing_tags = character(),
      stringsAsFactors = FALSE
    ))
  }
}

# Function to show tag additions in a clear format
show_tag_additions <- function(spectral_results, top_n = 20) {
  if (nrow(spectral_results) == 0) {
    return(data.frame(
      book_id = character(),
      tag_name = character(),
      score = numeric(),
      stringsAsFactors = FALSE
    ))
  }
  
  # Create the result data frame with the desired structure
  result_df <- spectral_results %>%
    select(
      book_id = book_id,
      tag_name = recommended_tag,
      score = score
    ) %>%
    arrange(book_id, desc(score))  # Sort by book_id, then by score descending
  
  return(result_df)
}

# Test the spectral tag recommendation function
cat("\n=== Testing spectral tag recommendations ===\n")
spectral_recommendations <- add_new_tags_spectral(
  eav = eav,
  binary_classifiers = binary_classifiers_spectral,
  dataset = dataset_spectral,
  binary_embedding = binary_embedding_spectral,
  threshold = 0.5,  # Lower threshold for testing
  min_neighbors = 3,
  max_recommendations = 5
)

# Get clean data frame of tag additions
tag_additions_df <- show_tag_additions(spectral_recommendations)

# Display the results
cat("Tag additions data frame:\n")
print(tag_additions_df)
``` 